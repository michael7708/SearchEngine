ics 175 project artificial intelligence 
assignment 3 
assignment
3
cs
175
project
artificial
intelligence
due
9
30am
thursday
october
18th
2007
instructions
assignment
3
assignment
will
construct
working
learning
algorithm
can
learn
classification
model
classify
feature
vectors
2
classes
classification
learning
algorithm
will
implement
incremental
gradient
descent
perceptron
learning
algorithm
will
covering
class
part
1
will
implement
function
calculates
perceptron
output
error
given
vector
perceptron
weights
data
set
part
2
will
write
simple
function
visualize
weights
part
3
will
design
implement
incremental
gradient
descent
perceptron
learning
algorithm
parts
4
5
will
develop
simple
functions
display
learning
capabilities
algorithm
part
6
optional
extra
part
assignment
can
investigate
automated
method
adjusting
learning
rate
online
will
grade
code
part
assignment
however
feel
free
turn
include
report
wish
experiments
will
use
type
data
used
train
nearest
neighbor
classifier
assignment
2
data
can
found
files
sampledata1
mat
sampledata2
mat
sampledata1
contains
dataset
size
20
2
vector
targets
size
20
1
values
1
1
corresponding
2
classes
sampledata2
contains
dataset
size
40x2
targets
size
40
1
part
1
calculating
perceptron
outputs
errors
will
implement
function
takes
set
weights
set
feature
vectors
returns
classification
decisions
calculated
perceptron
using
weights
simple
function
weight
vector
dimensions
1
1
row
vector
1
elements
number
features
components
1
weight
vector
correspond
features
1
1th
component
weight
vector
weight
constant
input
constant
input
set
1
argument
function
matrix
called
data
dimension
1
first
columns
different
features
1
th
column
column
1's
constant
input
probably
useful
just
define
augmented
version
feature
data
extra
column
1's
early
perceptron
learning
experiments
functions
can
just
define
matlab
prompt
total
number
training
examples
function
needs
following
header
information
need
give
exact
name
arguments
order
function
thresholded
outputs
perceptron
weights
data
function
thresholded
outputs
perceptron
weights
data
brief
description
function
name
cs
175
date
inputs
weights
1
1
row
vector
weights
data
1
matrix
training
data
outputs
thresholded
outputs
1
vector
thresholded
perceptron
outputs
entry
must
1
1
code
goes
part
2
calculating
classification
error
mean
square
error
mse
perceptron
next
function
implemented
takes
vector
weights
set
data
vectors
targets
described
earlier
calculates
mean
square
error
classification
error
perceptron
outputs
compared
target
values
will
discussing
class
calculated
mean
square
error
calculation
use
unthresholded
outputs
namely
sigmoid
weights
inputs
discussed
class
classification
error
threshold
real
valued
numbers
get
class
prediction
1
1
compare
true
targets
perceptron
models
true
targets
will
1
1
will
assumed
function
wish
add
code
check
targets
provided
inputs
indeed
take
valus
1
1
wish
function
can
call
perceptron
function
needs
following
header
information
need
give
exact
name
arguments
order
function
cerror
mse
perceptron
error
weights
data
targets
function
cerror
mse
perceptron
error
weights
data
targets
brief
description
function
name
cs
175
date
inputs
weights
1
1
row
vector
weights
data
1
matrix
training
data
targets
1
vector
target
values
1
1
outputs
cerror
percentage
examples
misclassified
0
100
using
thresholded
perceptron
outputs
mse
mean
square
error
sum
squared
errors
targets
unthresholded
outputs
divided
code
goes
part
3
perceptron
learning
algorithm
using
incremental
gradient
descent
learning
implement
matlab
function
trains
perceptron
classification
model
based
incremental
gradient
descent
discussed
class
summarize
pseudocode
algorithm
roughly
follows
please
also
refer
discussion
class
class
lecture
notes
lectures
5
6
initialize
weight
small
randomly
chosen
value
iteration
0
convergence
criterion
achieved
1
calculate
output
network
input
1
1
update
weight
see
class
slides
equation
end
end
calculate
convergence
criterion
iteration
optional
plot
current
location
decision
boundary
end
fairly
high
level
pseudo
code
details
update
weights
define
convergence
criterion
will
need
specified
function
weights
mse
acc
learn
perceptron
data
targets
rate
threshold
init
method
random
seed
plotflag
function
weights
mse
acc
learn
perceptron
data
targets
rate
threshold
init
method
random
seed
plotflag
brief
description
function
name
cs
175
date
inputs
data
1
matrix
training
data
targets
1
vector
target
values
1
1
rate
learning
rate
perceptron
algorithm
rate
0
001
threshold
reduction
mse
one
iteration
next
less
threshold
halt
learning
threshold
0
000001
init
method
method
used
initialize
weights
1
random
2
half
way
2
random
points
group
3
half
way
centroids
group
random
seed
integer
used
seed
random
number
generator
either
methods
1
2
initialization
useful
able
recreate
particular
run
exactly
plotflag
1
means
plotting
turned
default
value
0
many
iterations
plotting
100
outputs
weights
1
1
row
vector
learned
weights
mse
mean
squared
error
learned
weights
sum
squared
errors
divided
acc
classification
accuracy
learned
weights
percentage
0
100
code
goes
input
argument
rate
specifies
learning
rate
algorithm
discussed
class
may
experiment
little
find
good
setting
rate
given
problem
rate
small
algorithm
may
converge
slowly
rate
large
may
converge
since
may
take
steps
large
rate
values
0
001
0
0001
work
reasonably
well
data
sets
sampledata1
sampledata2
note
rate
large
weights
can
quickly
diverge
large
values
will
see
mean
squared
error
grow
quickly
indicating
try
smaller
value
mean
square
error
iteration
iteration
will
increase
rather
decrease
happens
put
check
algorithm
halts
mean
square
error
increases
several
different
ways
define
convergence
criterion
algorithm
essentially
one
wants
halt
learning
appears
perceptron
settled
global
minimum
error
surface
will
reflected
fact
mean
square
error
hardly
changing
one
iteration
next
one
simple
way
implement
compare
value
mean
squared
error
one
iteration
value
previous
iteration
decrease
error
less
input
argument
threshold
halt
threshold
small
threshold
value
can
set
smaller
threshold
stringent
determining
convergence
iterations
algorithm
will
take
threshold
value
0
00001
smaller
fairly
typical
discussed
class
need
select
initial
set
weights
perceptron
start
learning
process
help
can
call
function
initialize
weights175
will
need
figure
call
function
quite
simple
basically
gives
3
different
ways
initialize
weights
method
1
random
set
weights
method
2
selects
2
points
randomly
1
class
uses
weights
define
decision
boundary
half
way
points
method
3
like
method
2
except
instead
2
random
points
selects
centroids
2
dimensional
mean
class
can
experiment
methods
will
find
can
quite
difference
quickly
algorithm
converges
solution
depending
started
input
argument
random
seed
integer
specifies
value
used
function
seed
pseudo
random
number
generator
random
number
generator
initialize
weights175
uses
number
selecting
initial
weights
methods
1
2
specifying
seed
two
different
invocations
algorithm
will
generate
set
initial
weights
allowing
us
repeat
exactly
particular
experiment
can
ignore
input
arguments
plotflag
get
part
5
part
4
plotting
rate
convergence
algorithm
plotting
data
assignment
2
found
data
sampledata1
mat
linearly
separable
data
sampledata2
mat
expect
perceptron
can
learn
classify
simdata1
zero
error
will
able
get
zero
classification
error
simdata
2
now
modify
function
learn
percptron
plotflag
1
function
plots
2
separate
figures
algorithm
converged
value
mean
square
error
function
value
classification
accuracy
function
iteration
number
graph
axis
number
iterations
going
1
total
number
iterations
taken
algorithm
axis
mean
square
error
function
classification
accuracy
one
graph
will
need
store
array
value
mean
squared
error
accuracy
iteration
learning
proceeds
will
allow
see
perceptron
converged
solution
given
data
set
given
set
control
parameters
learning
rate
wish
include
plot
text
string
prints
name
data
set
used
value
learning
rate
convergence
threshold
information
like
will
make
easier
read
plots
start
generating
multiple
plots
different
data
sets
different
learning
rates
part
5
visualizing
decision
boundaries
part
assignment
modify
learn
perceptron
function
now
able
plot
location
decision
boundary
weights
every
kth
iteration
assist
provided
simple
function
called
weightplot175
will
need
figure
call
function
draw
decision
boundary
perceptron
show
training
data
quite
straightforward
figure
call
within
learn
perceptron
function
finally
want
call
every
kth
iteration
100
reason
takes
5000
iterations
converge
want
plot
position
decision
boundary
5000
iterations
may
require
experimentation
matlab
specifically
plotflag
1
code
following
addition
plots
part
4
iteration
0
create
figure
plot
perceptron
decision
boundary
superposed
data
iteration
0
corresponds
initial
randomly
chosen
weights
iteration
plot
current
perceptron
decision
boundary
superposed
data
iteration
2k
plot
current
perceptron
decision
boundary
superposed
data
iteration
3k
plot
current
perceptron
decision
boundary
superposed
data
convergence
now
test
learning
algorithm
2
data
sets
assignment
plot
figures
decision
boundary
superposed
data
example
every
50
iterations
50
able
see
decision
boundaries
improving
algorithm
converges
optional
part
6
automatically
adjusting
learning
rate
discussed
class
general
theory
learning
rate
generally
speaking
theory
suggests
decrease
learning
proceeds
one
simple
heuristic
idea
follows
let
delta
constant
0
1
delta
0
5
useful
starting
point
let
beta
number
slightly
greater
1
beta
1
1
often
used
lets
say
calculate
change
mean
squared
error
mse
error
decreased
case
adjust
learning
rate
follows
nu
beta
nu
multiply
current
learning
rate
beta
get
new
larger
learning
rate
however
nmse
decreased
adjust
learning
rate
follows
nu
delta
nu
multiply
current
learning
rate
delta
get
new
smaller
learning
rate
hope
reducing
learning
rate
can
take
smaller
step
reduce
mse
simplest
way
perform
update
iteration
start
initial
learning
rate
adjust
one
pass
examples
alternative
adjust
rate
often
one
adjust
example
although
adjusting
often
may
slow
algorithm
since
will
require
calculating
mse
time
automated
adjustment
learning
rate
enabled
setting
auto
adjust
parameter
1
flag
set
0
adjustment
initial
value
nu
used
throughout
learning
can
experiment
method
automatically
setting
learning
rate
example
single
plot
plot
mse
function
number
iterations
auto
adjust
turned
turned
will
2
curves
another
related
general
idea
called
line
search
calculate
direction
move
using
gradient
usual
manner
move
small
amount
epsilon
direction
compute
mse
new
point
weight
space
continue
move
small
steps
epsilon
multiple
epsilon
along
line
direction
gradient
direction
point
mse
increases
step
algorithm
backs
previous
point
new
gradient
line
direction
computed
point
algorithm
continues
main
difference
approach
keep
moving
along
line
mse
increases
whereas
recompute
direction
move
gradient
step
turn
turn
matlab
code
fully
implemented
versions
3
functions
listed
perceptron
perceptron
error
learn
perceptron
please
sure
use
exact
name
argument
list
specified
write
single
report
word
clearly
contains
following
report
clearly
mark
name
top
report
note
can
cut
paste
matlab
figures
directly
word
convenient
way
put
figures
report
use
simdata2
experiments
try
different
settings
learning
rate
high
say
0
01
medium
say
0
001
small
say
0
0001
use
code
part
4
produce
3
plots
mean
square
error
function
iteration
number
comment
results
use
random
method
1
choose
initial
weights
choose
particular
learning
rate
0
001
provide
plots
perceptron
decision
boundary
3
different
times
learning
1
beginning
initial
randomly
assigned
weights
2
roughly
midway
iterations
3
convergence
use
code
part
5
generate
plots
provide
comments
results
try
three
different
methods
initialization
methods
1
2
3
generate
plots
show
final
decision
boundaries
rate
convergence
part
4
three
methods
comment
results
implemented
optional
auto
adjust
method
part
6
can
include
plots
evaluate
whether
seems
help
convergence
data
note
part
completely
optional
will
included
grading
can
get
full
points
without
part
put
files
report
word
cs
175
eee
drop
box
call
matlab
functions
wrote
assignment
1
2
also
include
can
run
code
important
addition
uploading
files
electronically
please
hand
hardcopy
assignment
class
professor
smyth
1
stapled
copy
includes
word
writeup
matlab
programs
name
clearly
indicated
front
page
