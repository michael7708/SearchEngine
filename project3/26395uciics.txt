analyzing many servers google 
how many servers does google have 
google
servers
home
research
software
publications
industry
resume
flashback
introduction
attemp
tackle
greatest
question
modern
times
many
blade
pcs
google
run
initial
guess
2003
10
000
recently
consensus
seems
order
100
000
remained
guesses
best
try
see
number
can
deduced
various
requirements
network
bw
storage
cpu
etc
bandwidth
requirement
first
see
much
bandwidth
google
might
need
divide
much
bw
pc
can
handle
get
number
pcs
needed
constrain
analysis
north
america
similar
analysis
can
done
geographies
since
search
main
bread
butter
google
analysis
restricted
search
ok
first
numbers
that'll
help
us
pop
300
million
internet
penetration
us
70
google
market
share
30
assuming
average
person
using
google
20
searches
day
clicks
1
5
pages
results
search
supposedly
90
users
find
results
want
first
page
think
average
1
5
might
reasonable
web
page
served
google
approximately
25kb
size
can
check
saving
web
page
average
person
needs
20x1
5x25
750kb
data
google
day
bandwidth
served
per
day
google
therefore
0
3
mkt
share
0
7
internet
pop
300
million
750kb
around
48
million
mb
bandwidth
persecond
4
375
mbits
sec
note
conversion
bits
basically
around
4gbps
google
just
one
data
center
whole
us
highly
unlikely
need
oc96
pipe
can
calculate
much
google
need
pay
leasing
oc96
link
now
finally
many
pc's
needed
throw
4
375gbps
pcs
today
come
standard
1gbps
network
interfaces
google
need
5
pcs
course
assumes
pc
can
sustain
peak
throughput
1
gbps
bits
put
reach
target
feasible
number
think
without
getting
details
assumption
range
1mbps
case
google
need
around
5
000
pcs
well
analyzing
bandwidth
requirement
showed
surprising
statictic
something
can
used
accurately
guess
number
pcs
google
anywhere
5
5
000
vague
time
look
another
metric
storage
bandwidth
requirements
completely
left
another
part
equation
bandwidth
needed
google's
crawlers
get
back
later
things
make
bandwidth
estimate
conservative
looked
search
results
web
pages
image
search
news
pages
maps
gmail
etc
bandwidth
requirement
will
much
much
higher
storage
requirement
lets
restrict
discussion
just
web
pages
analysis
will
assume
google
audacious
enough
store
web
pages
memory
ram
search
queries
follow
usual
80
20
rule
80
queries
20
pages
maintaning
cache
20
requested
pages
work
quite
well
lets
assume
google
stores
web
pages
ram
current
estimate
total
number
web
pages
around
4
billion
lets
assume
average
web
page
100kb
pure
conjecture
something
start
working
also
lets
assume
blade
uses
2gb
ram
store
web
pages
simple
math
suggests
google
4
billion
100kb
2gb
blades
approximately
200
000
blades
2gb
blade
store
web
pages
well
assuming
blade
maximum
4gb
storage
32
bit
machines
remaining
2gb
rest
stuff
like
maintaining
indexes
data
running
page
rank
algorithm
etc
redundancy
ram
web
pages
well
hard
drives
blades
balde
comes
60gb
hard
drive
google
close
11
petabytes
note
terabytes
'slow
memory'
can
used
backup
gmail
etc
links
html
css
508
