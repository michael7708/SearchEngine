sli projects graphicalmodels 
graphical models 
classes
group
research
publications
code
login
projects
graphicalmodels
graphical
models
graphical
models
used
encode
structured
relationships
among
collections
random
variables
relationships
may
logical
example
expressing
forbidden
combinations
values
often
probabilistic
expressing
relative
likelihood
co
occurrence
relationships
expressed
graph
directly
related
variables
connected
can
used
simplify
automate
reasoning
full
system
work
focused
several
aspects
graphical
models
including
understanding
extending
variants
popular
belief
propagation
algorithm
inference
estimating
values
variables
given
observations
others
adaptive
incremental
inference
methods
organize
calculations
can
efficiently
reused
later
rapidly
find
changes
update
results
small
modifications
model
graphical
models
defined
continuous
valued
variables
example
2
3
positions
tracking
learning
structure
parameters
graphical
models
data
belief
propagation
loopy
belief
propagation
pearl
1988
popular
algorithm
approximate
inference
popularity
stems
part
extremely
effective
application
channel
coding
first
turbo
decoding
low
density
parity
check
ldpc
codes
mceliece
et
al
1998
initially
well
understood
recent
years
host
theoretical
results
obtained
help
quantify
behavior
belief
propagation
comes
two
general
flavors
sum
product
attempts
estimate
marginal
probabilities
outcomes
max
product
attempts
estimate
likely
configuration
variables
convergence
accuracy
sum
product
fig
1
bethe
saw
trees
developed
several
results
behavior
sum
product
algorithm
including
convergence
conditions
bounds
accuracy
resulting
marginal
probabilities
techniques
based
analyzing
bethe
tree
computation
tree
bp
algorithm
right
bethe
tree
formed
unrolling
graph
around
root
node
say
1
root
bethe
tree
corresponds
node
1
next
level
corresponds
1
's
neighbors
next
neighbors
including
parent
1
figure
1
black
red
nodes
tree
node
will
appear
node
may
many
copies
within
bethe
tree
bp
level
bottom
bethe
tree
corresponds
lth
iteration
loopy
bp
intuitively
convergence
corresonds
mixing
decoupling
root
distribution
initial
conditions
leaves
set
initial
conditions
belief
root
bp
must
converge
unique
fixed
point
intuitively
likely
happen
correlations
among
variables
weak
sufficiently
weak
dependence
level
level
dies
fast
enough
offset
increasing
number
nodes
level
developed
several
sufficient
conditions
guarantee
using
mixing
properties
factors
nips
2004
jmlr
2005
mooij
kappen
2007
extended
analysis
factor
graphs
accuracy
beliefs
can
assessed
using
subtree
called
self
avoiding
walk
saw
tree
fig
1
black
nodes
essence
corresponds
unrolling
graph
point
path
forms
loop
self
avoiding
slight
misnomer
since
walks
forming
graph
intersect
terminal
point
marginal
probability
bounds
can
computed
applying
convergence
analysis
subtree
reweighted
sum
product
variants
also
interested
called
reweighted
variants
sum
product
enable
bounds
normalization
constant
called
partition
function
distribution
bounds
can
used
learning
since
normalized
models
correspond
data
likelihood
can
also
used
produce
bounds
marginal
probabilities
negative
tree
reweighted
bp
work
shows
tree
reweighted
sum
product
can
modified
produce
lower
bound
partition
function
opposed
upper
bound
resulting
algorithm
generalizes
structured
mean
field
approach
reweighed
max
product
variants
reweighted
approaches
optimization
powerful
tool
combinatorial
search
methods
closely
related
linear
programming
relaxations
provide
upper
lower
bounds
optimal
configuration
work
developed
efficient
data
structures
optimize
solve
resulting
bounds
covering
tree
yarkony
et
al
2010
adaptive
inference
fig
2
hierarchical
clustering
adaptive
inference
describes
problem
repeatedly
modifying
performing
inference
model
since
sequence
models
used
similar
one
another
incremental
changes
made
stage
results
previous
inferential
calculations
can
used
compute
new
results
much
faster
performed
scratch
use
tree
contraction
process
define
hierarchical
clustering
nodes
factor
graph
fig
2
clustering
implies
partial
elimination
ordering
variables
graph
contraction
process
guarantees
change
model
log
computations
must
recomputed
data
structure
can
used
incorporate
arbitrary
changes
including
model
structure
observations
log
time
new
marginal
probabilities
can
computed
queried
log
time
moreover
without
knowing
number
position
priori
can
find
changes
optimizing
configuration
log
time
number
variables
whose
optimal
configuration
changed
continuous
random
variables
fig
3
nbp
uses
gaussian
mixtures
represent
beliefs
graphical
models
discrete
valued
random
variables
fairly
well
studied
continuous
non
gaussian
random
variables
much
complex
despite
fact
many
real
world
problems
consist
precisely
systems
developed
several
algorithms
dealing
continuous
valued
distributions
including
nonparametric
belief
propagation
represents
beliefs
using
gaussian
mixture
distributions
particle
belief
propagation
uses
importance
weighted
samples
learning
graphical
models
one
important
tasks
probabilistic
models
construction
based
collections
data
statistical
learning
interested
estimating
structure
model
parameters
given
fixed
structure
recent
emphases
include
using
composite
likelihood
improve
accuracy
model
preserving
efficiency
exploring
sequential
monte
carlo
approaches
learning
last
modified
july
06
2010
10
31
pm
bren
school
information
computer
science
university
california
irvine
