max welling nsf career page 
my nsf career page 
undirected
bipartite
graphical
models
applications
image
restoration
information
retrieval
max
welling
university
california
irvine
project
supported
nsf
career
grant
like
personally
thank
nsf
supporting
research
past
5
years
introduction
max
welling
2007
products
experts
scholarpedia
2007
pdf
url
large
scale
data
mining
problems
require
efficient
algorithms
processing
querying
storing
information
recognized
algorithms
need
able
model
uncertainty
model
assumptions
noise
data
probabilistic
models
particular
graphical
models
ideal
framework
tasks
also
pose
new
computational
challenges
directed
graphical
models
dominant
paradigm
many
large
scale
data
applications
however
properties
inherent
semantics
class
models
form
important
hurdle
processing
queries
learning
data
current
approach
deal
problem
focus
approximate
inference
algorithms
often
iterative
inefficient
manuscript
propose
new
class
models
``undirected
bipartite
graphical''
ubg
models
largely
avoids
computational
barrier
processing
queries
fast
learning
achieved
recently
introduced
technique
called
contrastive
divergence
project
1
probablistic
models
max
welling
michal
rosen
zvi
geoffrey
hinton
2004
exponential
family
harmoniums
application
information
retrieval
nips
2004
ps
pdf
peter
gehler
alex
holub
max
welling
2006
rate
adapting
poisson
rap
model
information
retrieval
object
recognition
icml
2006
pdf
software
directed
models
become
dominant
paradigm
machine
learning
many
good
reasons
easy
sample
nice
expectation
maximization
framework
learn
parameters
even
possible
search
optimal
structure
full
bayesian
setting
undirected
models
much
harder
handle
particular
presence
normalization
constant
partition
function
depends
parameters
usually
intractable
compute
harmoniums
two
layer
undirected
graphical
models
see
picture
top
layer
represents
array
hidden
variables
topic
variables
bottom
layer
represents
observed
random
variables
count
values
words
documents
effcient
learning
parameter
values
possible
using
contrastive
divergence
algorithm
hinton
type
model
particularly
suitable
modeling
text
data
others
shown
classiffication
retrieval
performance
better
directed
counterparts
lda
blei
ng
jordan
one
particularly
interesting
feature
follows
undirected
nature
model
process
inferring
topic
representations
documents
fast
one
bottom
pass
equivalently
one
matrix
multiplication
whereas
directed
models
deal
issues
like
explaining
away
makes
approximate
algorithms
often
way
affiiliated
people
peter
gehler
phd
candidate
max
planck
institute
tuebingen
alex
holub
phd
candidate
caltech
geoffrey
hinton
professor
univerity
toronto
michal
rosen
zvi
former
postdoc
uci
project
2
image
denoising
products
edgeperts
peter
gehler
max
welling
2005
products
edge
perts
nips
2005
pdf
software
one
successful
approach
image
denoising
1
transform
image
wavelet
domain
using
wavelet
pyramid
2
build
realistic
probabilistic
joint
model
wavelet
coeffcients
captures
statistical
dependencies
3
use
model
see
statistics
observed
noisy
wavelet
coefficients
differs
expect
model
4
correct
discrepencies
using
denoising
algorithm
compute
maximum
posterior
estimate
clean
wavelet
coeffcients
given
noisy
estimates
assuming
known
noise
model
5
invert
wavelet
pyramid
project
proposed
new
model
describe
statistical
regularities
wavelet
coeffcients
based
neural
network
shown
coeffcients
get
mapped
energy
serves
unnormalized
negative
log
probability
explained
paper
models
accurately
describes
heavy
tails
marginal
distributions
well
bowtie
dependencies
wavelet
coefficients
additional
feature
denoiser
will
automatically
pick
wavelet
coeffcients
modelled
jointly
one
expert
shown
results
competative
current
stat
art
affiiliated
people
peter
gehler
phd
candidate
max
planck
institute
tuebingen
project
3
bayesian
inference
structure
learning
undirected
graphical
models
sridevi
parise
max
welling
2005
learning
markov
random
fields
empirical
study
joint
statistical
meeting
jsm2005
pdf
software
max
welling
sridevi
parise
2006
bayesian
random
fields
bethe
laplace
approximation
uai
2006
pdf
sridevi
parise
max
welling
2006
bayesian
structure
scoring
markov
random
fields
nips
2006
ps
pdf
learning
structure
graphical
model
important
build
good
models
much
progress
made
infering
structure
directed
graphical
models
bayes'
nets
little
progress
undirected
graphical
models
mrfs
unidrected
bipartite
graphical
model
instance
reason
presence
normalization
constant
depends
parameters
intractable
compute
propose
combine
two
approximations
first
use
assumption
posterior
close
gaussian
distribution
makes
lot
sense
fully
observed
mrfs
likelihood
function
known
convex
parameters
second
approximation
based
algorithm
called
linear
response
propagation
compute
covariance
gaussian
approximation
turn
based
belief
propagation
finally
approximate
log
partition
function
bethe
free
energy
belief
propagation
find
good
estimates
maximum
posteriori
map
parameter
values
set
go
main
finding
approximation
indeed
accurate
fully
observed
mrfs
moreover
orders
magnitude
faster
sampling
based
methods
bottleneck
turns
good
estimate
map
parameter
values
performance
seen
crucially
depend
estimate
next
will
look
models
hidden
variables
fit
estimates
posterior
distribution
parameter
red
bars
compared
ground
truth
posterior
blue
bars
affiiliated
people
sridevi
parise
phd
candidate
uci
project
4
lifelong
learning
nonparametric
bayesian
models
project
covered
original
career
proposal
added
consent
programme
director
charge
ian
porteous
alex
ihler
padhriac
smyth
max
welling
2006
gibbs
sampling
coupled
infinite
mixture
models
stick
breaking
representation
uai
2006
pdf
yee
whye
teh
dave
newman
max
welling
2006
collapsed
variational
bayesian
inference
algorithm
latent
dirichlet
allocation
nips
2006
ps
pdf
kenichi
kurihara
max
welling
nikos
vlassis
2006
accelerated
variational
dp
mixture
models
nips
2006
ps
pdf
kenichi
kurihara
max
welling
yee
whye
teh
2007
collapsed
variational
dirichlet
process
mixture
models
ijcai
2007
ps
pdf
yee
whye
teh
kenichi
kurihara
max
welling
2007
collapsed
variational
inference
hdp
nips
2007
pdf
max
welling
ian
porteous
evgeniy
bart
2007
infinite
state
bayesian
networks
structured
domains
nips
2007
pdf
dave
newman
arthur
ascuncion
padhriac
smyth
max
welling
2007
distributed
inference
latent
dirichlet
allocation
nips
2007
pdf
ascuncion
smyth
welling
2008
asynchronous
distributed
learning
topic
models
nips
2008
pdf
porteous
ascuncion
newman
ihler
smyth
welling
2008
fast
collapsed
gibbs
sampling
latent
dirichlet
allocation
kdd
2008
pdf
max
welling
teh
kappen
2008
hybrid
variational
mcmc
inference
bayesian
networks
uai
2008
pdf
gomes
welling
perona
2008
memory
bounded
inference
topic
models
icml
2008
pdf
ian
porteous
evgeniy
bart
max
welling
2008
multi
hdp
nonparametric
bayesian
model
tensor
factorization
aaai
2008
pdf
ryan
gomes
max
welling
pietro
perona
2008
incremental
learning
nonparametric
bayesian
mixture
models
cvpr
2008
pdf
asuncion
smyth
welling
teh
2009
smoothing
inference
topic
models
uai
2009
pdf
newman
asuncion
smyth
welling
2009
distributed
algorithm
topic
models
journal
machine
learning
research
2009
pdf
porteous
asuncion
welling
2010
bayesian
matrix
factorization
side
information
dirichlet
process
mixtures
aaai
2010
pdf
asuncion
smyth
welling
2010
asynchronous
distributed
estimation
topic
models
document
analysis
statistical
methodology
2010
url
asuncion
newman
porteous
triglia
smyth
welling
2010
distributed
gibbs
sampling
latent
variable
models
bookchapter
scaling
machine
learning
cambridge
university
press
gorur
boyles
welling
2011
scalable
inference
kingman
coalescent
using
pair
similarity
aistats
2012
pdf
welling
porteous
kurihara
2012
exchangeable
inconsistent
priors
bayesian
posterior
inference
workshop
information
theory
applications
ita
2012
pdf
data
generated
society
doubling
every
year
google
currently
images
stored
single
human
will
see
lifetime
order
1
billion
images
assuming
process
approximately
one
image
per
second
big
science
projects
large
hadron
collider
lhc
large
synoptic
survey
telescope
lsst
low
frequency
array
lofar
will
generate
order
10
peta
bytes
data
year
1
peta
byte
equivalent
1
billion
books
text
industry
government
agencies
collect
large
quantities
data
form
hyperlink
clicking
patterns
purchasing
behaviour
credit
histories
surveillance
camera
video
multimedia
applications
cell
phones
digital
cameras
will
acquire
new
data
store
websites
flickr
youtube
unprecedented
scale
modern
datasets
dynamic
continuously
changing
tools
traditional
statistics
mainly
concerned
static
datasets
fixed
size
longer
adequate
instead
need
statistical
machinery
can
handle
open
ended
streams
information
necessitates
novel
learning
paradigms
adapt
complexity
statistical
model
response
amount
structure
available
data
model
simple
will
capture
predictive
structure
data
model
complex
will
fitted
noise
unpredictive
structure
data
moreover
unlike
today's
learning
algorithms
new
class
algorithms
learn
long
operation
property
will
refer
lifelong
learning
motivating
biological
example
consider
young
child
birth
will
recognize
simple
object
categories
mommies
face
representation
will
quickly
grow
complex
world
observed
estimated
age
two
years
child
learned
10
000
different
object
categories
implies
approximately
5
new
categories
every
day
average
representation
visual
world
changes
long
live
will
respond
data
deluge
presents
opportunity
well
challenge
plate
computer
scientist
promising
new
developments
respect
grid
computing
cloud
computing
already
amazon
allowing
anyone
log
network
servers
called
elastic
cloud
computing
run
processes
thousands
cpus
parallel
computing
power
grid
networks
can
exploited
algorithms
use
parallel
architecture
follows
algorithms
operate
distributed
fashion
analyze
data
locally
exchange
combine
partial
results
project
started
ambitious
goal
developing
algorithms
can
handle
large
streams
data
inference
algorithms
based
variational
approximations
collapsed
gibbs
sampling
algorithms
can
perform
inference
distributed
many
machines
allowing
scale
billions
tokens
moreover
algorithms
can
grow
response
newly
discovered
structure
data
process
information
online
fashion
essential
ingredient
allows
feature
nonparametric
bayesian
modeling
paradigm
combining
statistical
tool
bayesian
networks
fast
inference
led
us
develop
infinite
state
bayesian
network
isbn
see
figure
example
model
variables
related
variables
usual
dependency
structure
bayesian
networks
potentially
infinite
number
hidden
states
available
model
structure
data
states
alive
infinite
state
bayesian
network
affiiliated
people
ian
porteous
phd
candidate
uci
arthur
acuncion
phd
candidate
uci
kenichi
kurihara
former
phd
student
tokyo
institute
technology
yee
whye
teh
reader
university
college
london
dave
newman
project
scientist
uci
ryan
gomes
phd
candidate
caltech
padhriac
smyth
professor
uci
alex
ihler
assistant
professor
uci
pietro
perona
professor
caltech
nikos
vlassis
assistant
professor
university
crete
evgeniy
bart
former
postdoc
dilan
gorur
levi
boyles
project
5
extreme
components
analaysis
xca
max
welling
felix
agakov
chris
williams
2003
extreme
components
analysis
nips
2003
pdf
xca
statistical
technique
extends
pca
principal
component
analysis
mca
minor
components
analysis
one
probabilistic
model
xca
algorithm
extracts
optimal
combination
principal
minor
components
fit
data
addition
formulates
probabilistic
model
based
components
xca
algorithm
based
single
eigenvalue
decomposition
data
covariance
matrix
estimating
minor
components
difficult
statistical
perspective
reason
minor
components
prone
overfitting
due
sample
fluctuations
bound
directions
low
variance
recently
extended
technique
bayesian
variant
provides
principled
regularization
minor
components
can
show
bayesian
xca
model
picks
minor
components
really
present
data
dataset
landmark
points
wing
mosquito
see
figure
found
minor
components
directions
changed
landmark
positions
attached
mosquito's
body
clearly
represents
real
evolutionary
constraint
affiiliated
people
felix
agakov
chris
williams
yutian
chen
project
6
bayesian
kmeans
kenichi
kurihara
max
welling
2008
bayesian
means
maximization
expectation
algorithm
neural
computation
neural
computation
21
4
pp
1
28
pdf
max
welling
kenichi
kurihara
2005
bayesian
means
maximization
expectation
algorithm
siam
conference
data
mining
sdm2006
pdf
tech
report
software
expectation
maximization
algorithm
em
well
known
tool
find
maximum
likelihood
parameters
settings
probabilistic
model
hidden
variables
bayesian
extentions
exist
iterate
estimating
distribution
sover
hidden
variables
distributions
parameters
variational
bayesian
framework
certain
circumstances
efficient
treat
hidden
variables
point
estimates
instead
full
distributions
developed
idea
name
maximization
expectation
algorithm
still
advantage
full
bayesian
treatment
sense
overly
complex
models
penalized
provides
level
protection
overfitting
hand
point
estimates
states
hidden
variables
allow
fast
datastructures
speed
learning
example
considered
bayesian
means
still
maximize
cluster
assignments
treat
parameters
cluster
means
variances
mixture
weights
bayesian
manner
kd
trees
used
speed
learning
makes
bayesian
clustering
millions
datapoints
feasible
bonus
one
can
estimate
number
clusters
necessary
model
data
well
figure
show
philosophy
fits
general
scheme
algorithms
already
known
affiiliated
people
kenichi
kurihara
project
6
learning
dynamic
weights
welling
2009
herding
dynamic
weights
learn
icml
2009
pdf
welling
2009
herding
dynamic
weights
partially
observed
random
field
models
uai
2009
pdf
chen
welling
2010
parametric
herding
aistats
2010
pdf
welling
chen
2010
statistical
inference
using
weak
chaos
infinite
memory
proceedings
int'l
workshop
statistical
mechanical
informatics
iw
smi
2010
pdf
chen
welling
smola
2010
supersamples
kernel
herding
uai
2010
pdf
gelfand
van
der
maaten
chen
welling
2010
herding
perceptron
cycling
theorem
nips
2010
pdf
chen
gelfand
fowlkes
welling
2011
integrating
local
classifiers
nonlinear
dynamics
label
graphs
application
image
segmentation
iccv
2011
pdf
learning
markov
random
field
difficult
due
number
problems
1
compute
gradient
log
likelihood
one
needs
compute
sufficient
statistics
model
intractable
2
presence
hidden
variables
log
likelihood
highly
non
convex
function
parameters
leading
many
local
minima
learning
can
will
get
stuck
3
convergence
optimal
ml
parameter
slow
linear
almost
algorithm
used
practice
4
even
given
model
sampling
states
difficult
sampler
might
get
stuck
local
minima
state
space
considerations
led
define
deterministic
dynamical
system
performs
learning
sampling
one
go
major
advantage
system
mixes
fast
one
expect
reasonable
modes
distribution
see
figure
successive
samples
digit
4
shown
disadvantage
hard
assess
inductive
bias
represents
one
thing
can
proved
namely
average
sufficient
statisics
hold
maximum
likelihood
solution
corresponding
mrf
problem
still
hold
samples
herding
however
entropy
away
constraints
likely
maximal
intriguingly
set
allowed
weights
often
fractal
dimension
zero
lebesque
measure
one
can
also
show
entropy
production
dynamical
system
polynomial
instead
exponential
stochastic
fully
chaotic
systems
consistent
known
literature
weak
chaos
pseudochaos
currently
exploring
whether
can
formulate
generalized
maximum
entropy
principle
also
interested
design
features
make
herding
work
can
define
deep
herding
systems
many
layers
neurons
affiiliated
people
yutian
chen
andrew
gelfand
laurens
van
der
maaten
project
7
dynamical
product
models
financial
time
series
chen
welling
2010
dynamical
products
experts
modeling
financial
time
series
icml
2010
pdf
describe
dynamical
model
based
hierarchical
product
student
distribution
model
volatility
stocks
model
similar
garch
type
models
can
describe
persistence
volatility
time
form
higher
order
dependency
variances
two
consecutive
random
variables
unlike
garch
models
dependencies
stochastic
deterministic
model
can
also
considered
ica
time
show
improved
performance
predicting
value
risk
affiiliated
people
yutian
chen
project
8
statistical
optimization
korattikara
boyles
kim
park
welling
2011
statistical
optimization
nonnegative
matrix
factorization
aistats
2011
pdf
boyles
korattikara
ramanan
welling
2011
statistical
tests
optimization
efficiency
nips
2011
pdf
software
machine
learning
often
presented
choosing
loss
function
minimizing
loss
dataset
hopefully
resulting
convex
optimization
problem
view
limiting
sense
statistical
optimization
problems
ordinary
optimization
problems
due
random
nature
data
frequentist
view
world
may
imagine
sampling
datasets
observing
loss
function
fluctuates
due
resampling
one
consequences
simple
observations
course
can
overfit
words
scale
fluctuations
optimal
parameters
due
resampling
dataset
scale
precision
fit
parameters
methods
bagging
leverage
idea
much
less
exploited
fact
observations
also
allow
one
perform
optimization
loss
function
much
efficiently
particular
initial
stages
learning
every
datapoint
will
carry
roughly
information
improve
parameter
thus
model
still
bad
lot
redundancy
data
terms
change
model
result
need
query
data
items
get
good
idea
update
model
exploit
idea
performing
hypothesis
tests
every
update
inform
us
many
data
points
use
updates
precisely
probability
making
error
update
direction
90
degrees
large
will
recruit
datapoints
batch
order
increase
precision
moreover
can
pass
tests
even
recuited
data
dataset
stop
updating
altogether
lead
highly
effcient
learning
algorithm
principled
stopping
criterion
unlike
methods
stochastic
gradient
descent
basically
hyperparameters
tune
except
interpretable
threshold
large
probability
updating
wrong
direction
tolerated
affiiliated
people
levi
boyles
anoop
koratikkara
deva
ramanan
project
9
stochastic
gradient
bayesian
posterior
sampling
welling
teh
2011
bayesian
learning
via
stochastic
gradient
langevin
dynamics
icml
2011
pdf
frequentist
methods
stochastic
gradient
descent
statistical
optimization
see
project
8
efficient
quickly
finding
good
predictive
models
main
trick
realize
need
use
data
improve
model
parameters
fact
models
far
optimality
can
use
data
items
large
stepsizes
improve
models
either
decrease
stepsize
sgd
increase
batchsize
increase
resolution
updates
get
closer
optimal
solution
contrast
bayesian
mcmc
methods
sample
posterior
distribution
hopelessly
ineffcient
respect
example
single
accept
reject
step
will
inspect
data
items
twice
clearly
burn
often
nothing
stochastic
version
optimization
huge
waste
computational
resources
argue
even
sampler
reached
high
probablity
regions
can
trade
little
bit
approximation
error
bias
large
computational
wins
can
pay
back
terms
faster
mixing
reduction
error
due
variance
fact
fixed
amount
time
reduce
error
bias
often
completely
masked
sampling
variance
relevant
time
interval
allow
run
samplers
developed
new
samplers
leverage
trade
stochastic
gradient
langevin
sampler
starts
basically
stochastic
gradient
ascent
optimizer
since
add
noise
gradients
certain
point
noise
add
going
dominate
noise
create
due
subsampling
data
sampler
automatically
turns
langevin
posterior
sampler
adding
approapriate
preconditioner
can
also
make
sure
large
stepsizes
thus
fast
mixing
sampler
draws
optimal
gaussian
approximation
posterior
given
bayesian
central
limit
theorem
believe
time
start
thinking
approximate
bayesian
posterior
sampling
inherent
trade
offs
come
fact
limited
amount
time
learn
models
make
predictions
affiiliated
people
yee
whye
teh
sungjin
ahn
anoop
korattikara
back
max
welling's's
home
page
