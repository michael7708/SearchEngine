sli classes cs178 notes decisiontrees 
decision trees 
classes
group
research
publications
code
login
classes
cs178
notes
decisiontrees
decision
tree
classifiers
decision
trees
well
known
classifier
type
discrete
continuous
valued
features
one
advantage
decision
trees
produce
interpretable
decision
rules
easy
evaluate
hand
factors
went
class
decision
can
easily
stated
decision
tree
classifier
consists
sequence
comparison
nodes
single
feature
data
point
examined
continuous
valued
feature
decision
node
compares
feature
value
threshold
depending
whether
value
threshold
recurses
decision
tree
left
right
point
process
reaches
decision
node
one
possible
class
categories
output
discrete
valued
features
really
make
sense
threshold
value
instead
number
possible
options
straightforward
one
child
node
per
possible
feature
value
however
results
non
binary
tree
possibly
high
branching
factor
can
complicate
score
function
used
learning
see
discussion
duda
hart
another
possibility
keep
binary
tree
shape
case
discrete
values
assigned
left
others
right
child
datafirst
level2nd
level
learning
decision
trees
comparison
node
decision
tree
consists
selected
feature
index
threshold
comparison
typically
determine
values
parameters
simple
exhaustive
search
looping
possible
features
possible
thresholds
evaluating
score
function
picking
parameters
result
best
score
note
although
threshold
continuous
value
finite
number
possible
decisions
make
given
training
set
particular
training
data
sorted
along
feature
considered
threshold
falling
two
given
data
points
results
exactly
rule
training
data
thus
typically
indistinguishable
can
thus
enumerate
number
unique
thresholds
typically
pick
mean
two
nearest
data
points
value
score
functions
purpose
score
function
decide
good
particular
split
might
think
classification
accuracy
make
good
score
function
since
minimizing
true
goal
however
usual
classification
accuracy
particularly
well
behaved
will
often
focus
selecting
specialized
rules
try
get
one
data
point
correct
rather
trying
split
groups
data
holistic
way
also
among
rules
get
additional
data
points
correct
provides
guidance
whatsoever
one
useful
score
function
based
entropy
class
values
within
subtree
empirical
entropy
measured
bits
data
set
given
empirical
distribution
class
value
fraction
data
set
class
entropy
help
us
decide
partitioning
can
use
entropy
calculate
called
expected
information
gain
average
reduction
entropy
see
adopt
data
split
particular
suppose
split
data
set
compute
expected
information
gain
common
alternative
entropy
called
gini
index
measures
variance
class
variable
rather
entropy
gini
index
equivalents
equations
minimum
zero
variable
deterministic
single
class
within
subset
ig
measures
gain
increase
determinism
caused
conditioning
split
subsets
s1
s2
complexity
control
pruning
complexity
decision
tree
essentially
determined
depth
many
parameters
can
binary
decision
tree
depth
may
therefore
want
control
complexity
reducing
depth
common
stopping
rules
include
proceeding
past
maximum
depth
continuing
split
nodes
tree
fewer
training
data
points
associated
since
may
trust
ability
learn
general
rule
based
data
reducing
complexity
may
particularly
desirable
feel
extra
depth
significantly
improve
performance
often
hard
tell
whether
split
will
significantly
improve
performance
tree
initially
constructed
example
easy
make
examples
one
split
provides
measurable
gain
accuracy
score
allows
next
level's
split
significant
gains
reason
one
usually
constructs
entire
tree
prunes
given
full
decision
tree
start
leaves
walk
upward
checking
whether
parent
accuracy
nearly
equal
given
children
gain
threshold
prune
children
continue
upward
cease
recursing
node
ancestors
decision
stumps
decision
stump
single
layer
decision
tree
threshold
value
applied
single
feature
although
extremely
weak
learner
can
represent
extremely
simple
decision
boundaries
commonly
used
techniques
leverage
many
weak
learners
create
single
powerful
learner
ensemble
methods
related
links
information
learning
decision
trees
wikipedia
id3
algorithm
wikipedia
last
modified
february
23
2012
12
44
pm
bren
school
information
computer
science
university
california
irvine
