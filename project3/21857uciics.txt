computational statistics clustering 
methods clustering 
ics
280
spring
1999
computational
statistics
clustering
clustering
refers
several
related
problems
partitioning
set
input
points
fixed
number
closely
related
subsets
finding
small
number
representative
center
points
matching
point
distribution
family
overlapping
continuous
distributions
multiplicity
definitions
can
actually
helpful
major
clustering
algorithms
particularly
means
work
going
back
forth
partition
representative
point
view
problem
data
models
couple
different
ways
finding
data
model
clustering
naturally
perhaps
model
can
like
single
point
estimation
number
actual
data
points
data
model
chooses
among
noise
model
adds
errors
points
estimate
locations
partitioning
somehow
applying
single
point
estimation
method
subset
alternatively
data
model
can
different
overlayed
random
distributions
points
gaussians
noise
model
merely
samples
random
points
distribution
clustering
task
infer
distributions'
parameters
difference
second
model
longer
makes
sense
sharply
classify
point
part
cluster
best
one
can
estimate
likelihood
given
point
belongs
given
cluster
instance
data
form
two
crossed
gaussians
point
central
crossing
region
might
equally
likely
belong
either
distribution
despite
principles
however
many
clustering
algorithms
use
ad
hoc
ideas
choosing
partition
optimizes
functional
without
regard
fits
data
model
even
principled
approaches
means
can
viewed
based
max
likelihood
ideas
proven
converge
correct
clusters
even
find
global
max
likelihood
solution
many
clusters
use
hard
question
now
just
say
number
clusters
given
input
work
smyth
automatically
inferring
correct
number
clusters
means
like
clustering
methods
can
merge
split
clusters
hopefully
converge
something
alternatively
problem
hierarchical
clustering
can
viewed
simultaneously
solving
clustering
possible
number
clusters
means
commonly
used
clustering
method
can
seen
bayesian
max
likelihood
approach
point
model
clustering
iterative
hill
climbing
technique
guaranteed
find
global
maximum
likelihood
solution
little
can
said
theoretically
works
well
practice
can
adapted
various
different
noise
models
basic
idea
maintain
two
estimates
estimate
center
locations
cluster
separate
estimate
partition
data
points
according
one
goes
cluster
one
estimate
can
used
refine
estimate
center
locations
reasonable
prior
assumptions
max
likelihood
solution
data
point
belong
cluster
nearest
center
nearest
measured
according
distance
determined
noise
model
single
point
estimation
practice
always
euclidean
distance
therefore
set
center
locations
can
compute
new
partition
form
voronoi
diagram
centers
partition
space
regions
nearest
center
make
cluster
set
points
voronoi
cell
conversely
partition
data
points
clusters
max
likelihood
estimate
center
locations
reduces
independent
single
point
estimation
problems
likelihood
related
variance
max
likelihood
estimator
just
centroid
therefore
means
algorithms
proceeds
sequence
phases
alternates
moving
data
points
cluster
nearest
center
moving
center
points
nearest
centroid
variants
depending
whether
points
moved
one
time
batches
kanungo
et
al
kmnpsw99
looked
applying
computational
geometry
data
structures
speed
iteration
algorithm
optimal
solutions
centers
number
problems
problem's
dimension
small
clustering
may
become
amenable
exact
algorithmic
approach
general
type
problem
considered
area
partition
points
usually
two
subsets
order
minimize
function
subsets
example
two
center
problem
seeks
find
partition
two
subsets
minimizes
maximum
circumradius
one
can
come
models
max
likelihood
solution
noise
model
points
may
moved
arbitrarily
within
bounded
unknown
radius
larger
radii
less
likely
smaller
ones
seems
working
wrong
direction
one
start
noise
model
derive
algorithms
vice
versa
since
point
may
safely
clustered
nearest
circumradius
optimal
partition
formed
voronoi
diagram
two
points
just
line
early
two
center
algorithms
found
optimal
partition
testing
n2
ways
dividing
data
points
line
recently
discovered
problem
can
solved
time
polylog
sophisticated
geometric
searching
algorithms
e97
problem
minimizing
sum
distances
point
center
known
2
median
since
generalizes
one
dimensional
median
problem
minimizes
sum
distances
single
center
optimal
partition
line
fast
algorithm
exists
ther
problems
line
partition
works
therefore
fast
exact
algorithms
known
include
minimizing
sum
monotone
combination
circumradii
e92
problem
finding
partition
minimizes
sum
intra
cluster
distances
equivalently
maximizes
sum
inter
cluster
distances
known
euclidean
max
cut
partition
always
formed
circle
schulman
optimal
solution
can
found
polynomial
time
testing
n3
ways
dividing
data
points
circle
two
center
like
problems
efficient
algorithms
known
include
finding
partitions
minimize
maximum
radius
enclosing
square
size
enclosing
rectangle
area
perimeter
diameter
hs91
minimize
sum
two
cluster
diameters
h92
minimizing
maximum
width
two
clusters
as94
gks98
last
problem
can
thought
one
clustering
data
along
two
lines
rather
two
points
linfinity
criterion
used
within
cluster
approximation
np
hardness
problems
discussed
np
hard
number
clusters
variable
be96
often
possible
even
approximate
function
optimized
maximum
cluster
radius
arbitrarily
well
unless
np
however
exist
simple
greedy
approximation
algorithms
get
within
constant
factor
repeatedly
choosing
new
cluster
center
data
point
farthest
previously
chosen
centers
grouping
unchosen
points
nearest
center
will
approximate
radius
within
factor
two
unfortunately
difficult
impossible
say
much
approximating
actual
cluster
locations
rather
merely
approximating
criterion
used
find
clusters
overlayed
distributions
vaguely
recall
smyth
done
work
problem
inferring
parameters
set
overlayed
gaussians
using
kind
bayesian
approach
references
details
several
ways
defining
problem
formally
bayesian
point
view
one
attempt
find
set
distributions
maximizes
log
likelihood
log
prior
sum
log
prob
xi
log
prior
term
represents
priori
assumptions
distributions
likely
may
omitted
null
hypothesis
alternatively
one
attempt
minimize
discrepancy
maxs
measure
points
total
points
maximized
simple
class
functions
halfspaces
one
important
application
overlayed
distribution
model
belief
propagation
approach
coding
theory
one
assumes
message
bits
sent
signals
two
discrete
values
0
1
come
back
values
drawn
two
random
distributions
one
needs
estimate
distributions
order
derive
beliefs
bayesian
probabilities
measurements
0's
1's
beliefs
modified
belief
network
perform
error
correction
problem
fits
well
discrete
algorithmic
approach
since
number
data
points
message
length
high
least
simplest
cases
one
needs
find
two
clusters
one
dimensional
data
however
know
theoretical
cs
work
sort
problem
work
mentioned
finding
clustering
minimizes
maximum
cluster
width
can
also
viewed
problem
answer
consists
overlayed
distributions
two
infinite
strips
together
cover
point
set
geometric
sampling
one
way
view
sampling
way
finding
representative
subset
data
may
ok
even
desired
large
cluster
actual
data
represented
several
centers
cluster
output
possible
large
cluster
data
points
missing
representative
can
formalized
terminology
geometric
sampling
area
originally
developed
statisticians
one
used
extensively
within
computational
geometry
technique
deriving
deterministic
algorithms
randomized
ones
since
cluster
centers
derived
theory
behave
many
ways
like
random
samples
define
range
space
consist
universe
objects
often
always
points
rd
together
family
subsets
instance
halfspaces
rd
call
set
range
finite
set
necessarily
positive
value
epsilon
define
epsilon
net
subset
range
intersect
epsilon
intersect
must
nonempty
words
touches
large
ranges
similarly
define
epsilon
approximation
subset
range
epsilon
intersect
intersect
epsilon
words
covers
approximately
fraction
set
always
epsilon
approximation
epsilon
approximation
always
epsilon
net
important
result
range
spaces
many
important
geometric
examples
one
can
find
much
smaller
epsilon
nets
epsilon
approximations
fact
size
net
approximation
can
made
constant
independent
specifically
let
fs
denote
family
sets
formed
intersecting
ranges
define
scaffold
dimension
range
space
maximum
log
fs
log
maximized
finite
sets
instance
consider
sets
points
plane
closed
halfspace
ranges
halfspace
containing
one
point
pass
pair
points
range
contains
points
size
fs
can
1
1
little
care
one
can
see
fewer
n2
sets
therefore
scaffold
dimension
range
space
st
two
general
safe
think
scaffold
dimension
roughly
number
points
needed
determine
range
scaffold
dimension
circles
three
scaffold
dimension
ellipses
five
also
alternate
definition
dimension
range
space
called
vapnik
chervonenkis
dimension
bounded
iff
scaffold
dimension
bounded
proofs
involving
vc
dimension
can
expressed
directly
terms
scaffold
dimension
theorem
set
range
space
scaffold
dimension
exists
epsilon
net
size
epsilon
log
epsilon
epsilon
approximation
size
epsilon2
log
epsilon
proof
begins
observing
random
sample
size
1
epsilon
log
fs
1
epsilon2
log
fs
constant
probability
epsilon
net
epsilon
approximation
respectively
also
epsilon
2
approximation
epsilon
2
approximation
epsilon
approximation
induction
one
can
assume
small
epsilon
2
approximation
take
random
sample
get
constant
probability
even
smaller
epsilon
approximation
result
epsilon
approximations
proven
one
can
take
random
sample
small
epsilon
approximation
get
small
epsilon
net
although
construction
randomized
exist
deterministic
algorithms
constructing
epsilon
nets
epsilon
approximations
time
bounds
form
whenever
1
epsilon
bounded
1
citation
filled
later
reasonable
use
epsilon
net
range
space
disks
ellipses
form
clustering
definitions
imply
every
disk
shaped
elliptical
cluster
epsilon
points
representative
mentioned
epsilon
nets
epsilon
approximations
used
extensively
geometric
algorithms
example
statistical
application
consider
regression
depth
problem
appropriate
range
space
family
double
wedges
bounded
one
vertical
one
non
vertical
hyperplane
regression
depth
given
hyperplane
measured
fraction
size
overall
data
set
within
epsilon
regression
depth
hyperplane
relative
epsilon
approximation
data
therefore
linear
time
preprocessing
computing
epsilon
approximation
can
compute
good
approximations
regression
depth
constant
time
deepest
plane
respect
approximation
must
within
epsilon
deepest
plane
overall
data
set
can
approximate
depth
deepest
plane
within
1
epsilon
factor
linear
total
time
sw98
types
clustering
problems
recent
interest
problems
robust
separation
partitioned
points
two
clusters
somehow
maybe
binary
classification
already
given
input
quickly
can
find
boundary
plane
separates
one
side
way
maximizes
distance
point
boundary
equivalent
finding
closest
pair
points
convex
hulls
two
sides
note
closest
pair
data
points
since
separating
plane
perpendicular
bisector
segment
two
points
problem
can
solved
linear
time
lp
type
methods
g95
similar
used
smallest
enclosing
ball
aware
implementation
ideas
perhaps
rtner's
miniball
code
g99
adapted
problem
another
clustering
like
problem
find
single
cluster
among
set
points
ignoring
less
tightly
clustered
points
essentially
discussed
already
robust
single
point
estimation
emphasis
different
might
allow
many
2
outliers
algorithmically
one
can
optimize
case
number
points
cluster
small
next
hierarchical
clustering
david
eppstein
theory
group
dept
information
computer
science
uc
irvine
last
update
18
nov
1999
11
50
11
pst
