computational statistics hierarchical clustering 
methods hierarchical clustering 
ics
280
spring
1999
computational
statistics
hierarchical
clustering
hierarchical
clustering
refers
formation
recursive
clustering
data
points
partition
two
clusters
hierarchically
clustered
one
way
draw
kind
system
nested
subsets
maximal
sense
one
identify
additional
subsets
without
violating
nesting
alternatively
one
can
draw
dendrogram
binary
tree
distinguished
root
data
items
leaves
conventionally
leaves
shown
level
drawing
ordering
leaves
arbitrary
horizontal
position
heights
internal
nodes
may
arbitrary
may
related
metric
information
used
form
clustering
data
models
data
clustering
problem
may
consist
points
euclidean
vector
space
structured
objects
dna
sequences
case
hierarchical
clustering
problem
essentially
equivalent
reconstructing
evolutionary
trees
also
known
phylogeny
however
many
clustering
algorithms
assume
simply
input
given
distance
matrix
distances
may
may
define
metric
one
popular
data
model
data
form
ultrametric
archimedean
metric
special
type
metric
distances
satisfy
ultrametric
triangle
inequality
dist
max
dist
dist
inequality
satisfied
instance
data
points
leaves
dendrogram
drawing
distance
defined
height
least
common
ancestor
fact
just
equivalent
way
defining
ultrametric
inequality
also
satisfied
vertices
graph
length
path
defined
maximum
weight
edge
ultrametric
requirement
may
strong
evolutionary
trees
one
measures
distance
mutation
rate
imply
biologically
unrealistic
condition
species
evolve
rate
weaker
condition
distances
formed
path
lengths
tree
without
ultrametric
requirement
root
leaf
path
equal
length
distance
function
meeting
weaker
requirement
also
known
additive
metric
data
model
data
points
form
ultrametric
input
clustering
algorithm
distance
matrix
typical
noise
model
values
matrix
independently
perturbed
random
distribution
additional
data
observations
may
consist
new
points
ultrametric
large
population
genetic
studies
used
test
hypothesis
human
species
evolved
africa
migrating
continents
cite
may
used
reduce
perturbation
existing
distance
measurements
comparing
larger
amounts
dna
sequence
information
another
common
data
noise
model
dna
sequence
input
cavender
farris
model
c78
data
parameters
consist
ultrametric
dendrogram
edge
lengths
together
prototype
sequence
stored
root
dendrogram
observed
data
model
represent
contents
single
position
dna
sequence
species
formed
starting
symbol
root
dendrogram
propagating
value
downwards
mutation
rates
proportional
dendrogram
edge
lengths
dynamic
programming
methods
dynamic
programming
may
used
find
parsimonious
tree
given
set
sequences
dendrogram
internal
vertex
labeled
sequence
constructed
algorithm
leaves
labeled
input
sequences
total
amount
mutation
sequences
adjacent
tree
minimized
however
computational
complexity
prohibitive
typically
something
like
nk
sequence
length
number
sequences
method
may
always
converge
correct
tree
cavender
ferris
model
rice
warnow
show
rw97
performs
well
practice
another
dynamic
program
arises
problem
fitting
optimal
dendrogram
distance
matrix
one
particular
dendrogram
mind
abstract
tree
problem
assigning
heights
vertices
minimize
maximum
difference
dendrogram
distance
given
distance
matrix
simply
linear
program
height
linearly
constrained
children
within
distance
represented
matrix
since
linear
program
two
variables
per
inequality
can
solved
efficiently
cite
however
difficult
part
procedure
choosing
dendrogram
use
number
possible
different
dendrograms
leaves
2k
1
1
3
5
7
2k
1
since
2k
1
ways
adding
kth
leaf
1
leaf
dendrogram
unless
small
one
hope
try
one
possibly
find
optimal
solution
somewhat
larger
still
large
dynamic
programming
method
one
finds
optimal
dendrogram
subset
data
points
trying
ways
partitioning
subset
two
smaller
subsets
local
improvement
methods
one
compute
global
optimum
one
can
least
achieve
local
optimum
start
tree
repeatedly
use
dynamic
programming
re
optimize
small
subtrees
idea
can
used
get
1
epsilon
factor
approximation
parsimony
cite
bottom
methods
large
family
clustering
algorithms
work
follows
choose
measure
affinity
clusters
start
clusters
consisting
single
point
repeatedly
merge
two
clusters
highest
affinity
single
supercluster
differences
algorithms
involve
definition
affinity
implementation
details
note
even
points
metric
space
affinity
need
satisfy
triangle
inequality
nice
properties
affinity
two
clusters
defined
distance
closest
pair
points
problem
known
single
linkage
clustering
essentially
solved
minimum
spanning
tree
top
level
clustering
formed
removing
heaviest
edge
mst
remaining
levels
formed
manner
recursively
since
minimum
spanning
trees
can
often
found
efficiently
can
clustering
produce
correct
results
input
ultrametric
distance
matrix
without
errors
however
points
euclidean
spaces
tends
produce
unsatisfactory
long
stringy
clusters
another
common
affinity
function
average
distance
points
cluster
complete
linkage
clustering
distance
matrix
input
can
found
replacing
two
rows
matrix
appropriate
weighted
average
alternatively
one
good
notion
single
point
estimation
cluster
centroid
one
can
define
affinity
terms
distance
cluster
centers
neighbor
joining
sn87
complicated
affinity
assumes
one
tree
point
connected
common
center
measures
amount
total
edge
length
tree
reduced
placing
steiner
point
two
points
connecting
steiner
point
center
formally
affinity
distance
center
distance
center
distance
sp
distance
sp
distance
sp
center
sp
steiner
point
distance
matrix
input
center
found
averaging
rows
whole
matrix
distance
center
just
average
distance
point
similarly
steiner
point
can
found
averaging
rows
case
two
terms
distance
sp
distance
sp
add
distance
atteson
a96
showed
method
converges
correct
tree
distance
matrices
sufficiently
close
terms
linfinity
distance
tree
metric
many
methods
efficient
algorithms
distance
matrix
input
e98
points
low
dimensional
euclidean
spaces
kl95
however
neighbor
joining
seems
difficult
best
known
time
bound
n3
commonly
available
implementations
taking
even
however
even
faster
methods
slow
data
consisting
millions
points
moderate
high
dimensional
euclidean
spaces
top
methods
one
can
also
form
hierarchical
clusterings
top
following
definition
use
favorite
nonhierarchical
clustering
algorithm
find
partition
input
two
clusters
continue
recursively
two
clusters
advantage
methods
speed
can
scale
problem
sizes
beyond
bottom
methods
however
quality
results
may
poorer
important
decisions
made
early
algorithm
accumulated
enough
information
make
well
also
emphasis
speed
nonhierarchical
clustering
methods
used
tend
primitive
split
points
axis
parallel
hyperplane
incremental
methods
incremental
hierarchical
clustering
methods
can
even
faster
top
approach
methods
build
hierarchy
one
point
time
without
changing
existing
hierarchy
add
new
point
simply
trace
root
step
choosing
child
cluster
best
contains
given
point
reach
cluster
containing
single
point
split
two
smaller
clusters
one
point
one
new
point
extremely
fast
good
applications
nearest
neighbor
classification
good
accurately
reconstructing
clusters
present
original
data
model
numerical
taxonomy
cavalli
sforza
edwards
ce67
introduced
problem
finding
taxonomy
finding
nearest
tree
metric
ultrametric
given
distance
matrix
course
one
define
nearest
natural
choices
seem
l1
l2
linfinity
metrics
distance
matrix
coordinates
l1
l2
problems
np
complete
tree
metric
ultrametric
variants
d87
however
linfinity
nearest
ultrametric
can
found
time
linear
distance
matrix
size
fkw95
agarwala
et
al
abfnpt98
show
np
complete
get
within
factor
9
8
linfinity
nearest
tree
metric
however
describe
fast
approximation
algorithm
achieves
factor
three
also
work
within
theoretical
computer
science
community
motivated
approximation
algorithms
various
network
design
problems
approximating
distance
matrix
tree
metric
minimum
dilation
stretch
maximum
ratio
original
distance
tree
distance
main
result
area
one
can
define
random
distribution
trees
expected
dilation
given
pair
vertices
log
log
log
ccggp98
seem
mean
much
finding
individual
tree
optimal
nearly
optimal
dilation
imply
one
can
find
tree
average
dilation
log
log
log
gu
noche
g96
proposed
way
measuring
nearness
given
distance
matrix
tree
metric
depends
ordering
among
distances
much
exact
distance
values
sense
distance
free
similarly
centerpoint
regression
depth
methods
single
point
estimation
linear
regression
defines
order
distance
delta
number
pairs
disagree
ordering
distances
dist
dist
dist
dist
pair
counted
half
case
equality
suggests
applying
incremental
method
using
delta
instead
original
distance
next
references
david
eppstein
theory
group
dept
information
computer
science
uc
irvine
last
update
06
may
1999
11
43
20
pdt
