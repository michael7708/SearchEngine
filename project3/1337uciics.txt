michael j pazzani students affiliated researchers 
affiliated students 
michael
pazzani
research
group
graduate
students
ph
graduates
stephen
bay
daniel
billsus
eamonn
keogh
james
wogulis
approach
repairing
evaluating
first
order
theories
containing
multiple
concepts
negation
1
2m
dissertation
addresses
problem
theory
revision
machine
learning
task
requires
learner
minimally
revise
initial
incorrect
theory
revised
theory
explains
given
set
training
data
learning
system
a3
presented
solves
task
main
contributions
dissertation
include
learning
system
a3
can
revise
theories
containing
multiple
concepts
expressed
function
free
first
order
horn
clauses
approach
repairing
theories
containing
negation
introduction
distance
metric
theories
evaluate
degree
revision
performed
experimental
evidence
presented
demonstrates
a3's
ability
solve
theory
revision
task
assumptions
commonly
made
approaches
theory
revision
whether
theory
needs
generalized
specialized
respect
misclassified
examples
shown
incorrect
theories
containing
negation
a3
able
repair
theories
containing
negation
demonstrates
simple
general
approach
identifying
types
errors
theory
using
single
mechanism
handling
positive
negative
examples
well
examples
multiple
concepts
syntactic
distance
two
theories
proposed
evaluation
metric
theory
revision
systems
distance
defined
terms
minimum
number
edit
operations
required
transform
one
theory
another
allows
precise
measurement
much
theory
revised
allows
comparison
different
systems'
abilities
perform
minimal
revisions
distance
metric
also
used
a3
order
bias
towards
finding
minimal
revisions
accurately
explain
data
distance
metric
also
leads
insights
theory
revision
task
particular
shown
theory
revision
task
underconstrained
additional
goal
learning
particular
correct
theory
met
without
additional
constraints
potentially
many
accurate
revisions
far
apart
syntactically
shown
providing
examples
multiple
concepts
theory
can
provide
constraints
david
schulenburg
learning
using
context
connectionist
model
language
understanding
2
5m
natural
languages
ambiguous
especially
true
non
literal
figurative
constructs
metaphors
indirect
speech
acts
even
literal
text
suffers
problems
ambiguity
exemplified
text
containing
words
multiple
meanings
understanding
ambiguous
text
fairly
simple
task
us
humans
well
recognized
context
often
used
people
aid
resolution
problems
ambiguity
dissertation
presents
discussion
computational
model
pip
designed
address
issue
ambiguity
natural
language
understanding
incorporating
textual
context
understanding
process
pip
able
disambiguate
text
containing
ambiguous
constructs
metaphors
indirect
speech
acts
furthermore
entire
understanding
process
uniform
sense
exact
mechanisms
used
process
literal
non
literal
ambiguous
text
special
processing
``rules''
necessary
deal
ambiguity
underlying
computational
model
pip
feed
forward
artificial
neural
network
incorporation
textual
context
accomplished
recurrent
relation
context
constructed
network
input
processing
words
text
fashion
pip
able
use
context
directly
processes
text
pip
demonstrated
effectiveness
sets
text
include
ambiguous
lexemes
metaphors
indirect
speech
acts
using
context
constructed
earlier
sentences
texts
pip
able
derive
intended
meaning
ambiguous
sentences
end
texts
shown
without
use
context
pip
unable
produce
intended
meaning
many
cases
decide
meaning
give
ambiguous
sentences
kamal
ali
learning
probabilistic
relational
concept
descriptions
1
6m
dissertation
presents
results
area
multiple
models
multiple
classifiers
learning
probabilistic
relational
first
order
rules
noisy
real
world
data
reducing
small
disjuncts
problem
problem
whereby
learned
rules
cover
training
examples
high
error
rates
test
data
several
results
also
presented
arena
multiple
models
multiple
models
approach
relevant
problem
making
accurate
classifications
``real
world''
domains
since
facilitates
evidence
combination
needed
accurately
learn
domains
also
useful
learning
small
training
data
samples
many
models
appear
equally
good
given
evaluation
metric
models
often
quite
varying
error
rates
test
data
situations
single
model
method
problems
increasing
search
partly
addresses
problem
whereas
multiple
models
approach
potential
much
useful
important
result
multiple
models
research
amount
error
reduction
afforded
multiple
models
approach
linearly
correlated
degree
individual
models
make
errors
uncorrelated
manner
work
first
model
degree
error
reduction
due
use
multiple
models
also
shown
possible
learn
models
make
less
correlated
errors
domains
many
ties
search
evaluation
metric
learning
third
major
result
research
multiple
models
realization
models
learned
make
errors
negatively
correlated
manner
rather
make
errors
uncorrelated
statistically
independent
manner
thesis
also
presents
results
learning
probabilistic
first
order
rules
relational
data
shown
learning
class
description
class
data
one
per
class
approach
attaching
probabilistic
estimates
learned
rules
allows
accurate
classifications
made
real
world
data
sets
thesis
presents
system
hydra
implements
approach
shown
resulting
classifications
often
accurate
made
three
existing
methods
learning
noisy
relational
data
furthermore
learned
rules
relational
expressive
attribute
value
rules
learned
induction
systems
finally
results
presented
small
disjuncts
problem
rules
apply
rare
subclasses
high
error
rates
thesis
presents
first
approach
simultaneously
successful
reducing
error
rates
small
disjucnts
also
reducing
overall
error
rate
statistically
significant
margin
previous
approach
aimed
reduce
small
disjunct
error
rates
expense
increasing
error
rates
large
disjuncts
shown
one
per
class
approach
reduces
error
rates
rare
rules
sacrificing
error
rates
rules
cliff
brunk
investigation
knowledge
intensive
approaches
concept
learning
theory
refinement
1
6m
concept
learning
algorithms
used
solve
difficult
problems
fields
ranging
medical
diagnosis
astronomy
spite
successful
application
concept
learners
able
utilize
knowledge
expressed
form
set
training
examples
relevant
knowledge
sources
can
utilized
even
available
evidence
presented
using
knowledge
sources
leads
accurate
learned
models
dissertation
investigation
techniques
utilizing
knowledge
form
approximate
theory
facilitate
concept
learning
techniques
explored
divided
two
classes
theory
guided
learning
algorithms
use
approximate
theory
constrain
search
new
concept
description
theory
revision
algorithms
attempt
repair
approximate
theory
techniques
accurate
approaches
ignore
information
contained
approximate
theory
experimental
evidence
indicates
theory
revision
algorithms
produce
accurate
models
furthermore
models
structurally
similar
approximate
theory
ideal
theory
produced
theory
guided
learning
algorithms
main
contributions
dissertation
include
new
approach
theory
guided
learning
conceptual
framework
comparing
evaluating
theory
revision
algorithms
enhanced
techniques
identifying
repairing
errors
within
theory
lexically
enhanced
approach
evaluating
repairs
lexically
enhanced
theory
revision
novel
technique
utilizing
previously
unused
form
knowledge
contained
approximate
theory
technique
uses
lexical
information
contained
term
names
approximate
theory
prefer
repairs
lexically
coherent
evidence
indicates
reduces
structural
difference
revised
theory
ideal
theory
christopher
merz
classification
regression
combining
models
two
novel
methods
combining
predictors
introduced
thesis
one
task
regression
task
classification
goal
combining
predictions
set
models
form
improved
predictor
dissertation
demonstrates
combining
scheme
can
rely
stability
consensus
opinion
time
capitalize
unique
contributions
model
empirical
evaluation
reveals
new
methods
consistently
perform
well
better
existing
combining
schemes
variety
prediction
problems
success
algorithms
explained
empirically
analytically
demonstrating
adhere
set
theoretical
heuristic
guidelines
byproduct
empirical
investigation
evidence
existing
combining
methods
fail
satisfy
one
guidelines
defined
new
combining
approaches
satisfy
criteria
relying
upon
singular
value
decomposition
tool
filtering
redundancy
noise
predictions
learn
models
characterizing
areas
example
space
model
superior
svd
based
representation
used
new
combining
methods
aids
avoiding
sensitivity
correlated
predictions
without
discarding
learned
models
therefore
unique
contributions
model
can
still
discovered
exploited
added
advantage
combining
algorithms
derived
dissertation
limited
models
generated
single
algorithm
may
applied
model
sets
generated
diverse
collection
machine
learning
statistical
modeling
methods
three
main
contributions
dissertation
introduction
two
new
combining
methods
capable
robustly
combining
classification
regression
estimates
applicable
broad
range
model
sets
depth
analysis
revealing
new
methods
address
specific
problems
encountered
combining
multiple
learned
models
detailed
account
existing
combining
methods
assessment
fall
short
criteria
combining
approaches
michael
pazzani
department
information
computer
science
university
california
irvine
irvine
ca
92697
3425
pazzani
ics
uci
edu
