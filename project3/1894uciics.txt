learning sets related concepts a shared task model 
learning sets related concepts a shared task model 
learning
sets
related
concepts
shared
task
model
tim
hume
ics
dept
university
california
irvine
irvine
ca
92717
hume
interplay
com
michael
pazzani
ics
dept
university
california
irvine
irvine
ca
92717
pazzani
ics
uci
edu
949
824
5888
http
www
ics
uci
edu
dir
faculty
ai
pazzani
abstract
investigate
learning
set
causally
related
concepts
examples
show
human
subjects
make
fewer
errors
learn
rapidly
set
concepts
logically
consistent
compare
results
subjects
subjects
learning
equivalent
concepts
share
sets
relevant
features
logically
consistent
present
shared
task
neural
network
model
simulation
psychological
experimentation
introduction
researchers
investigated
relevant
background
knowledge
learner
influences
speed
accuracy
concept
learning
murphy
medin
1985
nakamura
1985
pazzani
1991
wattenmaker
et
al
1986
however
psychological
investigation
date
explored
problems
subjects
learn
single
concept
relevant
background
knowledge
either
brought
experiment
subject
given
written
instructions
contrast
research
machine
learning
addressed
issues
occur
learning
set
related
concepts
example
relevant
background
concepts
might
learned
inductively
examples
learning
concepts
depend
upon
knowledge
pazzani
1990
report
two
experiments
subjects
induce
relevant
background
knowledge
examples
use
background
knowledge
facilitate
later
learning
experiments
illustrate
importance
learning
relevance
combinations
features
rather
individual
features
model
experiment
shared
task
neural
networks
caruana
1993
first
experiment
subjects
first
induce
relevant
background
knowledge
opportunity
use
knowledge
later
learning
closely
simulate
real
world
ran
second
experiment
wherein
subjects
induce
relevant
background
knowledge
time
learning
concept
depends
knowledge
experiments
subjects
divided
two
groups
one
group
feature
consistency
group
learned
complex
concept
shared
relevant
features
previously
learned
related
concepts
logically
consistent
concepts
another
group
logical
consistency
group
learned
complex
concept
logically
consistent
previously
learned
related
concepts
initial
psychological
experimentation
first
experiment
subjects
asked
imagine
work
us
forest
service
assigned
task
learning
predict
years
severe
risk
forest
fire
danger
fall
four
concepts
learned
experiment
one
concept
four
phases
subjects
learned
3
background
concepts
phases
1
3
phase
4
divided
two
groups
logical
consistency
group
feature
consistency
group
learn
one
two
separate
concepts
depended
background
concepts
first
phase
experiment
designed
minimize
effects
subjects'
domain
specific
pre
existing
theories
every
subject
learn
concept
first
phase
subjects
learn
severe
risk
forest
fires
fall
given
data
rain
spring
summer
example
data
shown
figure
1
subjects
given
data
indicated
severe
risk
forest
fires
fall
wet
spring
dry
summer
rule
consistent
knowledge
people
live
southern
california
remaining
phases
measure
learning
rate
number
errors
made
subjects
novel
stimuli
used
features
insure
knowledge
acquired
experiment
figure
1
example
abstract
feature
stimuli
used
first
concept
next
subjects
told
us
forest
service
needs
advance
planning
wait
end
summer
predict
will
severe
risk
fire
fall
subjects
examined
data
several
years
time
however
data
five
simulated
scientific
instruments
used
january
detect
presence
factors
may
useful
predicting
amount
rain
one
instruments
detects
presence
particular
factor
displays
distinctive
graph
shown
figure
2
otherwise
bar
shown
mark
absence
instrument's
graph
see
instrument
3
figure
2
instrument
displays
graph
whose
shape
differs
instruments
second
concept
learning
problem
subjects
learn
predict
instrument
readings
rainy
spring
subjects
given
data
indicated
wet
spring
one
particular
instrument
showed
distinctive
graph
subjects
learned
rule
form
will
wet
spring
instrument
displays
graph
instrument
corresponding
instrument
selected
randomly
concept
will
serve
background
knowledge
learning
fourth
concept
figure
2
example
stimuli
used
second
third
fourth
concepts
third
concept
learning
problem
subjects
learned
another
piece
background
knowledge
subjects
learn
predict
instrument
readings
dry
summer
subjects
shown
data
derived
rule
will
dry
summer
instrument
instrument
displays
graph
fourth
final
concept
learning
problem
subjects
learn
predict
instrument
readings
severe
risk
fire
fall
concepts
1
3
served
background
knowledge
concept
subjects
logical
consistency
group
given
data
indicated
severe
risk
fire
instrument
displayed
graph
either
instrument
instrument
displayed
graph
concept
logically
consistent
first
three
concepts
learned
subjects
feature
consistency
group
given
data
indicated
severe
risk
fire
instrument
displayed
graph
either
instrument
instrument
displayed
graph
although
consistent
concepts
learned
concept
shares
relevant
features
logical
consistency
concept
subjects
subjects
18
male
female
undergraduates
attending
university
california
irvine
participated
experiment
receive
extra
credit
introductory
psychology
course
stimuli
stimuli
consisted
data
displayed
computer
monitor
first
concept
since
two
two
valued
features
4
distinct
stimuli
constructed
remaining
three
concepts
32
distinct
stimuli
since
five
two
valued
features
stimuli
presented
random
order
subject
procedures
subject
shown
data
computer
single
year
asked
make
prediction
whether
severe
risk
fire
fall
clicking
circle
next
word
yes
circle
next
word
using
mouse
move
pointer
circle
pressing
button
mouse
next
subject
clicked
box
labeled
check
answer
still
displaying
data
computer
indicated
subject
whether
answer
correct
answer
subject's
answer
correct
subject
click
box
labeled
continue
data
another
year
shown
otherwise
selected
different
answer
clicked
check
answer
process
repeated
subjects
performed
level
ensured
learned
accurate
approximation
concept
making
one
error
sequence
24
consecutive
trials
subjects
allowed
much
time
wanted
make
prediction
view
data
correct
answer
shown
process
learning
concept
criteria
repeated
four
concepts
learned
recorded
number
last
trial
subject
made
error
total
number
errors
made
subject
concept
number
made
block
16
trials
subject
obtain
correct
answer
96
trials
recorded
last
error
made
trial
96
results
subjects
logical
consistency
group
required
average
27
6
trials
learn
fourth
concept
subjects
feature
consistency
group
required
average
50
4
trials
16
1
91
05
subjects
logical
consistency
group
made
average
6
8
errors
subjects
feature
consistency
group
made
average
14
0
errors
16
2
135
05
multiple
concept
learning
experiment
1
subjects
accurately
induced
three
relevant
background
concepts
prior
learning
single
concept
depended
upon
concepts
order
concepts
ideal
order
subjects
first
acquire
knowledge
inductively
use
knowledge
future
learning
however
natural
world
benevolent
teacher
orders
experiences
learner
closely
simulate
natural
world
second
experiment
concepts
stimuli
first
experiment
last
three
concepts
learned
time
presentation
stimuli
subjects
predicted
whether
rainy
spring
dry
summer
severe
risk
fire
fall
see
figure
3
exception
experiment
2
identical
experiment
1
second
learning
phase
subjects
click
three
boxes
correctly
proceeding
next
stimuli
recorded
number
last
trial
subject
made
error
total
number
errors
made
subject
concept
involved
predicting
whether
severe
risk
fire
fall
instrument
data
addition
concept
also
recorded
number
errors
made
subject
blocks
16
trials
subject
obtain
correct
answer
128
trials
recorded
last
error
made
trial
128
figure
3
example
stimuli
used
second
phase
experiment
2
results
subjects
logical
consistency
group
required
average
77
8
trials
predict
whether
severe
risk
fire
fall
instrument
data
subjects
feature
consistency
group
required
average
109
9
trials
16
1
81
05
addition
subjects
logical
consistency
group
made
average
29
3
errors
subjects
feature
consistency
group
made
average
41
4
errors
last
figure
marginally
significant
16
1
41
1
results
demonstrate
simultaneously
learning
set
related
concepts
easier
concepts
logically
consistent
concepts
merely
share
set
relevant
features
figure
4
graphs
percentage
errors
made
two
groups
predicting
severe
risk
fire
fall
instrument
data
function
number
trials
shows
subjects
logical
consistency
feature
consistency
groups
perform
similarly
trial
64
point
subjects
logical
consistency
group
make
fewer
errors
feature
consistency
group
figure
4
mean
percentage
errors
made
subjects
logical
consistency
feature
consistency
groups
function
trial
experiment
2
discussion
three
findings
note
experiments
first
subjects
logical
consistency
condition
make
fewer
errors
require
fewer
trials
learn
finding
agrees
intuition
people
learn
previous
experiments
involving
background
knowledge
subjects
learn
background
knowledge
furthermore
current
cognitive
models
perform
manner
quantitative
data
background
knowledge
learned
inductively
influences
learning
rate
number
errors
made
learners
second
learning
relevance
individual
features
account
findings
wisniewski
medin
1994
use
term
selection
models
refer
learning
models
use
prior
knowledge
determine
features
relevant
lien
cheng
1989
present
one
model
selection
models
able
explain
results
since
logical
consistency
feature
consistency
groups
learn
concepts
relevant
features
third
although
subjects
logical
consistency
group
learn
faster
make
fewer
errors
subjects
feature
consistency
group
learner
slower
make
errors
predicted
existing
computational
models
influence
prior
knowledge
explanation
based
learning
ebl
mitchell
et
al
1986
ebl
machine
learning
method
derives
concepts
background
knowledge
first
might
seem
ebl
serve
ideal
model
use
prior
knowledge
learning
inputs
correspond
exactly
items
learned
phases
1
3
first
experiment
output
correspond
exactly
concept
learned
phase
4
however
several
problems
ebl
model
human
learning
first
ebl
algorithms
learn
quickly
logical
consistency
subjects
since
fourth
concept
can
deductively
derived
preceding
three
ebl
make
errors
data
second
ebl
function
unless
background
knowledge
complete
example
ebl
acquire
concepts
phases
2
3
since
just
associations
stimuli
weather
predictions
modeling
shared
task
networks
propose
model
psychological
experiments
using
multi
layer
neural
networks
trained
error
backpropagation
rumelhart
et
al
1986
learn
multiple
concepts
first
like
make
distinction
sub
task
learning
shared
task
learning
sub
task
learning
concepts
learned
serve
background
concepts
concepts
learned
instance
poker
learning
hands
two
pair
one
pair
sub
task
problem
one
pair
background
concept
learning
two
pair
shared
task
learning
hand
involves
learning
concepts
share
subordinate
concepts
example
learning
hands
two
pair
full
house
require
knowing
one
pair
two
pair
full
house
require
knowledge
network
diagrammed
left
side
figure
5
shows
typical
way
using
networks
learn
sub
task
concepts
network
applied
experiment
1
please
note
order
make
diagrams
comprehensible
connections
nodes
drawn
actual
network
nodes
hidden
layer
connected
input
output
nodes
network
first
learns
section
enclosed
solid
line
two
inputs
analogous
abstract
features
shown
subjects
first
phase
experiment
output
network's
guess
whether
will
severe
risk
fire
fall
second
network
trained
section
enclosed
dashed
line
represents
learning
wet
spring
concept
five
inputs
left
represent
five
instrument
displays
shown
human
subjects
output
network's
prediction
whether
will
wet
spring
third
dry
summer
concept
trained
network
section
enclosed
dotted
line
five
inputs
used
used
learn
previous
concept
output
network's
guess
whether
will
dry
summer
wet
spring
dry
summer
concepts
sub
tasks
network
learns
final
fire
fall
concept
represented
training
testing
entire
network
network
uses
five
inputs
decide
will
severe
risk
fire
fall
figure
5
neural
network
diagrams
network
left
sub
task
learning
model
network
right
shared
task
learning
model
system
kbann
towell
et
al
1990
set
network
like
one
left
side
figure
5
given
symbolic
inferences
rules
represent
knowledge
acquired
first
three
phases
experiment
problem
method
modeling
experiment
since
network
already
trained
three
background
concepts
require
training
learn
final
concept
logical
consistency
group
experiment
problem
ebl
suffers
caruana
1993
done
work
shared
task
learning
using
networks
one
hidden
layer
network
right
figure
5
representation
network
major
advantage
model
hidden
layer
can
create
new
features
can
shared
output
units
model
first
experiment
network
first
uses
five
inputs
wet
spring
output
unit
trained
receives
feedback
performance
second
five
inputs
used
dry
summer
output
unit
trained
third
fire
fall
output
unit
trained
tested
using
five
inputs
performed
experiments
shared
task
neural
nets
see
model
results
psychological
experiments
since
appeared
method
learn
combinations
features
addition
feature
relevancy
networks
might
also
able
combine
features
store
combination
network
just
stores
learned
knowledge
experiments
first
phase
used
2
abstract
features
stimuli
later
phases
used
5
instrument
displays
since
network
learn
concepts
different
forms
inputs
trained
first
phase
however
network
can
used
learn
phases
experiments
model
sequential
experiment
experiment
1
network
first
uses
5
inputs
wet
spring
output
unit
trained
receives
feedback
performance
second
5
inputs
used
dry
summer
output
unit
trained
third
fire
fall
output
unit
trained
tested
using
5
inputs
modeling
simultaneous
experiment
experiment
2
done
training
3
output
nodes
time
using
fire
fall
unit
testing
logical
form
data
used
psychological
experiments
first
output
unit
value
one
one
random
feature
say
value
1
second
output
unit
value
1
either
two
randomly
selected
features
say
values
1
model
logical
consistency
group
third
output
unit
value
1
feature
value
1
either
feature
feature
value
1
network
used
feed
forward
system
one
layer
20
hidden
units
generalized
delta
rule
used
training
logistic
function
used
activation
testing
network
output
value
greater
0
5
treated
1
value
0
5
treated
0
model
forced
guessing
applied
human
subjects
momentum
set
0
90
learning
rate
set
0
25
model
experiment
1
trained
network
sequentially
learn
3
concepts
wet
spring
dry
summer
fire
fall
first
trained
network
learn
example
positive
example
wet
spring
concept
first
output
unit
value
1
function
5
features
epoch
training
data
network
tested
see
correctly
predict
value
first
output
unit
least
31
32
examples
network
trained
learning
second
output
unit
dry
summer
true
function
5
features
reliably
predict
first
feature
trained
another
epoch
data
learned
reliably
predict
second
output
unit
trained
predict
third
output
unit
fire
fall
concept
data
recorded
many
epochs
network
took
learn
final
concept
process
learning
concept
sequentially
repeated
50
times
network
required
average
5
96
epochs
190
72
trials
learn
logical
consistency
set
took
significantly
longer
8
50
epochs
272
00
trials
learn
feature
consistency
set
98
6
06
05
similar
human
subjects
network
sequentially
learned
set
concepts
easily
logically
consistent
concepts
merely
share
features
model
experiment
2
trained
network
simultaneously
learn
three
concepts
network
trained
3
concepts
tested
third
concept
epoch
training
data
network
tested
see
correctly
predict
value
third
feature
least
31
32
examples
training
stopped
otherwise
trained
another
epoch
data
kept
many
errors
network
made
epoch
epoch
network
learned
final
concept
process
learning
concepts
repeated
50
times
neural
net
required
average
7
12
epochs
227
84
trials
learn
logical
consistency
set
took
significantly
longer
9
66
epochs
309
12
trials
learn
feature
consistency
set
98
5
039
05
similar
human
subjects
network
simultaneously
learned
set
concepts
easily
logically
consistent
concepts
merely
share
features
figure
6
graphs
percentage
errors
made
two
sets
function
number
epochs
shows
second
epoch
graph
similar
figure
4
logically
consistent
condition
network
becomes
accurate
fewer
training
epochs
figure
6
mean
percentage
errors
made
neural
network
logical
consistency
feature
consistency
groups
function
epoch
shared
task
networks
able
model
results
can
create
new
abstract
features
use
features
influence
learning
concepts
network
requires
training
determine
use
abstract
features
less
training
required
new
concepts
consistent
concepts
learned
earlier
shared
task
network
example
wisniewski
medin
1994
call
tightly
coupled
model
prior
knowledge
case
created
prior
learning
selects
relevance
features
higher
weights
connections
creates
new
features
represented
hidden
units
furthermore
feedback
learning
one
concept
can
change
features
strengths
hidden
units
used
concepts
conclusions
although
general
topic
learning
series
concepts
discussed
previous
research
focused
attentional
phenomena
intradimensional
extradimensional
shift
subsequent
concepts
share
related
features
prior
concepts
however
approaches
consider
sets
arbitrary
groups
concepts
rather
concepts
causally
related
waldmann
holyoak
1990
argue
causal
induction
process
differs
learning
process
used
acquire
arbitrary
concepts
particular
show
concepts
acquired
induction
one
phase
experiment
influence
later
learning
much
manner
concepts
acquired
reading
written
instructions
prior
background
concepts
focused
prior
knowledge
facilitates
learning
also
point
incorrect
prior
knowledge
may
also
hinder
learning
providing
misconceptions
chi
slotta
de
leeuw
1994
prior
knowledge
compatible
new
knowledge
acquired
anticipate
positive
effect
classical
concepts
consistent
sets
necessary
sufficient
features
several
flaws
concepts
people
encounter
rigorous
logical
definitions
rosch
1978
recently
become
apparent
concepts
exist
learned
isolation
presented
quantitative
results
induced
background
knowledge
influence
rate
learning
number
errors
made
learning
found
relevant
correct
background
knowledge
facilitates
learning
eliminate
need
learning
unlike
previous
learning
models
subjects
learned
rules
corresponding
wetspring
drysummmer
wetspring
drysummmer
fireinfall
automatically
know
fireinfall
believe
one
flaw
previous
learning
models
use
prior
knowledge
equate
explanation
logical
proof
use
rules
necessary
sufficient
preconditions
rules
may
rare
real
world
cognitively
implausible
concepts
consistent
necessary
sufficient
definitions
acknowledgments
research
reported
supported
part
nsf
grant
iri
9310413
thank
kamal
ali
dorrit
billman
cliff
brunk
piew
datta
dennis
kibler
chris
merz
brian
ross
david
schulenburg
comments
various
phases
work
references
caruana
1992
multitask
learning
knowledge
based
source
inductive
bias
proceedings
tenth
international
machine
learning
conference
pp
41
48
san
mateo
ca
morgan
kaufman
chi
slotta
de
leeuw
1994
theories
processes
theory
conceptual
changes
learning
science
concepts
learning
instruction
4
27
43
lien
cheng
1989
framework
psychological
induction
integrating
power
law
covariation
views
eleventh
annual
conference
cognitive
science
society
pp
729
733
ann
arbor
mi
lawrence
erlbaum
associates
inc
mitchell
keller
kedar
cabelli
1986
explanation
based
learning
unifying
view
machine
learning
vol
1
1
murphy
medin
1985
role
theories
conceptual
coherence
psychological
review
92
3
nakamura
1985
knowledge
based
classification
ill
defined
categories
memory
cognition
13
377
84
pazzani
1990
creating
memory
causal
relationships
integration
empirical
explanation
based
learning
methods
hillsdale
nj
lawrence
erlbaum
associates
pazzani
1991
influence
prior
knowledge
concept
acquisition
experimental
computational
results
journal
experimental
psychology
learning
memory
cognition
17
3
416
32
rosch
1978
principles
categorization
cognition
categorization
ed
rosch
lloyd
hillsdale
nj
lawrence
erlbaum
associates
rumelhart
hinton
williams
1986
learning
internal
representations
backpropagating
errors
rumelhart
mcclelland
eds
parallel
distributed
processing
cambridge
ma
mit
press
towell
shavlik
noordewier
1990
refinement
approximate
domain
theories
knowledge
based
neural
networks
proceedings
eighth
national
conference
artificial
intelligence
pp
861
66
cambridge
ma
mit
press
waldmann
holyoak
1990
can
causal
induction
reduced
associative
learning
proceedings
twelfth
annual
conference
cognitive
science
society
cambridge
ma
lawrence
erlbaum
wattenmaker
dewey
murphy
medin
1986
linear
separability
concept
learning
context
relational
properties
concept
naturalness
cognitive
psychology
18
158
194
wisniewski
medin
1994
interaction
data
theory
concept
learning
cognitive
science
18
221
282
