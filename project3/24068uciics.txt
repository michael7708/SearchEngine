data compression section 4 
adaptive huffman coding 
data
compression
4
adaptive
huffman
coding
adaptive
huffman
coding
first
conceived
independently
faller
gallager
faller
1973
gallager
1978
knuth
contributed
improvements
original
algorithm
knuth
1985
resulting
algorithm
referred
algorithm
fgk
recent
version
adaptive
huffman
coding
described
vitter
vitter
1987
methods
defined
word
schemes
determine
mapping
source
messages
codewords
based
upon
running
estimate
source
message
probabilities
code
adaptive
changing
remain
optimal
current
estimates
way
adaptive
huffman
codes
respond
locality
essence
encoder
learning
characteristics
source
decoder
must
learn
along
encoder
continually
updating
huffman
tree
stay
synchronization
encoder
another
advantage
systems
require
one
pass
data
course
one
pass
methods
interesting
number
bits
transmit
significantly
greater
two
pass
scheme
interestingly
performance
methods
terms
number
bits
transmitted
can
better
static
huffman
coding
contradict
optimality
static
method
static
method
optimal
methods
assume
time
invariant
mapping
performance
adaptive
methods
can
also
worse
static
method
upper
bounds
redundancy
methods
presented
section
discussed
introduction
adaptive
method
faller
gallager
knuth
basis
unix
utility
compact
performance
compact
quite
good
providing
typical
compression
factors
30
40
4
1
algorithm
fgk
basis
algorithm
fgk
sibling
property
defined
gallager
gallager
1978
binary
code
tree
sibling
property
node
except
root
sibling
nodes
can
listed
order
nonincreasing
weight
node
adjacent
sibling
gallager
proves
binary
prefix
code
huffman
code
code
tree
sibling
property
algorithm
fgk
sender
receiver
maintain
dynamically
changing
huffman
code
trees
leaves
code
tree
represent
source
messages
weights
leaves
represent
frequency
counts
messages
point
time
possible
source
messages
occurred
message
ensemble
figure
4
1
algorithm
fgk
processing
ensemble
example
tree
processing
aa
bb
11
will
transmitted
next
encoding
third
101
will
transmitted
next
space
tree
will
change
100
will
transmitted
first
tree
update
following
first
initially
code
tree
consists
single
leaf
node
called
0
node
0
node
special
node
used
represent
unused
messages
message
transmitted
parties
must
increment
corresponding
weight
recompute
code
tree
maintain
sibling
property
point
time
messages
transmitted
distinct
tree
legal
huffman
code
tree
1
leaves
one
messages
one
0
node
1
st
message
one
already
seen
algorithm
transmits
1
's
current
code
increments
appropriate
counter
recomputes
tree
unused
message
occurs
0
node
split
create
pair
leaves
one
1
sibling
new
0
node
tree
recomputed
case
code
0
node
sent
addition
receiver
must
told
unused
messages
appeared
node
count
occurrences
corresponding
message
stored
nodes
numbered
indicating
position
sibling
property
ordering
updating
tree
can
done
single
traversal
1
node
root
traversal
must
increment
count
1
node
ancestors
nodes
may
exchanged
maintain
sibling
property
exchanges
involve
node
path
1
root
figure
4
2
shows
final
code
tree
formed
process
ensemble
example
figure
4
2
tree
formed
algorithm
fgk
ensemble
example
disregarding
overhead
number
bits
transmitted
algorithm
fgk
example
129
static
huffman
algorithm
transmit
117
bits
processing
data
overhead
associated
adaptive
method
actually
less
static
algorithm
adaptive
case
overhead
lg
bits
needed
represent
different
source
messages
appear
first
time
fact
conservative
rather
transmitting
unique
code
source
messages
sender
transmit
message's
position
list
remaining
messages
save
bits
average
case
static
case
source
messages
need
sent
shape
code
tree
discussed
section
3
2
efficient
representation
tree
shape
requires
2n
bits
algorithm
fgk
compares
well
static
huffman
coding
ensemble
overhead
taken
account
figure
4
3
illustrates
example
algorithm
fgk
performs
better
static
huffman
coding
even
without
taking
overhead
account
algorithm
fgk
transmits
47
bits
ensemble
static
huffman
code
requires
53
figure
4
3
tree
formed
algorithm
fgk
ensemble
eae
de
eabe
eae
dcf
vitter
proved
total
number
bits
transmitted
algorithm
fgk
message
ensemble
length
containing
distinct
messages
bounded
1
performance
static
method
bounded
2s
4n
2
vitter
1987
performance
algorithm
fgk
never
much
worse
twice
optimal
knuth
provides
complete
implementation
algorithm
fgk
proof
time
required
encoding
decoding
operation
current
length
codeword
knuth
1985
noted
since
mapping
defined
dynamically
transmission
encoding
decoding
algorithms
stand
alone
additional
algorithm
determine
mapping
static
methods
4
2
algorithm
figure
4
4
fgk
tree
non
level
order
numbering
adaptive
huffman
algorithm
vitter
algorithm
incorporates
two
improvements
algorithm
fgk
first
number
interchanges
node
moved
upward
tree
recomputation
limited
one
number
bounded
algorithm
fgk
2
length
codeword
1
recomputation
begins
second
vitter's
method
minimizes
values
sum
max
subject
requirement
minimizing
sum
intuitive
explanation
algorithm
v's
advantage
algorithm
fgk
follows
algorithm
fgk
code
tree
constructed
algorithm
huffman
code
tree
prefix
ensemble
seen
far
adaptive
methods
assume
relative
frequencies
prefix
represent
accurately
symbol
probabilities
entire
message
therefore
fact
algorithm
guarantees
tree
minimum
height
height
max
minimum
external
path
length
sum
implies
better
suited
coding
next
message
ensemble
given
leaves
tree
may
represent
next
message
improvements
accomplished
use
new
system
numbering
nodes
numbering
called
implicit
numbering
corresponds
level
ordering
nodes
bottom
top
left
right
figure
4
4
illiustrates
numbering
algorithm
fgk
always
level
ordering
following
invariant
maintained
vitter's
algorithm
weight
leaves
weight
precede
implicit
numbering
internal
nodes
weight
vitter
proves
invariant
enforces
desired
bound
node
promotions
vitter
1987
invariant
also
implements
bottom
merging
discussed
section
3
2
minimize
sum
max
difference
vitter's
method
algorithm
fgk
way
tree
updated
transmissions
order
understand
revised
update
operation
following
definition
block
nodes
necessary
blocks
equivalence
classes
nodes
defined
equivalent
iff
weight
weight
either
leaves
internal
nodes
leader
block
highest
numbered
implicit
numbering
node
block
blocks
ordered
increasing
weight
convention
leaf
block
always
precedes
internal
block
weight
exchange
nodes
required
maintain
sibling
property
algorithm
requires
node
promoted
moved
position
currently
occupied
highest
numbered
node
target
block
figure
4
5
vitter
tree
corresponding
figure
4
1c
shown
first
point
example
algorithm
fgk
algorithm
differ
significantly
point
vitter
tree
height
3
external
path
length
12
fgk
tree
height
4
external
path
length
14
algorithm
transmits
codeword
001
second
fgk
transmits
1101
demonstrates
intuition
given
earlier
algorithm
better
suited
coding
next
message
vitter
tree
corresponding
figure
4
2
representing
final
tree
produced
processing
example
different
figure
4
2
internal
node
weight
5
right
leaf
nodes
weight
5
algorithm
transmits
124
bits
processing
example
compared
129
bits
algorithm
fgk
117
bits
static
huffman
coding
noted
figures
include
overhead
result
disadvantage
adaptive
methods
figure
4
5
algorithm
processing
ensemble
aa
bbb
figure
4
6
ilustrates
tree
built
vitter's
method
ensemble
figure
4
3
sum
max
smaller
tree
figure
4
6
number
bits
transmitted
processing
sequence
47
used
algorithm
fgk
however
transmission
continues
unused
letter
cost
algorithm
will
less
algorithm
fgk
illustrates
benefit
minimizing
external
path
length
sum
height
max
figure
4
6
tree
formed
algorithm
ensemble
fig
4
3
noted
strategy
minimizing
external
path
length
height
optimal
assumption
source
letter
equally
likely
occur
next
reasonable
strategies
include
one
assumes
locality
take
advantage
locality
ordering
tree
nodes
equal
weights
determined
basis
recency
another
reasonable
assumption
adaptive
coding
weights
current
tree
correspond
closely
probabilities
associated
source
assumption
becomes
reasonable
length
ensemble
increases
assumption
expected
cost
transmitting
next
letter
sum
approximately
sum
neither
algorithm
fgk
algorithm
advantage
vitter
proves
performance
algorithm
bounded
1
2n
1
vitter
1987
worst
vitter's
adaptive
method
may
transmit
one
bit
per
codeword
static
huffman
method
improvements
made
vitter
change
complexity
algorithm
algorithm
encodes
decodes
time
algorithm
fgk
