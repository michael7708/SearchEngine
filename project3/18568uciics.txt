projects 
projects 
projects
improving
single
core
performance
via
compiler
assisted
order
commit
growth
uniprocessor
single
core
performance
resulting
improvements
semiconductor
technology
recently
slowed
significantly
sequential
applications
sequential
portions
parallel
applications
require
advances
improve
performance
today's
ooo
processors
complete
instructions
program
order
major
performance
bottleneck
long
latency
instruction
access
memory
delays
completion
subsequent
instructions
project
aims
achieve
higher
single
core
performance
defining
new
compiler
assisted
mechanism
order
instruction
completion
investigates
use
compile
time
program
knowledge
can
passed
hardware
used
simplify
architectural
checks
required
order
completion
architecture
standard
processor
will
fully
preserved
legacy
software
can
execute
without
modification
supported
national
science
foundation
cache
aware
synchronization
scheduling
data
parallel
programs
multi
core
processors
multi
core
parallel
processors
become
ubiquitous
use
systems
key
science
engineering
finance
major
areas
economy
however
increased
applications
performance
systems
can
achieved
advances
mapping
applications
multi
core
machines
task
made
difficult
presence
complex
memory
organizations
perhaps
key
bottleneck
efficient
execution
addressed
effectively
research
involves
making
mapping
program
machine
aware
complexities
memory
hierarchy
phases
compilation
process
will
ensure
good
fit
application
code
actual
machine
thereby
guarantee
much
effective
utilization
hardware
thus
efficient
fast
execution
previously
possible
multi
cores
can
benefit
new
cache
hierarchy
aware
compilation
runtime
system
including
compilation
scheduling
static
dynamic
processor
mapping
parallel
programs
tasks
one
thing
common
need
accurate
estimates
data
element
iteration
task
computation
memory
access
times
currently
beyond
cache
oblivious
state
art
research
thus
develops
new
techniques
iteration
space
partitioning
scheduling
synchronization
capture
variability
due
cache
memory
conditional
statement
behavior
interaction
supported
national
science
foundation
acceleration
neural
simulations
collaborating
scientists
study
model
human
brain
performs
certain
tasks
vision
computer
simulation
models
extremely
compute
bound
looking
parallel
application
specific
custom
architectures
accelerate
computations
preliminary
experience
fpga
based
cell
gpu
parallel
architectures
encouraging
reducing
power
consumption
processors
systems
power
dissipation
major
issue
designing
new
processors
systems
particular
cmos
technology
scaling
significantly
increased
leakage
power
dissipation
accounts
increasingly
large
share
processor
power
dissipation
one
main
issue
achieve
power
savings
without
loss
performance
much
work
area
focused
cache
power
dissipation
addressed
issues
l1
cache
dynamic
well
static
power
consumption
included
way
caching
save
static
dynamic
power
high
associativity
caches
alternative
way
prediction
cached
load
store
queue
low
cost
alternative
l0
cache
using
branch
prediction
information
save
power
instruction
caches
addressed
l2
power
consumption
particular
leakage
power
l2
peripheral
circuits
results
research
applicable
embedded
high
performance
processors
another
aspect
research
low
power
instruction
queue
design
order
processors
cam
based
instruction
queues
scalable
consume
significant
amount
power
due
wide
issue
cam
search
cycle
one
approach
proposed
used
banked
queue
thus
dividing
cam
smaller
banks
faster
search
pointer
table
indicates
bank
instruction
belongs
complex
approach
disposed
cam
based
queue
altogether
used
instruction
dependence
pointers
ram
based
queue
direct
wakeup
solved
problem
achieve
fast
branch
misprediction
recovery
using
pointers
using
dependent
pointers
investigated
problem
power
consumption
register
file
content
aware
register
file
utilized
knowledge
instruction
operand
effective
address
width
reduce
number
bits
read
rf
speed
tlb
access
using
l0
tlb
type
register
file
also
shown
enable
new
type
clustered
processor
improved
performance
reduced
power
leakage
peripheral
circuits
sram
based
units
major
contributor
overall
power
dissipation
well
temperature
increases
developed
number
circuit
techniques
using
sleep
transistors
reduce
leakage
well
architectural
techniques
control
application
leakage
reduction
techniques
finally
studied
power
consumption
main
memory
dram
embedded
systems
certain
types
embedded
systems
applications
important
component
overall
power
processor
proposed
ways
reduce
power
consumption
drams
utiizing
buffering
delayed
writes
prefetching
techniques
supported
national
science
foundation
darpa
past
projects
speeding
mobile
code
execution
resource
constrained
embedded
processors
supported
national
science
foundation
compiler
controlled
continuous
power
performance
management
supported
darpa
adaptive
memory
reconfiguration
management
supported
darpa
