sli classes cs178 notes regression 
regression 
classes
group
research
publications
code
login
classes
cs178
notes
regression
regression
problems
regression
type
supervised
learning
task
target
variable
real
valued
example
may
wish
predict
sale
price
house
given
observable
characteristics
house
size
square
footage
location
distance
coast
likely
base
prediction
sale
price
relationship
observable
features
set
training
data
example
historical
sales
data
simplicity
plots
will
initially
assume
single
feature
size
allowing
us
plot
historical
data
points
scatter
plot
plot
predictor
function
observed
features
predicted
value
scatter
plot
training
datalinear
function
nonlinear
function
will
initially
focus
linear
regression
prediction
form
linear
function
observed
features
example
helpful
define
variables
matrix
form
will
allow
us
write
compact
equations
will
also
translate
well
matlab
syntax
example
defining
zeroth
feature
can
write
vectors
formed
concatenating
features
respectively
parameters
together
data
will
use
training
consist
number
examples
example
houses
recently
sold
recent
stock
behavior
will
index
example
parenthesized
super
script
parentheses
differentiate
indexing
example
taking
number
power
square
can
vectorize
entire
collection
data
used
training
matrix
shorthand
chosen
represent
row
training
example
values
feature
observed
particular
datum
single
house
historical
set
column
indicates
particular
feature
first
column
ones
indicates
feature
zero
constant
value
prepend
features
manage
offset
term
subsequent
column
represents
values
particular
feature
size
distance
etc
observed
across
examples
training
data
note
potentially
confusing
difference
syntax
presentations
result
transposition
one
quantities
helpful
keep
mind
correct
dimensionality
vector
elements
number
training
data
elements
one
observed
features
matlab
syntax
predicting
yhat
theta'
dot
product
example
datum
column
vector
error
measures
optimization
given
observation
can
compute
error
prediction
also
called
residual
residuals
constant
residuals
linear
general
like
minimize
overall
size
errors
common
shall
see
computationally
convenient
choice
euclidean
norm
vector
residuals
sum
squared
errors
sse
cost
cost
function
tells
us
accuracy
given
parameter
vector
predicting
training
data
function
defined
space
parameters
two
dimensional
parameter
vector
can
visualize
surface
use
colors
contours
suggest
three
dimensional
height
cost
2d
plot
color
map3d
surfacesurface
contourscontours
gradient
descent
one
option
follow
local
slope
downward
towards
local
minimum
can
evaluate
gradient
direction
direction
cost
function
greatest
increase
going
opposite
direction
gives
us
direction
greatest
decrease
cost
gradient
will
vector
dimension
gradient
descent
simply
initialize
starting
value
repeatedly
update
choosing
new
moving
direction
steepest
descent
indicates
updating
value
using
quantity
right
value
step
size
parameter
tells
us
far
move
direction
gradient
choice
step
size
can
important
gradient
descent
methods
often
controls
fast
converge
local
minimum
values
small
move
slowly
values
large
can
skip
even
oscillate
around
local
minima
common
heuristic
approach
setting
step
size
let
decrease
iteration
step
example
choosing
inversely
proportional
iteration
number
can
see
behavior
gradient
descent
cost
function
defined
parameter
space
top
point
corresponds
vector
induced
predictors
bottom
value
corresponds
different
linear
predictor
initializationiteration
1iteration
10iteration
30iteration
90
closed
form
optimum
squared
error
can
solve
quadratic
equation
defined
stationary
point
rearranging
gives
quadratic
equation
solution
minimum
squared
error
mse
estimator
term
called
moore
penrose
pseudo
inverse
compensates
fact
typically
constrained
value
theta
will
exactly
predict
every
value
increasing
number
features
far
considered
linear
functions
several
observed
features
suppose
one
feature
like
predictor
nonlinear
function
example
can
simply
define
new
features
just
defined
predictor
becomes
simply
words
still
fits
linear
regression
model
new
feature
space
additional
features
deterministic
functions
observations
applying
least
squares
estimator
gives
polynomial
fits
0p
1p
3p
7p
10
think
target
variable
likely
linearly
related
complex
function
several
observed
variables
combination
can
also
added
new
observed
feature
two
ways
view
process
first
creating
complex
functional
predictor
set
functions
can
represent
now
larger
set
lines
set
cubic
polynomials
alternative
forget
features
now
deterministically
related
one
another
imagine
adding
extra
measurements
features
predict
view
learning
linear
predictor
data
data
lie
higher
dimensional
space
will
return
views
later
classification
overfitting
saw
polynomial
fits
11
data
points
degree
10
polynomial
11
coefficients
can
predict
training
data
exactly
yet
something
predictor
satisfying
look
like
good
predictor
fact
overfit
data
chosen
complex
predictor
although
able
reconstruct
training
data
faith
will
accurately
predict
new
data
can
see
generalization
test
error
predictor
simply
gathering
data
evaluating
cost
function
mean
squared
error
new
points
green
find
simple
predictors
0
1
performance
new
test
data
much
like
performance
training
data
however
function
gets
complex
training
error
continues
decrease
test
error
10
training
error
zero
test
error
extremely
high
0p
1p
3p
7p
10
can
see
directly
plotting
training
test
error
function
polynomial
degree
simple
predictors
unable
capture
complexity
dependence
overly
complex
predictors
memorize
values
expense
generalization
notionally
axis
can
thought
complexity
find
similar
curve
whenever
complexity
learner
increases
will
see
much
practical
aspects
machine
learning
come
choosing
controlling
position
curve
increasing
complexity
fitting
decreasing
overfitting
last
modified
january
11
2011
12
59
pm
bren
school
information
computer
science
university
california
irvine
