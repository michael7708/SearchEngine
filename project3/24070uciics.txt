data compression section 6 
empirical results 
data
compression
6
empirical
results
empirical
tests
efficiencies
algorithms
presented
reported
bentley
et
al
1986
knuth
1985
schwartz
kallick
1964
vitter
1987
welch
1984
experiments
compare
number
bits
per
word
required
processing
time
reported
theoretical
considerations
bound
performance
various
algorithms
experimental
data
invaluable
providing
additional
insight
clear
performance
methods
dependent
upon
characteristics
source
ensemble
schwartz
kallick
test
implementation
static
huffman
coding
bottom
merging
used
determine
codeword
lengths
codewords
given
length
sequential
binary
numbers
schwartz
kallick
1964
source
alphabet
experiment
consists
5
114
frequently
used
english
words
27
geographical
names
10
numerals
14
symbols
43
suffixes
entropy
document
8
884
binary
digits
per
message
average
codeword
constructed
length
8
920
document
also
coded
one
character
time
case
entropy
source
4
03
coded
ensemble
contains
average
4
09
bits
per
letter
redundancy
low
cases
however
relative
redundancy
redundancy
entropy
lower
document
encoded
words
knuth
describes
algorithm
fgk's
performance
three
types
data
file
containing
text
grimm's
first
ten
fairy
tales
text
technical
book
file
graphical
data
knuth
1985
first
two
files
source
messages
individual
characters
alphabet
size
128
data
coded
using
pairs
characters
alphabet
size
1968
graphical
data
number
source
messages
343
case
fairy
tales
performance
fgk
close
optimum
although
performance
degrades
increasing
file
size
performance
technical
book
good
still
respectable
graphical
data
proves
harder
yet
compress
fgk
performs
reasonably
well
latter
two
cases
trend
performance
degradation
file
size
continues
defining
source
messages
consist
character
pairs
results
slightly
better
compression
difference
appear
justify
increased
memory
requirement
imposed
larger
alphabet
static
alg
alg
fgk
100
96
83
0
71
1
82
4
500
96
83
0
80
8
83
5
961
97
83
5
82
3
83
7
figure
6
1
simulation
results
small
text
file
vitter
1987
file
size
8
bit
bytes
number
distinct
messages
vitter
tests
performance
algorithms
fgk
static
huffman
coding
method
run
data
includes
pascal
source
code
tex
source
author's
thesis
electronic
mail
files
vitter
1987
figure
6
1
summarizes
results
experiment
small
file
text
performance
algorithm
measured
number
bits
coded
ensemble
overhead
costs
included
compression
achieved
algorithm
represented
size
file
creates
given
percentage
original
file
size
figure
6
2
presents
data
pascal
source
code
tex
source
alphabet
consists
128
individual
characters
two
file
types
97
characters
appear
experiment
overhead
costs
taken
account
algorithm
outperforms
static
huffman
coding
long
size
message
ensemble
number
characters
10
4
algorithm
fgk
displays
slightly
higher
costs
never
100
4
static
algorithm
static
alg
alg
fgk
100
32
57
4
56
2
58
9
500
49
61
5
62
2
63
0
1000
57
61
3
61
8
62
4
10000
73
59
8
59
9
60
0
12067
78
59
6
59
8
59
9
figure
6
2
simulation
results
pascal
source
code
vitter
1987
file
size
bytes
number
distinct
messages
witten
et
al
compare
adaptive
arithmetic
coding
adaptive
huffman
coding
witten
et
al
1987
version
arithmetic
coding
tested
employs
single
character
adaptive
frequencies
mildly
optimized
implementation
witten
et
al
compare
results
provided
version
arithmetic
coding
results
achieved
unix
compact
program
compact
based
algorithm
fgk
three
large
files
typify
data
compression
applications
compression
achieved
arithmetic
coding
better
provided
compact
slightly
better
average
file
size
98
compacted
size
file
three
character
alphabet
skewed
symbol
probabilities
encoded
arithmetic
coding
less
one
bit
per
character
resulting
file
size
74
size
file
generated
compact
witten
et
al
also
report
encoding
decoding
times
encoding
time
arithmetic
coding
generally
half
time
required
adaptive
huffman
coding
method
decode
time
averages
65
time
required
compact
case
skewed
file
time
statistics
quite
different
arithmetic
coding
achieves
faster
encoding
67
time
required
compact
however
compact
decodes
quickly
using
78
time
arithmetic
method
bentley
et
al
use
pascal
source
files
troff
source
files
terminal
session
transcript
several
hours
experiments
compare
performance
algorithm
bstw
static
huffman
coding
defined
words
consist
two
disjoint
classes
sequences
alphanumeric
characters
sequences
nonalphanumeric
characters
performance
algorithm
bstw
close
static
huffman
coding
cases
experiments
reported
bentley
et
al
particular
interest
incorporate
another
dimension
possibility
move
front
scheme
one
might
want
limit
size
data
structure
containing
codes
include
recent
words
bentley
et
al
1986
tests
consider
cache
sizes
8
16
32
64
128
256
although
performance
tends
increase
cache
size
increase
erratic
documents
exhibiting
nonmonotonicity
performance
increases
cache
size
point
decreases
cache
size
increased
welch
reports
simulation
results
lempel
ziv
codes
terms
compression
ratios
welch
1984
definition
compression
ratio
one
given
section
1
3
average
message
length
average
codeword
length
ratios
reported
1
8
english
text
2
6
cobol
data
files
1
0
floating
point
arrays
2
1
formatted
scientific
data
2
6
system
log
data
2
3
source
code
1
5
object
code
tests
involving
english
text
files
showed
long
individual
documents
compress
better
groups
short
documents
observation
somewhat
surprising
seems
refute
intuition
redundancy
due
least
part
correlation
content
purposes
comparison
welch
cites
results
pechura
rubin
pechura
achieved
1
5
compression
ratio
using
static
huffman
coding
files
english
text
pechura
1982
rubin
reports
2
4
ratio
english
text
employing
complex
technique
choosing
source
messages
huffman
coding
applied
rubin
1976
results
provide
weak
basis
comparison
since
characteristics
files
used
three
authors
unknown
likely
single
algorithm
may
produce
compression
ratios
ranging
1
5
2
4
depending
upon
source
applied
7
susceptibility
error
discrete
noiseless
channel
unfortunately
realistic
model
communication
system
actual
data
transmission
systems
prone
two
types
error
phase
error
code
symbol
lost
gained
amplitude
error
code
symbol
corrupted
neumann
1962
degree
channel
errors
degrade
transmission
important
parameter
choice
data
compression
method
susceptibility
error
coding
algorithm
depends
heavily
whether
method
static
adaptive
7
1
static
codes
generally
known
huffman
codes
tend
self
correcting
standish
1980
transmission
error
tends
propagate
far
codeword
error
occurs
incorrectly
received
likely
several
subsequent
codewords
misinterpreted
long
receiver
back
synchronization
sender
static
code
synchronization
means
simply
sender
receiver
identify
beginnings
codewords
way
figure
7
1
example
used
illustrate
ability
huffman
code
recover
phase
errors
message
ensemble
bcdaeb
encoded
using
huffman
code
figure
3
4
source
letters
1
5
represent
respectively
yielding
coded
ensemble
0110100011000011
figure
7
1
demonstrates
impact
loss
first
bit
second
bit
fourth
bit
dots
show
way
line
parsed
codewords
loss
first
bit
results
re
synchronization
third
bit
first
source
message
lost
replaced
aa
second
bit
lost
first
eight
bits
coded
ensemble
misinterpreted
synchronization
regained
bit
9
dropping
fourth
bit
causes
degree
disturbance
dropping
second
011
010
001
1
000
011
coded
ensemble
bcdaeb
1
1
010
001
1
000
011
bit
1
lost
interpreted
aacdaeb
010
1
000
1
1
000
011
bit
2
lost
interpreted
caeaaeb
011
1
000
1
1
000
011
bit
4
lost
interpreted
baeaaeb
figure
7
1
recovery
phase
errors
0
1
1
0
1
0
00
1
1
000
011
coded
ensemble
bcdaeb
1
1
1
0
1
0
00
1
1
000
011
bit
1
inverted
interpreted
dcdaeb
0
0
1
0
1
0
00
1
1
000
011
bit
2
inverted
interpreted
aaacdaeb
0
1
1
1
1
0
00
1
1
000
011
bit
4
inverted
interpreted
baaeaaeb
figure
7
2
recovery
amplitude
errors
effect
amplitude
errors
demonstrated
figure
7
2
format
illustration
figure
7
1
time
bits
1
2
4
inverted
rather
lost
synchronization
regained
almost
immediately
bit
1
bit
2
changed
first
three
bits
first
character
ensemble
disturbed
inversion
bit
four
causes
loss
synchronization
ninth
bit
simple
explanation
self
synchronization
present
example
can
given
since
many
codewords
end
sequence
digits
decoder
likely
reach
leaf
huffman
code
tree
one
codeword
boundaries
original
coded
ensemble
happens
decoder
back
synchronization
encoder
self
synchronization
may
discussed
carefully
following
definitions
presented
noted
definitions
hold
arbitrary
prefix
codes
discussion
includes
codes
described
section
3
suffix
codeword
exist
sequences
codewords
gamma
delta
gamma
delta
gamma
said
synchronizing
sequence
example
huffman
code
used
1
synchronizing
sequence
suffix
01
000001
011
synchronizing
sequences
suffix
10
every
suffix
every
codeword
synchronizing
sequence
code
completely
self
synchronizing
none
proper
suffixes
synchronizing
sequences
code
respectively
partially
never
self
synchronizing
finally
exists
sequence
gamma
synchronizing
sequence
every
suffix
gamma
defined
universal
synchronizing
sequence
code
used
examples
completely
self
synchronizing
universal
synchronizing
sequence
00000011000
gilbert
moore
prove
existence
universal
synchronizing
sequence
necessary
well
sufficient
condition
code
completely
self
synchronizing
gilbert
moore
1959
also
state
prefix
code
completely
self
synchronizing
will
synchronize
probability
1
source
ensemble
consists
successive
messages
independently
chosen
given
set
probabilities
true
since
probability
occurrence
universal
synchronizing
sequence
given
time
positive
important
realize
fact
completely
self
synchronizing
code
will
re
synchronize
probability
1
guarantee
recovery
error
bounded
delay
fact
every
completely
self
synchronizing
prefix
code
two
codewords
errors
within
one
codeword
cause
unbounded
error
propagation
neumann
1962
addition
prefix
codes
always
completely
self
synchronizing
bobrow
hakimi
state
necessary
condition
prefix
code
codeword
lengths
1
completely
self
synchronizing
greatest
common
divisor
must
equal
one
bobrow
hakimi
1969
huffman
code
00
01
10
1100
1101
1110
1111
completely
self
synchronizing
partially
self
synchronizing
since
suffixes
00
01
10
synchronized
codeword
huffman
code
000
0010
0011
01
100
1010
1011
100
111
never
self
synchronizing
examples
never
self
synchronizing
huffman
codes
difficult
construct
example
one
fewer
16
source
messages
stiffler
proves
code
never
self
synchronizing
none
proper
suffixes
codewords
codewords
stiffler
1971
conclusions
may
drawn
discussion
common
huffman
codes
self
synchronize
guaranteed
self
synchronization
assured
bound
propagation
error
additional
difficulty
self
synchronization
provides
indication
error
occurred
problem
error
detection
correction
connection
huffman
codes
received
great
deal
attention
several
ideas
subject
reported
rudner
states
synchronizing
sequences
short
possible
minimize
re
synchronization
delay
addition
synchronizing
sequence
used
codeword
high
probability
message
re
synchronization
will
frequent
method
constructing
minimum
redundancy
code
shortest
possible
synchronizing
sequence
described
rudner
rudner
1971
neumann
suggests
purposely
adding
redundancy
huffman
codes
order
permit
detection
certain
types
errors
neumann
1962
clearly
done
carefully
negate
redundancy
reduction
provided
huffman
coding
mcintyre
pechura
cite
data
integrity
advantage
codebook
approach
discussed
section
3
2
mcintyre
pechura
1985
code
stored
separately
coded
data
code
may
backed
protect
perturbation
however
code
stored
transmitted
data
susceptible
errors
error
code
representation
constitutes
drastic
loss
therefore
extreme
measures
protecting
part
transmission
justified
elias
codes
section
3
3
robust
codes
gamma
delta
can
thought
generating
codewords
consist
number
substrings
substring
encodes
length
subsequent
substring
code
gamma
may
think
codeword
gamma
concatenation
string
zeros
string
length
1
floor
lg
one
zeros
substring
lost
synchronization
will
lost
last
symbol
will
pushed
next
codeword
since
1
front
substring
delimits
end
zero
changed
1
synchronization
will
lost
symbols
pushed
following
codeword
similarly
ones
front
inverted
zeros
synchronization
will
lost
codeword
gamma
consumes
symbols
following
codeword
synchronization
lost
normally
recovered
figure
7
3
codewords
gamma
6
gamma
4
gamma
8
used
illustrate
ideas
case
synchronization
lost
never
recovered
001
1
0
00
1
00
0
00100
0
coded
integers
6
4
8
0
1
1
0
00
1
00
0
00100
0
bit
2
lost
interpreted
3
8
2
etc
011
1
0
00
1
00
0
00100
0
bit
2
inverted
interpreted
3
1
8
4
etc
000
1
0
00
1
00
0
00100
0
bit
3
inverted
interpreted
8
1
etc
figure
7
3
effects
errors
elias
codes
elias
code
delta
may
thought
three
part
ramp
delta
zmb
string
zeros
string
length
1
binary
value
string
length
1
example
delta
16
00
101
0000
2
5
final
substring
binary
value
16
leading
1
removed
length
1
4
fact
substring
determines
length
subsequent
substring
means
error
one
first
two
substrings
disastrous
changing
way
rest
codeword
interpreted
like
code
gamma
code
delta
properties
aid
regaining
synchronization
lost
fibonacci
codes
section
3
3
hand
quite
robust
robustness
due
fact
every
codeword
ends
substring
11
substring
can
appear
nowhere
else
codeword
error
occurs
anywhere
11
substring
error
contained
within
one
codeword
possible
one
codeword
will
become
two
see
sixth
line
figure
7
4
codewords
will
disturbed
last
symbol
codeword
lost
changed
current
codeword
will
fused
successor
two
codewords
lost
penultimate
bit
disturbed
three
codewords
can
lost
example
coded
message
011
11
011
becomes
0011
1011
bit
2
inverted
maximum
disturbance
resulting
either
amplitude
error
phase
error
disturbance
three
codewords
figure
7
4
illustrations
based
fibonacci
coding
ensemble
example
shown
figure
3
9
given
bit
3
part
11
substring
lost
changed
single
codeword
degraded
bit
6
final
bit
first
codeword
lost
changed
first
two
codewords
incorrectly
decoded
bit
20
changed
first
incorrectly
decoded
fg
000011
000011
00011
010
11
01011
coded
ensemble
aa
bb
00
011
000011
00011
010
11
01011
bit
3
lost
interpreted
bb
001011
000011
00011
010
11
01011
bit
3
inverted
interpreted
bb
00001
000011
00011
010
11
01011
bit
6
lost
interpreted
bb
000010
000011
00011
010
11
01011
bit
6
inverted
interpreted
bb
000011
000011
00011
011
11
01011
bit
20
inverted
interpreted
aa
fgb
figure
7
4
effects
errors
fibonacci
codes
7
2
adaptive
codes
adaptive
codes
far
adversely
affected
transmission
errors
static
codes
example
case
adaptive
huffman
code
even
though
receiver
may
re
synchronize
sender
terms
correctly
locating
beginning
codeword
information
lost
represents
bits
characters
source
ensemble
fact
sender
receiver
dynamically
redefining
code
indicates
time
synchronization
regained
may
radically
different
representations
code
synchronization
defined
section
7
1
refers
synchronization
bit
stream
sufficient
adaptive
methods
needed
code
synchronization
synchronization
bit
stream
dynamic
data
structure
representing
current
code
mapping
evidence
adaptive
methods
self
synchronizing
bentley
et
al
note
algorithm
bstw
loss
synchronization
can
catastrophic
whereas
true
static
huffman
coding
bentley
et
al
1986
ziv
lempel
recognize
major
drawback
algorithm
susceptibility
error
propagation
ziv
lempel
1977
welch
also
considers
problem
error
tolerance
lempel
ziv
codes
suggests
entire
ensemble
embedded
error
detecting
code
welch
1984
neither
static
adaptive
arithmetic
coding
ability
tolerate
errors
8
new
directions
data
compression
still
much
active
research
area
section
suggests
possibilities
study
discussion
section
7
illustrates
susceptibility
error
codes
presented
survey
strategies
increasing
reliability
codes
incurring
moderate
loss
efficiency
great
value
area
appears
largely
unexplored
possible
approaches
include
embedding
entire
ensemble
error
correcting
code
reserving
one
codewords
act
error
flags
adaptive
methods
may
necessary
receiver
sender
verify
current
code
mapping
periodically
adaptive
huffman
coding
gallager
suggests
aging
scheme
whereby
recent
occurrences
character
contribute
frequency
count
earlier
occurrences
gallager
1978
strategy
introduces
notion
locality
adaptive
huffman
scheme
cormack
horspool
describe
algorithm
approximating
exponential
aging
cormack
horspool
1984
however
effectiveness
algorithm
established
knuth
bentley
et
al
suggest
possibility
using
cache
concept
exploit
locality
minimize
effect
anomalous
source
messages
preliminary
empirical
results
indicate
may
helpful
knuth
1985
bentley
et
al
1986
problem
related
use
cache
overhead
time
required
deletion
strategies
reducing
cost
deletion
considered
another
possible
extension
algorithm
bstw
investigate
locality
heuristics
bentley
et
al
prove
intermittent
move
front
move
front
every
occurrences
effective
move
front
bentley
et
al
1986
noted
many
self
organizing
methods
yet
considered
horspool
cormack
describe
experimental
results
imply
transpose
heuristic
performs
well
move
front
suggest
also
easier
implement
horspool
cormack
1987
several
aspects
free
parse
methods
merit
attention
lempel
ziv
codes
appear
promising
although
absence
worst
case
bound
redundancy
individual
finite
source
ensemble
drawback
variable
block
type
lempel
ziv
codes
implemented
success
arc
1986
construction
variable
variable
lempel
ziv
code
sketched
ziv
lempel
1978
efficiency
variable
variable
model
investigated
addition
implementation
lempel
ziv
coding
combines
time
efficiency
rodeh
et
al
method
efficient
use
space
worthy
consideration
another
important
research
topic
development
theoretical
models
data
compression
address
problem
local
redundancy
models
based
markov
chains
may
exploited
take
advantage
interaction
groups
symbols
entropy
tends
overestimated
symbol
interaction
considered
models
exploit
relationships
source
messages
may
achieve
better
compression
predicted
entropy
calculation
based
upon
symbol
probabilities
use
markov
modeling
considered
llewellyn
langdon
rissanen
llewellyn
1987
langdon
rissanen
1983
9
summary
data
compression
topic
much
importance
many
applications
methods
data
compression
studied
almost
four
decades
paper
provided
overview
data
compression
methods
general
utility
algorithms
evaluated
terms
amount
compression
provide
algorithm
efficiency
susceptibility
error
algorithm
efficiency
susceptibility
error
relatively
independent
characteristics
source
ensemble
amount
compression
achieved
depends
upon
characteristics
source
great
extent
semantic
dependent
data
compression
techniques
discussed
section
2
special
purpose
methods
designed
exploit
local
redundancy
context
information
semantic
dependent
scheme
can
usually
viewed
special
case
one
general
purpose
algorithms
also
noted
algorithm
bstw
general
purpose
technique
exploits
locality
reference
type
local
redundancy
susceptibility
error
main
drawback
algorithms
presented
although
channel
errors
devastating
adaptive
algorithms
static
ones
possible
error
propagate
without
limit
even
static
case
methods
limiting
effect
error
effectiveness
data
compression
algorithm
investigated
