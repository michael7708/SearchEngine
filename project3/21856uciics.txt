computational statistics regression 
methods regression 
ics
280
spring
1999
computational
statistics
regression
data
model
linear
regression
data
points
constrained
lie
hyperplane
linear
relation
coordinates
another
way
interpreting
treat
one
coordinates
dependent
rest
independent
dependent
coordinate
linear
function
independent
ones
data
may
chosen
according
probability
distribution
line
may
known
pattern
independent
coordinates
chosen
grid
point
one
measures
dependent
coordinate
probability
distribution
models
much
matter
dependent
independent
variables
restriction
hyperplane
must
parallel
dependent
coordinate
axis
choice
may
make
difference
independent
coordinates
nonrandom
noise
model
adds
noise
dependent
coordinate
data
models
data
points
must
satisfy
multiple
linear
relations
lie
lower
dimensional
subspace
can
often
handled
treating
dependent
coordinate
separately
appropriate
noise
models
coordinates
independent
gaussian
noise
projective
duality
another
picture
regression
mathematically
equivalent
may
help
human
intuition
interpret
coordinates
defining
line
ax
instead
define
point
dual
plane
set
lines
point
primal
plane
turns
set
points
dual
plane
set
just
line
collection
points
xi
yi
defining
regression
problem
can
think
instead
defining
different
line
xi
yi
plane
want
find
point
plane
near
lines
expect
lines
roughly
look
like
go
common
point
want
find
common
point
similar
methods
transform
higher
dimensional
regression
problem
fitting
single
hyperplane
set
data
points
equivalent
problem
fitting
single
point
arrangement
hyperplanes
lp
models
dependent
noise
first
consider
case
noise
random
variable
added
independent
variable
common
algorithm
fitting
model
least
squares
trying
find
linear
model
dependent
variable
vector
independent
variables
parameters
trying
find
measure
error
fit
sum
xi
yi
2
least
squares
fit
one
minimizing
error
estimate
big
advantage
least
squares
easy
solve
just
positive
definite
quadratic
form
unique
minimum
since
smooth
function
minimum
occurs
point
partial
derivatives
coordinate
zero
gives
system
equations
unknowns
one
can
solve
gaussian
elimination
one
can
instead
use
local
improvement
methods
approximate
optimal
fit
thus
time
linear
number
unknowns
worst
cubic
dimension
generally
lp
error
formed
using
pth
powers
place
squares
definition
limit
linfinity
norm
just
maximum
error
individual
data
point
commonly
used
l1
norm
also
known
least
absolute
deviation
lad
regression
less
sensitive
outliers
least
squares
linfinity
norm
sensitive
outliers
therefore
useful
contexts
one
wants
detect
measuring
flatness
surface
linfinity
estimator
can
expressed
low
dimensional
linear
program
find
minimizing
satisfying
linear
constraints
xi
yi
dimension
number
variables
found
just
1
therefore
can
solved
time
linear
number
data
points
exponential
dimension
using
low
dimensional
linear
programming
techniques
citations
filled
later
also
exists
similar
linear
time
bound
l1
estimation
z84
lp
estimators
invariant
coordinate
changes
affine
transformations
among
dependent
variables
one
advantage
assuming
dependent
noise
preserved
duality
transformation
described
vertical
distance
point
xi
yi
line
ax
vertical
distance
point
line
xi
yi
just
work
equations
discussion
one
students
suggested
using
convex
combination
different
lp
metrics
might
come
truncated
power
series
harder
estimate
metric
alternatively
coefficients
convex
combination
might
come
sort
statistical
estimation
fit
noise
actually
apparent
data
however
know
references
related
ideas
lp
models
independent
noise
noise
added
coordinates
just
dependent
ones
may
appropriate
use
euclidean
distance
regression
hyperplane
place
vertical
distance
used
alternatively
considerations
single
point
estimation
may
make
different
distance
function
appropriate
l2
estimation
euclidean
distance
known
total
least
squares
gl80
m59
p01
optimal
estimator
must
go
centroid
data
hard
see
one
applies
duality
algebra
describing
estimator
quality
function
parameters
messy
hard
invert
however
problem
can
apparently
solved
singular
value
decomposition
nice
connection
l1
estimation
independent
noise
model
famous
sets
level
problem
computational
geometry
fix
choice
slope
problem
just
one
dimensional
l1
problem
can
solved
median
use
projective
duality
want
know
median
among
points
dual
hyperplanes
cross
possible
vertical
line
sequence
medians
forms
surface
arrangement
known
level
2
two
dimensions
surface
just
polygonal
path
formed
walking
left
right
arrangement
turning
every
crossing
point
pass
famous
problem
posed
erd
lov
sz
number
edges
path
can
known
bounds
omega
log
nk1
3
linfinity
estimation
equivalent
computing
width
set
minimum
distance
parallel
supporting
hyperplanes
can
computed
log
time
plane
best
time
known
three
dimensions
n3
2
citations
filled
later
independent
noise
estimators
generally
invariant
arbitrary
translations
rotations
coordinate
system
however
invariant
general
affine
transformations
dual
independent
noise
one
mind
physical
meaning
dual
interpretation
data
define
lines
estimator
one
trying
find
point
near
lines
may
make
sense
use
euclidean
distance
dual
plane
instead
primal
define
distance
estimator
point
data
line
xi
yi
usual
euclidean
distance
point
line
definition
one
can
make
sort
lp
regression
etc
l1
l2
linfinity
regressions
can
solved
easily
using
simple
modifications
algorithms
dependent
noise
models
one
big
advantage
dual
independence
primal
independence
know
references
work
actual
statisticians
sort
model
least
median
squares
lms
estimator
defined
rousseeuw
r84
minimizes
median
error
rather
sum
errors
squared
errors
l1
l2
estimation
equivalently
seek
planar
strip
minimum
possible
vertical
width
covers
least
half
data
points
commonly
claimed
estimator
provides
maximum
possible
robustness
outliers
half
data
may
arbitrarily
corrupted
claim
seems
rely
assumption
data
well
spread
along
independent
coordinates
rather
tightly
clustered
along
lower
dimensional
subspaces
way
allow
adversary
set
false
estimates
using
large
numbers
valid
data
points
assumption
must
made
robust
regression
method
unfortunately
algorithms
computing
lms
estimator
relatively
slow
n2
even
two
dimensions
ss87
es90
theoretical
reasons
believing
exact
algorithm
can
faster
however
mount
et
al
mnrsw97
provide
log
time
approximation
well
randomized
algorithm
believe
work
well
practice
although
lacks
theoretical
performance
guarantees
dual
framework
given
arrangement
lines
wish
find
shortest
possible
vertical
line
segment
crosses
least
half
lines
estimator
assumes
dependent
noise
model
one
can
define
lms
like
estimator
similar
ways
independent
noise
case
one
trying
find
line
minimizing
median
distance
points
dual
independent
noise
case
one
trying
find
minimum
radius
circle
crosses
least
half
lines
repeated
median
know
subject
mmn98
guess
based
name
looked
paper
yet
lms
estimation
can
interpreted
finding
set
outliers
throw
away
ignore
namely
points
away
median
distance
finds
throws
away
points
something
fairly
stupid
rest
points
just
linfinity
estimator
sensitive
outliers
data
left
throwing
away
half
points
still
suspect
seems
reasonable
instead
apply
outlier
detection
strategy
recursively
much
say
algorithms
least
without
progress
algorithms
lms
estimator
reason
quadratic
algorithm
lms
apply
recursively
can
describe
recursion's
time
recurrence
2
n2
quadratic
asymptotically
time
recursive
approach
time
simple
lms
estimation
median
slope
called
thiel
sen
estimator
planar
data
finds
line
mx
choosing
median
among
1
2
slopes
lines
determined
pairs
data
points
remaining
parameter
can
found
choosing
median
among
values
mx
dual
framework
given
arrangement
lines
plane
pair
lines
determines
point
two
lines
cross
wish
find
median
coordinate
among
choose
2
crossings
points
method
robust
1
1
sqrt
2
0
29n
outliers
rather
2
lms
estimator
proof
sqrt
2
valid
data
points
define
roughly
2
4
slopes
many
outliers
perturb
median
however
can
computed
quickly
log
time
several
algorithms
bc98
csss89
dmn92
ks93
m91b
guess
higher
dimensions
one
apply
robust
single
point
estimation
technique
centerpoint
least
median
squares
normal
vectors
hyperplanes
defined
tuples
points
know
work
generalizations
regression
depth
nonfit
context
dependent
noise
regression
defined
vertical
hyperplane
bad
used
predict
values
dependent
variable
rousseeuw
hubert
hr99
define
depth
hyperplane
minimum
number
points
must
cross
continuous
motion
moves
nonfit
make
difference
restrict
class
continuous
motions
rotations
around
lower
dimensional
axis
equivalently
one
can
say
plane
partitions
data
two
subsets
depth
minimum
number
points
change
one
subset
get
partition
achievable
vertical
hyperplane
depth
plane
lower
bound
number
outliers
occurred
make
plane
inaccurate
dual
version
much
easier
understand
depth
point
arrangement
just
minimum
number
lines
hyperplanes
need
cross
get
point
infinity
good
fit
according
measure
just
point
high
depth
one
enclosed
large
number
hyperplanes
every
possible
direction
one
dimension
hyperplane
just
point
vertical
hyperplane
just
point
infinity
median
point
set
gives
optimal
depth
2
two
dimensions
simple
construction
catline
hr98
depth
3
optimal
since
one
can
find
sets
points
line
better
3
turns
point
set
dimension
hyperplane
depth
least
1
exactly
matching
quality
bounds
known
centerpoints
abet98
however
proof
nonconstructive
just
centerpoint
probably
want
deepest
possible
plane
just
deep
plane
can
found
efficiently
plane
kmrsss99
less
efficiently
higher
dimensions
one
can
also
efficiently
approximate
deepest
plane
linear
time
dimension
using
geometric
sampling
techniques
sw98
primal
formulation
estimator
depends
definition
vertical
direction
makes
sense
dependent
dual
independent
noise
models
independent
noise
invariant
affine
transformations
among
independent
variables
arbitrary
affine
transformations
dual
space
generally
continuous
deformation
space
ever
flatten
tetrahedron
produce
1
coplanar
points
make
vertical
triangle
points
forming
vertical
plane
next
clustering
david
eppstein
theory
group
dept
information
computer
science
uc
irvine
last
update
01
jun
1999
16
05
11
pdt
