data compression section 1 
fundamental concepts 
data
compression
1
fundamental
concepts
brief
introduction
information
theory
provided
section
definitions
assumptions
necessary
comprehensive
discussion
evaluation
data
compression
methods
discussed
following
string
characters
used
illustrate
concepts
defined
example
aa
bbb
cccc
ddddd
eeeeee
fffffffgggggggg
1
1
definitions
code
mapping
source
messages
words
source
alphabet
alpha
codewords
words
code
alphabet
beta
source
messages
basic
units
string
represented
partitioned
basic
units
may
single
symbols
source
alphabet
may
strings
symbols
string
example
alpha
space
purposes
explanation
beta
will
taken
0
1
codes
can
categorized
block
block
block
variable
variable
block
variable
variable
block
block
indicates
source
messages
codewords
fixed
length
variable
variable
codes
map
variable
length
source
messages
variable
length
codewords
block
block
code
example
shown
figure
1
1
variable
variable
code
given
figure
1
2
string
example
coded
using
figure
1
1
code
length
coded
message
120
using
figure
1
2
length
30
source
message
codeword
source
message
codeword
000
aa
0
001
bbb
1
010
cccc
10
011
ddddd
11
100
eeeeee
100
101
fffffff
101
110
gggggggg
110
space
111
space
111
figure
1
1
block
block
code
figure
1
2
variable
variable
code
oldest
widely
used
codes
ascii
ebcdic
examples
block
block
codes
mapping
alphabet
64
256
single
characters
onto
6
bit
8
bit
codewords
discussed
provide
compression
codes
featured
survey
block
variable
variable
variable
variable
block
types
source
messages
variable
length
allowed
question
message
ensemble
sequence
messages
parsed
individual
messages
arises
many
algorithms
described
defined
word
schemes
set
source
messages
determined
prior
invocation
coding
scheme
example
text
file
processing
character
may
constitute
message
messages
may
defined
consist
alphanumeric
non
alphanumeric
strings
pascal
source
code
token
may
represent
message
codes
involving
fixed
length
source
messages
default
defined
word
codes
free
parse
methods
coding
algorithm
parses
ensemble
variable
length
sequences
symbols
known
data
compression
methods
defined
word
schemes
free
parse
model
differs
fundamental
way
classical
coding
paradigm
code
distinct
codeword
distinguishable
every
mapping
source
messages
codewords
one
one
distinct
code
uniquely
decodable
every
codeword
identifiable
immersed
sequence
codewords
clearly
features
desirable
codes
figure
1
1
figure
1
2
distinct
code
figure
1
2
uniquely
decodable
example
coded
message
11
decoded
either
ddddd
bbbbbb
uniquely
decodable
code
prefix
code
prefix
free
code
prefix
property
requires
codeword
proper
prefix
codeword
uniquely
decodable
block
block
variable
block
codes
prefix
codes
code
codewords
1
100000
00
example
code
uniquely
decodable
prefix
property
prefix
codes
instantaneously
decodable
desirable
property
coded
message
can
parsed
codewords
without
need
lookahead
order
decode
message
encoded
using
codeword
set
1
100000
00
lookahead
required
example
first
codeword
message
1000000001
1
determined
last
tenth
symbol
message
read
string
zeros
odd
length
first
codeword
100000
minimal
prefix
code
prefix
code
proper
prefix
codeword
sigma
either
codeword
proper
prefix
codeword
letter
sigma
beta
set
codewords
00
01
10
example
prefix
code
minimal
fact
1
proper
prefix
codeword
10
requires
11
either
codeword
proper
prefix
codeword
neither
intuitively
minimality
constraint
prevents
use
codewords
longer
necessary
example
codeword
10
replaced
codeword
1
yielding
minimal
prefix
code
shorter
codewords
codes
discussed
paper
minimal
prefix
codes
section
code
defined
mapping
source
alphabet
code
alphabet
now
define
related
terms
process
transforming
source
ensemble
coded
message
coding
encoding
encoded
message
may
referred
encoding
source
ensemble
algorithm
constructs
mapping
uses
transform
source
ensemble
called
encoder
decoder
performs
inverse
operation
restoring
coded
message
original
form
1
2
classification
methods
addition
categorization
data
compression
schemes
respect
message
codeword
lengths
methods
classified
either
static
dynamic
static
method
one
mapping
set
messages
set
codewords
fixed
transmission
begins
given
message
represented
codeword
every
time
appears
message
ensemble
classic
static
defined
word
scheme
huffman
coding
huffman
1952
huffman
coding
assignment
codewords
source
messages
based
probabilities
source
messages
appear
message
ensemble
messages
appear
frequently
represented
short
codewords
messages
smaller
probabilities
map
longer
codewords
probabilities
determined
transmission
begins
huffman
code
ensemble
example
given
figure
1
3
example
coded
using
huffman
mapping
length
coded
message
117
static
huffman
coding
discussed
section
3
2
static
schemes
discussed
sections
2
3
source
message
probability
codeword
2
40
1001
3
40
1000
4
40
011
5
40
010
6
40
111
7
40
110
8
40
00
space
5
40
101
figure
1
3
huffman
code
message
example
code
length
117
code
dynamic
mapping
set
messages
set
codewords
changes
time
example
dynamic
huffman
coding
involves
computing
approximation
probabilities
occurrence
fly
ensemble
transmitted
assignment
codewords
messages
based
values
relative
frequencies
occurrence
point
time
message
may
represented
short
codeword
early
transmission
occurs
frequently
beginning
ensemble
even
though
probability
occurrence
total
ensemble
low
later
probable
messages
begin
occur
higher
frequency
short
codeword
will
mapped
one
higher
probability
messages
will
mapped
longer
codeword
illustration
figure
1
4
presents
dynamic
huffman
code
table
corresponding
prefix
aa
bbb
example
although
frequency
space
entire
message
greater
point
time
higher
frequency
therefore
mapped
shorter
codeword
source
message
probability
codeword
2
6
10
3
6
0
space
1
6
11
figure
1
4
dynamic
huffman
code
table
prefix
aa
bbb
message
example
dynamic
codes
also
referred
literature
adaptive
adapt
changes
ensemble
characteristics
time
term
adaptive
will
used
remainder
paper
fact
codes
adapt
changing
characteristics
source
appeal
adaptive
methods
adapt
changing
patterns
source
welch
1984
others
exploit
locality
reference
bentley
et
al
1986
locality
reference
tendency
common
wide
variety
text
types
particular
word
occur
frequently
short
periods
time
fall
disuse
long
periods
adaptive
methods
one
pass
methods
one
scan
ensemble
required
static
huffman
coding
requires
two
passes
one
pass
compute
probabilities
determine
mapping
second
pass
transmission
thus
long
encoding
decoding
times
adaptive
method
substantially
greater
static
method
fact
initial
scan
needed
implies
speed
improvement
adaptive
case
addition
mapping
determined
first
pass
static
coding
scheme
must
transmitted
encoder
decoder
mapping
may
preface
transmission
file
sent
single
mapping
may
agreed
upon
used
multiple
transmissions
one
pass
methods
encoder
defines
redefines
mapping
dynamically
transmission
decoder
must
define
redefine
mapping
sympathy
essence
learning
mapping
codewords
received
adaptive
methods
discussed
sections
4
5
algorithm
may
also
hybrid
neither
completely
static
completely
dynamic
simple
hybrid
scheme
sender
receiver
maintain
identical
codebooks
containing
static
codes
transmission
sender
must
choose
one
previously
agreed
upon
codes
inform
receiver
choice
transmitting
first
name
number
chosen
code
hybrid
methods
discussed
section
2
section
3
2
1
3
data
compression
model
order
discuss
relative
merits
data
compression
techniques
framework
comparison
must
established
two
dimensions
along
schemes
discussed
may
measured
algorithm
complexity
amount
compression
data
compression
used
data
transmission
application
goal
speed
speed
transmission
depends
upon
number
bits
sent
time
required
encoder
generate
coded
message
time
required
decoder
recover
original
ensemble
data
storage
application
although
degree
compression
primary
concern
nonetheless
necessary
algorithm
efficient
order
scheme
practical
static
scheme
three
algorithms
analyze
map
construction
algorithm
encoding
algorithm
decoding
algorithm
dynamic
scheme
just
two
algorithms
encoding
algorithm
decoding
algorithm
several
common
measures
compression
suggested
redundancy
shannon
weaver
1949
average
message
length
huffman
1952
compression
ratio
rubin
1976
ruth
kreutzer
1972
measures
defined
related
measures
assumptions
characteristics
source
generally
assumed
information
theory
statistical
parameters
message
source
known
perfect
accuracy
gilbert
1971
common
model
discrete
memoryless
source
source
whose
output
sequence
letters
messages
letter
selection
fixed
alphabet
letters
taken
random
statistically
independent
selections
alphabet
selection
made
according
fixed
probability
assignment
gallager
1968
without
loss
generality
code
alphabet
assumed
0
1
throughout
paper
modifications
necessary
larger
code
alphabets
straightforward
assumed
cost
associated
code
letters
uniform
reasonable
assumption
although
omits
applications
like
telegraphy
code
symbols
different
durations
assumption
also
important
since
problem
constructing
optimal
codes
unequal
code
letter
costs
significantly
different
difficult
problem
perl
et
al
varn
developed
algorithms
minimum
redundancy
prefix
coding
case
arbitrary
symbol
cost
equal
codeword
probability
perl
et
al
1975
varn
1971
assumption
equal
probabilities
mitigates
difficulty
presented
variable
symbol
cost
general
unequal
letter
costs
unequal
probabilities
model
karp
proposed
integer
linear
programming
approach
karp
1961
several
approximation
algorithms
proposed
difficult
problem
krause
1962
cot
1977
mehlhorn
1980
data
compressed
goal
reduce
redundancy
leaving
informational
content
measure
information
source
message
bits
lg
lg
denotes
base
2
logarithm
definition
intuitive
appeal
case
1
clear
informative
since
occur
similarly
smaller
value
unlikely
appear
hence
larger
information
content
reader
referred
abramson
longer
elegant
discussion
legitimacy
technical
definition
concept
information
abramson
1963
pp
6
13
average
information
content
source
alphabet
can
computed
weighting
information
content
source
letter
probability
occurrence
yielding
expression
sum
1
lg
quantity
referred
entropy
source
letter
entropy
source
denoted
since
length
codeword
message
must
sufficient
carry
information
content
entropy
imposes
lower
bound
number
bits
required
coded
message
total
number
bits
must
least
large
product
length
source
ensemble
since
value
generally
integer
variable
length
codewords
must
used
lower
bound
achieved
given
message
example
encoded
one
letter
time
entropy
source
can
calculated
using
probabilities
given
figure
1
3
2
894
minimum
number
bits
contained
encoding
example
116
huffman
code
given
section
1
2
quite
achieve
theoretical
minimum
case
definitions
information
content
due
shannon
derivation
concept
entropy
relates
information
theory
presented
shannon
shannon
weaver
1949
simpler
intuitive
explanation
entropy
offered
ash
ash
1965
common
notion
good
code
one
optimal
sense
minimum
redundancy
redundancy
can
defined
sum
sum
lg
length
codeword
representing
message
expression
sum
represents
lengths
codewords
weighted
probabilities
occurrence
average
codeword
length
expression
sum
lg
entropy
thus
redundancy
measure
difference
average
codeword
length
average
information
content
code
minimum
average
codeword
length
given
discrete
probability
distribution
said
minimum
redundancy
code
define
term
local
redundancy
capture
notion
redundancy
caused
local
properties
message
ensemble
rather
global
characteristics
model
used
analyzing
general
purpose
coding
techniques
assumes
random
distribution
source
messages
may
actually
case
particular
applications
tendency
messages
cluster
predictable
patterns
may
known
existence
predictable
patterns
may
exploited
minimize
local
redundancy
examples
applications
local
redundancy
common
methods
dealing
local
redundancy
discussed
section
2
section
6
2
huffman
uses
average
message
length
sum
measure
efficiency
code
clearly
meaning
term
average
length
coded
message
will
use
term
average
codeword
length
represent
quantity
since
redundancy
defined
average
codeword
length
minus
entropy
entropy
constant
given
probability
distribution
minimizing
average
codeword
length
minimizes
redundancy
code
asymptotically
optimal
property
given
probability
distribution
ratio
average
codeword
length
entropy
approaches
1
entropy
tends
infinity
asymptotic
optimality
guarantees
average
codeword
length
approaches
theoretical
minimum
entropy
represents
information
content
imposes
lower
bound
codeword
length
amount
compression
yielded
coding
scheme
can
measured
compression
ratio
term
compression
ratio
defined
several
ways
definition
average
message
length
average
codeword
length
captures
common
meaning
comparison
length
coded
message
length
original
ensemble
cappellini
1985
think
characters
ensemble
example
6
bit
ascii
characters
average
message
length
6
bits
huffman
code
section
1
2
represents
example
117
bits
2
9
bits
per
character
yields
compression
ratio
6
2
9
representing
compression
factor
2
alternatively
may
say
huffman
encoding
produces
file
whose
size
49
original
ascii
file
49
compression
achieved
somewhat
different
definition
compression
ratio
rubin
includes
representation
code
transmission
cost
rubin
1976
definition
represents
length
source
ensemble
length
output
coded
message
size
output
representation
eg
number
bits
required
encoder
transmit
code
mapping
decoder
quantity
constitutes
charge
algorithm
transmission
information
coding
scheme
intention
measure
total
size
transmission
file
stored
1
4
motivation
discussed
introduction
data
compression
wide
application
terms
information
storage
including
representation
abstract
data
type
string
standish
1980
file
compression
huffman
coding
used
compression
several
file
archival
systems
arc
1986
pkarc
1987
lempel
ziv
coding
one
adaptive
schemes
discussed
section
5
adaptive
huffman
coding
technique
basis
compact
command
unix
operating
system
unix
compress
utility
employs
lempel
ziv
approach
unix
1984
area
data
transmission
huffman
coding
passed
years
favor
block
block
codes
notably
ascii
advantage
huffman
coding
average
number
bits
per
character
transmitted
may
much
smaller
lg
bits
per
character
source
alphabet
size
block
block
system
primary
difficulty
associated
variable
length
codewords
rate
bits
presented
transmission
channel
will
fluctuate
depending
relative
frequencies
source
messages
requires
buffering
source
channel
advances
technology
overcome
difficulty
contributed
appeal
variable
length
codes
current
data
networks
allocate
communication
resources
sources
basis
need
provide
buffering
part
system
systems
require
significant
amounts
protocol
fixed
length
codes
quite
inefficient
applications
packet
headers
addition
communication
costs
beginning
dominate
storage
processing
costs
variable
length
coding
schemes
reduce
communication
costs
attractive
even
complex
reasons
one
expect
see
even
greater
use
variable
length
coding
future
interesting
note
huffman
coding
algorithm
originally
developed
efficient
transmission
data
also
wide
variety
applications
outside
sphere
data
compression
include
construction
optimal
search
trees
zimmerman
1959
hu
tucker
1971
itai
1976
list
merging
brent
kung
1978
generating
optimal
evaluation
trees
compilation
expressions
parker
1980
additional
applications
involve
search
jumps
monotone
function
single
variable
sources
pollution
along
river
leaks
pipeline
glassey
karp
1976
fact
elegant
combinatorial
algorithm
influenced
many
diverse
areas
underscores
importance
