data compression section 3 
static defined word schemes 
data
compression
3
static
defined
word
schemes
classic
defined
word
scheme
developed
30
years
ago
huffman's
well
known
paper
minimum
redundancy
coding
huffman
1952
huffman's
algorithm
provided
first
solution
problem
constructing
minimum
redundancy
codes
many
people
believe
huffman
coding
improved
upon
guaranteed
achieve
best
possible
compression
ratio
true
however
constraints
source
message
mapped
unique
codeword
compressed
text
concatenation
codewords
source
messages
earlier
algorithm
due
independently
shannon
fano
shannon
weaver
1949
fano
1949
guaranteed
provide
optimal
codes
approaches
optimal
behavior
number
messages
approaches
infinity
huffman
algorithm
also
importance
provided
foundation
upon
data
compression
techniques
built
benchmark
may
compared
classify
codes
generated
huffman
shannon
fano
algorithms
variable
variable
note
include
block
variable
codes
special
case
depending
upon
source
messages
defined
section
3
3
codes
map
integers
onto
binary
codewords
discussed
since
finite
alphabet
may
enumerated
type
code
general
purpose
utility
however
common
use
codes
called
universal
codes
conjunction
adaptive
scheme
connection
discussed
section
5
2
arithmetic
coding
presented
section
3
4
takes
significantly
different
approach
data
compression
static
methods
construct
code
sense
mapping
source
messages
codewords
instead
arithmetic
coding
replaces
source
ensemble
code
string
unlike
codes
discussed
concatenation
codewords
corresponding
individual
source
messages
arithmetic
coding
capable
achieving
compression
results
arbitrarily
close
entropy
source
3
1
shannon
fano
coding
shannon
fano
technique
advantage
simplicity
code
constructed
follows
source
messages
probabilities
listed
order
nonincreasing
probability
list
divided
way
form
two
groups
nearly
equal
total
probabilities
possible
message
first
group
receives
0
first
digit
codeword
messages
second
half
codewords
beginning
1
groups
divided
according
criterion
additional
code
digits
appended
process
continued
subset
contains
one
message
clearly
shannon
fano
algorithm
yields
minimal
prefix
code
1
2
0
1
4
10
1
8
110
1
16
1110
1
32
11110
1
32
11111
figure
3
1
shannon
fano
code
figure
3
1
shows
application
method
particularly
simple
probability
distribution
length
codeword
equal
lg
true
long
possible
divide
list
subgroups
exactly
equal
probability
possible
codewords
may
length
lg
1
shannon
fano
algorithm
yields
average
codeword
length
satisfies
1
figure
3
2
shannon
fano
code
ensemble
example
given
often
case
average
codeword
length
achieved
huffman
code
see
figure
1
3
shannon
fano
algorithm
guaranteed
produce
optimal
code
demonstrated
following
set
probabilities
35
17
17
16
15
shannon
fano
code
distribution
compared
huffman
code
section
3
2
8
40
00
7
40
010
6
40
011
5
40
100
space
5
40
101
4
40
110
3
40
1110
2
40
1111
figure
3
2
shannon
fano
code
example
code
length
117
3
2
static
huffman
coding
huffman's
algorithm
expressed
graphically
takes
input
list
nonnegative
weights
1
constructs
full
binary
tree
binary
tree
full
every
node
either
zero
two
children
whose
leaves
labeled
weights
huffman
algorithm
used
construct
code
weights
represent
probabilities
associated
source
letters
initially
set
singleton
trees
one
weight
list
step
algorithm
trees
corresponding
two
smallest
weights
merged
new
tree
whose
weight
whose
root
two
children
subtrees
represented
weights
removed
list
inserted
list
process
continues
weight
list
contains
single
value
time
one
way
choose
smallest
pair
weights
pair
may
chosen
huffman's
paper
process
begins
nonincreasing
list
weights
detail
important
correctness
algorithm
provide
efficient
implementation
huffman
1952
huffman
algorithm
demonstrated
figure
3
3
figure
3
3
huffman
process
list
tree
huffman
algorithm
determines
lengths
codewords
mapped
source
letters
many
alternatives
specifying
actual
digits
necessary
code
prefix
property
usual
assignment
entails
labeling
edge
parent
left
child
digit
0
edge
right
child
1
codeword
source
letter
sequence
labels
along
path
root
leaf
node
representing
letter
codewords
source
figure
3
3
order
decreasing
probability
01
11
001
100
101
000
0001
clearly
process
yields
minimal
prefix
code
algorithm
guaranteed
produce
optimal
minimum
redundancy
code
huffman
1952
gallager
proved
upper
bound
redundancy
huffman
code
lg
2
lg
approximately
0
086
probability
least
likely
source
message
gallager
1978
recent
paper
capocelli
et
al
provide
new
bounds
tighter
gallagher
probability
distributions
capocelli
et
al
1986
figure
3
4
shows
distribution
huffman
code
optimal
shannon
fano
code
addition
fact
many
ways
forming
codewords
appropriate
lengths
cases
huffman
algorithm
uniquely
determine
lengths
due
arbitrary
choice
among
equal
minimum
weights
example
codes
codeword
lengths
1
2
3
4
4
2
2
2
3
3
yield
average
codeword
length
source
probabilities
4
2
2
1
1
schwartz
defines
variation
huffman
algorithm
performs
bottom
merging
orders
new
parent
node
existing
nodes
weight
always
merges
last
two
weights
list
code
constructed
huffman
code
minimum
values
maximum
codeword
length
max
total
codeword
length
sum
schwartz
1964
schwartz
kallick
describe
implementation
huffman's
algorithm
bottom
merging
schwartz
kallick
1964
schwartz
kallick
algorithm
later
algorithm
connell
connell
1973
use
huffman's
procedure
determine
lengths
codewords
actual
digits
assigned
code
numerical
sequence
property
codewords
equal
length
form
consecutive
sequence
binary
numbers
shannon
fano
codes
also
numerical
sequence
property
property
can
exploited
achieve
compact
representation
code
rapid
encoding
decoding
huffman
1
0
35
00
1
2
0
17
01
011
3
0
17
10
010
4
0
16
110
001
5
0
15
111
000
average
codeword
length
2
31
2
30
figure
3
4
comparison
shannon
fano
huffman
codes
huffman
shannon
fano
mappings
can
generated
time
number
messages
source
ensemble
assuming
weights
presorted
algorithms
maps
source
message
probability
codeword
length
lg
lg
1
encoding
decoding
times
depend
upon
representation
mapping
mapping
stored
binary
tree
decoding
codeword
involves
following
path
length
tree
table
indexed
source
messages
used
encoding
code
stored
position
table
encoding
time
connell's
algorithm
makes
use
index
huffman
code
representation
distribution
codeword
lengths
encode
decode
time
number
different
codeword
lengths
tanaka
presents
implementation
huffman
coding
based
finite
state
machines
can
realized
efficiently
either
hardware
software
tanaka
1987
noted
earlier
redundancy
bound
shannon
fano
codes
1
bound
huffman
method
0
086
probability
least
likely
source
message
less
equal
5
generally
much
less
important
note
defining
redundancy
average
codeword
length
minus
entropy
cost
transmitting
code
mapping
computed
algorithms
ignored
overhead
cost
method
source
alphabet
established
prior
transmission
includes
lg
bits
sending
source
letters
shannon
fano
code
list
codewords
ordered
correspond
source
letters
transmitted
additional
time
required
sum
lengths
codewords
huffman
coding
encoding
shape
code
tree
might
transmitted
since
full
binary
tree
may
legal
huffman
code
tree
encoding
tree
shape
may
require
many
lg
4
2n
bits
cases
message
ensemble
large
number
bits
overhead
minute
comparison
total
length
encoded
transmission
however
imprudent
ignore
cost
less
optimal
code
acceptable
overhead
costs
can
avoided
prior
agreement
sender
receiver
code
mapping
rather
using
huffman
code
based
upon
characteristics
current
message
ensemble
code
used
based
statistics
class
transmissions
current
ensemble
assumed
belong
sender
receiver
access
codebook
mappings
one
pascal
source
one
english
text
etc
sender
simply
alert
receiver
common
codes
using
requires
lg
bits
overhead
assuming
classes
transmission
relatively
stable
characteristics
identified
hybrid
approach
greatly
reduce
redundancy
due
overhead
without
significantly
increasing
expected
codeword
length
addition
cost
computing
mapping
amortized
files
given
class
mapping
computed
statistically
significant
sample
used
great
number
files
sample
representative
clearly
substantial
risk
associated
assumptions
file
characteristics
great
care
necessary
choosing
sample
mapping
derived
categories
partition
transmissions
extreme
example
risk
associated
codebook
approach
provided
author
ernest
wright
wrote
novel
gadsby
1939
containing
occurrences
letter
since
commonly
used
letter
english
language
encoding
based
upon
sample
gadsby
disastrous
used
normal
examples
english
text
similarly
normal
encoding
provide
poor
compression
gadsby
mcintyre
pechura
describe
experiment
codebook
approach
compared
static
huffman
coding
mcintyre
pechura
1985
sample
used
comparison
collection
530
source
programs
four
languages
codebook
contains
pascal
code
tree
fortran
code
tree
cobol
code
tree
pl
1
code
tree
code
tree
pascal
code
tree
result
applying
static
huffman
algorithm
combined
character
frequencies
pascal
programs
sample
code
tree
based
upon
combined
character
frequencies
programs
experiment
involves
encoding
programs
using
five
codes
codebook
static
huffman
algorithm
data
reported
530
programs
consists
size
coded
program
five
predetermined
codes
size
coded
program
plus
size
mapping
table
form
static
huffman
method
every
case
code
tree
language
class
program
belongs
generates
compact
encoding
although
using
huffman
algorithm
program
yields
optimal
mapping
overhead
cost
greater
added
redundancy
incurred
less
optimal
code
many
cases
code
tree
also
generates
compact
encoding
static
huffman
algorithm
worst
case
encoding
constructed
codebook
6
6
larger
constructed
huffman
algorithm
results
suggest
files
source
code
codebook
approach
may
appropriate
gilbert
discusses
construction
huffman
codes
based
inaccurate
source
probabilities
gilbert
1971
simple
solution
problem
incomplete
knowledge
source
avoid
long
codewords
thereby
minimizing
error
underestimating
badly
probability
message
problem
becomes
one
constructing
optimal
binary
tree
subject
height
restriction
see
knuth
1971
hu
tan
1972
garey
1974
another
approach
involves
collecting
statistics
several
sources
constructing
code
based
upon
combined
criterion
approach
applied
problem
designing
single
code
use
english
french
german
etc
sources
accomplish
huffman's
algorithm
used
minimize
either
average
codeword
length
combined
source
probabilities
average
codeword
length
english
subject
constraints
average
codeword
lengths
sources
3
3
universal
codes
representations
integers
code
universal
maps
source
messages
codewords
resulting
average
codeword
length
bounded
c1
c2
given
arbitrary
source
nonzero
entropy
universal
code
achieves
average
codeword
length
constant
times
optimal
possible
source
potential
compression
offered
universal
code
clearly
depends
magnitudes
constants
c1
c2
recall
definition
asymptotically
optimal
code
one
average
codeword
length
approaches
entropy
remark
universal
code
c1
1
asymptotically
optimal
advantage
universal
codes
huffman
codes
necessary
know
exact
probabilities
source
messages
appear
huffman
coding
applicable
unless
probabilities
known
sufficient
case
universal
coding
know
probability
distribution
extent
source
messages
can
ranked
probability
order
mapping
messages
order
decreasing
probability
codewords
order
increasing
length
universality
can
achieved
another
advantage
universal
codes
codeword
sets
fixed
necessary
compute
codeword
set
based
upon
statistics
ensemble
universal
codeword
set
will
suffice
long
source
messages
ranked
encoding
decoding
processes
thus
simplified
universal
codes
can
used
instead
huffman
codes
general
purpose
static
schemes
common
application
adjunct
dynamic
scheme
type
application
will
demonstrated
section
5
since
ranking
source
messages
essential
parameter
universal
coding
may
think
universal
code
representing
enumeration
source
messages
representing
integers
provide
enumeration
elias
defines
sequence
universal
coding
schemes
map
set
positive
integers
onto
set
binary
codewords
elias
1975
gamma
delta
1
1
1
2
010
0100
3
011
0101
4
00100
01100
5
00101
01101
6
00110
01110
7
00111
01111
8
0001000
00100000
16
000010000
001010000
17
000010001
001010001
32
00000100000
0011000000
figure
3
5
elias
codes
first
elias
code
one
simple
optimal
code
gamma
maps
integer
onto
binary
value
prefaced
floor
lg
zeros
binary
value
expressed
bits
possible
therefore
begins
1
serves
delimit
prefix
result
instantaneously
decodable
code
since
total
length
codeword
exactly
one
greater
twice
number
zeros
prefix
therefore
soon
first
1
codeword
encountered
length
known
code
minimum
redundancy
code
since
ratio
expected
codeword
length
entropy
goes
2
entropy
approaches
infinity
second
code
delta
maps
integer
codeword
consisting
gamma
floor
lg
1
followed
binary
value
leading
1
deleted
resulting
codeword
length
floor
lg
2
floor
lg
1
floor
lg
1
concept
can
applied
recursively
shorten
codeword
lengths
benefits
decrease
rapidly
code
delta
asymptotically
optimal
since
limit
ratio
expected
codeword
length
entropy
1
figure
3
5
lists
values
gamma
delta
sampling
integers
figure
3
6
shows
elias
code
string
example
number
bits
transmitted
using
mapping
161
compare
well
117
bits
transmitted
huffman
code
figure
1
3
huffman
coding
optimal
static
mapping
model
even
asymptotically
optimal
universal
code
compare
static
huffman
coding
source
probabilities
messages
known
source
frequency
rank
codeword
message
8
1
delta
1
1
7
2
delta
2
0100
6
3
delta
3
0101
5
4
delta
4
01100
space
5
5
delta
5
01101
4
6
delta
6
01110
3
7
delta
7
01111
2
8
delta
8
00100000
figure
3
6
elias
code
example
code
length
161
second
sequence
universal
coding
schemes
based
fibonacci
numbers
defined
apostolico
fraenkel
apostolico
fraenkel
1985
fibonacci
codes
asymptotically
optimal
compare
well
elias
codes
long
number
source
messages
large
fibonacci
codes
additional
attribute
robustness
manifests
local
containment
errors
aspect
fibonacci
codes
will
discussed
section
7
sequence
fibonacci
codes
described
apostolico
fraenkel
based
fibonacci
numbers
order
2
fibonacci
numbers
order
2
standard
fibonacci
numbers
1
1
2
3
5
8
13
general
fibonnaci
numbers
order
defined
recurrence
fibonacci
numbers
1
0
equal
1
kth
number
1
sum
preceding
numbers
describe
order
2
fibonacci
code
extension
higher
orders
straightforward
1
1
11
2
1
0
011
3
1
0
0
0011
4
1
0
1
1011
5
1
0
0
0
00011
6
1
0
0
1
10011
7
1
0
1
0
01011
8
1
0
0
0
0
000011
16
1
0
0
1
0
0
0010011
32
1
0
1
0
1
0
0
00101011
21
13
8
5
3
2
1
figure
3
7
fibonacci
representations
fibonacci
codes
every
nonnegative
integer
precisely
one
binary
representation
form
sum
0
0
1
order
2
fibonacci
numbers
defined
adjacent
ones
representation
fibonacci
representations
small
sampling
integers
shown
figure
3
7
using
standard
bit
sequence
high
order
low
bottom
row
figure
gives
values
bit
positions
immediately
obvious
fibonacci
representation
constitute
prefix
code
order
2
fibonacci
code
defined
d1
0
1
2
defined
fibonacci
representation
reversed
1
appended
fibonacci
code
values
small
subset
integers
given
figure
3
7
binary
codewords
form
prefix
code
since
every
codeword
now
terminates
two
consecutive
ones
appear
anywhere
else
codeword
fraenkel
klein
prove
fibonacci
code
order
2
universal
c1
2
c2
3
fraenkel
klein
1985
asymptotically
optimal
since
c1
1
fraenkel
klein
also
show
fibonacci
codes
higher
order
compress
better
order
2
code
source
language
large
enough
number
distinct
source
messages
large
probability
distribution
nearly
uniform
however
fibonacci
code
asymptotically
optimal
elias
codeword
delta
asymptotically
shorter
fibonacci
codeword
integers
large
initial
range
shorter
fibonacci
codewords
2
example
transition
point
514
228
apostolico
fraenkel
1985
thus
fibonacci
code
provides
better
compression
elias
code
size
source
language
becomes
large
figure
3
8
shows
fibonacci
code
string
example
number
bits
transmitted
using
mapping
153
improvement
elias
code
figure
3
6
still
compares
poorly
huffman
code
figure
1
3
source
frequency
rank
codeword
message
8
1
1
11
7
2
2
011
6
3
3
0011
5
4
4
1011
space
5
5
5
00011
4
6
6
10011
3
7
7
01011
2
8
8
000011
figure
3
8
fibonacci
code
example
code
length
153
3
4
arithmetic
coding
method
arithmetic
coding
suggested
elias
presented
abramson
text
information
theory
abramson
1963
implementations
elias'
technique
developed
rissanen
pasco
rubin
recently
witten
et
al
rissanen
1976
pasco
1976
rubin
1979
witten
et
al
1987
present
concept
arithmetic
coding
first
follow
discussion
implementation
details
performance
arithmetic
coding
source
ensemble
represented
interval
0
1
real
number
line
symbol
ensemble
narrows
interval
interval
becomes
smaller
number
bits
needed
specify
grows
arithmetic
coding
assumes
explicit
probabilistic
model
source
defined
word
scheme
uses
probabilities
source
messages
successively
narrow
interval
used
represent
ensemble
high
probability
message
narrows
interval
less
low
probability
message
high
probability
messages
contribute
fewer
bits
coded
ensemble
method
begins
unordered
list
source
messages
probabilities
number
line
partitioned
subintervals
based
cumulative
probabilities
small
example
will
used
illustrate
idea
arithmetic
coding
given
source
messages
probabilities
2
4
1
2
1
figure
3
9
demonstrates
initial
partitioning
number
line
symbol
corresponds
first
1
5
interval
0
1
next
2
5
subinterval
size
1
5
begins
70
way
left
endpoint
right
encoding
begins
source
ensemble
represented
entire
interval
0
1
ensemble
aadb
first
reduces
interval
0
2
second
0
04
first
1
5
previous
interval
narrows
interval
028
036
1
5
previous
size
beginning
70
distance
left
right
narrows
interval
0296
0328
yields
final
interval
03248
0328
interval
alternatively
number
within
interval
may
now
used
represent
source
ensemble
source
probability
cumulative
range
message
probability
2
2
0
2
4
6
2
6
1
7
6
7
2
9
7
9
1
1
0
9
1
0
figure
3
9
arithmetic
coding
model
two
equations
may
used
define
narrowing
process
described
newleft
prevleft
msgleft
prevsize
1
newsize
prevsize
msgsize
2
first
equation
states
left
endpoint
new
interval
calculated
previous
interval
current
source
message
left
endpoint
range
associated
current
message
specifies
percent
previous
interval
remove
left
order
form
new
interval
example
new
left
endpoint
moved
7
04
70
size
previous
interval
second
equation
computes
size
new
interval
previous
interval
size
probability
current
message
equivalent
size
associated
range
thus
size
interval
determined
04
2
right
endpoint
028
008
036
left
endpoint
size
size
final
subinterval
determines
number
bits
needed
specify
number
range
number
bits
needed
specify
subinterval
0
1
size
lg
since
size
final
subinterval
product
probabilities
source
messages
ensemble
prod
1
source
message
length
ensemble
lg
sum
1
lg
source
message
sum
1
lg
number
unique
source
messages
1
2
thus
number
bits
generated
arithmetic
coding
technique
exactly
equal
entropy
demonstrates
fact
arithmetic
coding
achieves
compression
almost
exactly
predicted
entropy
source
order
recover
original
ensemble
decoder
must
know
model
source
used
encoder
eg
source
messages
associated
ranges
single
number
within
interval
determined
encoder
decoding
consists
series
comparisons
number
ranges
representing
source
messages
example
might
0325
03248
0326
0327
just
well
decoder
uses
simulate
actions
encoder
since
lies
0
2
deduces
first
letter
since
range
0
2
corresponds
source
message
narrows
interval
0
2
decoder
can
now
deduce
next
message
will
narrow
interval
one
following
ways
0
04
04
12
12
14
14
18
18
2
since
falls
interval
0
04
knows
second
message
process
continues
entire
ensemble
recovered
several
difficulties
become
evident
implementation
arithmetic
coding
attempted
first
decoder
needs
way
knowing
stop
evidence
number
0
represent
source
ensembles
aa
aaa
etc
two
solutions
problem
suggested
one
encoder
transmit
size
ensemble
part
description
model
another
special
symbol
included
model
purpose
signaling
end
message
example
serves
purpose
second
alternative
preferable
several
reasons
first
sending
size
ensemble
requires
two
pass
process
precludes
use
arithmetic
coding
part
hybrid
codebook
scheme
see
sections
1
2
3
2
secondly
adaptive
methods
arithmetic
coding
easily
developed
first
pass
determine
ensemble
size
inappropriate
line
adaptive
scheme
second
issue
left
unresolved
fundamental
concept
arithmetic
coding
incremental
transmission
reception
appears
discussion
encoding
algorithm
transmits
nothing
final
interval
determined
however
delay
necessary
interval
narrows
leading
bits
left
right
endpoints
become
leading
bits
may
transmitted
immediately
will
affected
narrowing
third
issue
precision
description
arithmetic
coding
appears
precision
required
grows
without
bound
length
ensemble
grows
witten
et
al
rubin
address
issue
witten
et
al
1987
rubin
1979
fixed
precision
registers
may
used
long
underflow
overflow
detected
managed
degree
compression
achieved
implementation
arithmetic
coding
exactly
implied
concept
arithmetic
coding
use
message
terminator
use
fixed
length
arithmetic
reduce
coding
effectiveness
however
clear
end
message
symbol
will
significant
effect
large
source
ensemble
witten
et
al
approximate
overhead
due
use
fixed
precision
10
4
bits
per
source
message
also
negligible
arithmetic
coding
model
ensemble
example
given
figure
3
10
final
interval
size
2
3
4
5
6
7
8
space
5
number
bits
needed
specify
value
interval
lg
1
44
10
35
115
7
excluding
overhead
arithmetic
coding
transmits
example
116
bits
one
less
bit
static
huffman
coding
source
probability
cumulative
range
message
probability
05
05
0
05
075
125
05
125
1
225
125
225
125
35
225
35
15
5
35
5
175
675
5
675
2
875
675
875
space
125
1
0
875
1
0
figure
3
10
arithmetic
coding
model
example
witten
et
al
provide
implementation
arithmetic
coding
written
separates
model
source
coding
process
coding
process
defined
equations
3
1
3
2
witten
et
al
1987
model
separate
program
module
consulted
encoder
decoder
every
step
processing
fact
model
can
separated
easily
renders
classification
static
adaptive
irrelevent
technique
indeed
fact
coding
method
provides
compression
efficiency
nearly
equal
entropy
source
model
allows
arithmetic
coding
coupled
static
adaptive
method
computing
probabilities
frequencies
source
messages
witten
et
al
implement
adaptive
model
similar
techniques
described
section
4
performance
implementation
discussed
section
6
