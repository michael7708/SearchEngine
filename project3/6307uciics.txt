interface '01 sessions 
special invited sessions 
interface
'01
sessions
thursday
june
14th
10
45am
12
30pm
statistical
graphics
flexible
models
prediction
model
based
clustering
time
series
sequential
analysis
office
naval
research
overview
2
00pm
3
45pm
visualization
data
mining
beyond
correlation
statistical
models
text
bayesian
methods
army
research
office
overview
4
15pm
6
00pm
gene
expression
graphical
models
environmental
modeling
computational
finance
national
security
agency
overview
friday
june
15th
10
45am
12
30pm
analyzing
web
data
bayesian
bioinformatics
john
tukey
interface
ecological
earth
science
applications
international
association
statistical
computing
overview
2
00pm
3
45pm
large
web
support
vector
machines
chernoff
faces
interface
clusters
outliers
density
models
journal
computational
graphics
statistics
overview
4
15pm
6
00pm
software
support
bayesian
analysis
systems
massive
data
sets
census
2000
lessons
census
2010
gene
expression
ii
national
science
foundation
overview
saturday
june
16th
9
00am
10
45am
computational
methods
tools
statistical
software
web
based
applications
decision
support
forecasting
classification
methods
11
05am
12
50pm
regression
function
approximation
visualization
image
data
data
mining
sampling
resampling
methods
bioinformatics
day
saturday
june
16
statistical
graphics
session
organizer
leland
wilkinson
session
time
thursday
june
14th
10
45am
12
30pm
room
location
santa
ana
room
session
chair
leland
wilkinson
making
trees
interactive
klimt
author
simon
urbanek
university
augsburg
germany
antony
unwin
university
augsburg
germany
abstract
trees
valuable
way
displaying
structure
data
sets
adding
interactive
tools
make
even
valuable
paper
describes
prototype
software
klimt
classification
interactive
methods
trees
interactive
graphical
analysis
trees
research
work
progress
many
different
possible
options
features
analysts
want
graphics
recursive
partitioning
author
daniel
carr
george
mason
university
ru
sun
george
mason
university
abstract
paper
presents
graphical
templates
constructing
describing
studying
recursive
partitioning
trees
templates
multiple
applications
example
layout
approaches
showing
random
trees
generally
applicable
display
clusters
thus
paper
may
interest
make
little
use
recursive
partitioning
paper
provides
perspective
raising
general
issues
one
issue
concerns
limits
human
sensory
input
resulting
conscious
awareness
limits
cause
humility
face
overwhelming
quantities
data
example
one
paper
indicates
using
recursive
partitioning
algorithm
problem
two
million
variables
analysts
look
data
much
smaller
problems
inevitably
end
looking
caricatures
data
making
assumption
still
beneficial
analysts
involved
analysis
process
seems
thought
computational
power
devoted
producing
prioritizing
caricatures
exploit
analysts'
visual
processing
strengths
terms
constructing
trees
dynamic
example
shows
approach
using
grand
tour
brushing
alphablending
graphical
partitioning
build
trees
visual
approach
uses
linear
combinations
predictor
variables
data
view
allows
partitioning
one
predictor
variable
approach
includes
type
look
ahead
compared
one
variable
time
algorithm
generally
views
can
smoothed
regression
surfaces
various
approaches
can
used
graphically
define
multivariate
partitions
view
used
partitioning
may
well
chosen
thus
projection
pursuit
related
algorithms
can
help
analysts
select
views
trees
displays
can
use
graphical
representations
show
partition
boundaries
can
done
traditional
well
graphically
defined
partitions
paper
emphasizes
graphical
possibilities
evaluate
quality
analyst
defined
trees
adjusting
significance
tests
human
defined
partitions
multiple
comparison
open
research
question
algorithm
emulation
possibilities
generally
graphics
can
also
used
evaluation
process
one
evaluation
process
generates
different
trees
weighted
random
selection
prioritized
variables
partitioning
step
paper
closes
describing
approach
laying
trees
based
similarities
flexible
models
prediction
session
organizer
jerome
friedman
padhraic
smyth
session
time
thursday
june
14th
10
45am
12
30pm
room
location
costa
mesa
room
session
chair
david
madigan
rutgers
predictive
data
mining
multiple
additive
regression
trees
author
jerome
friedman
stanford
university
abstract
predicting
future
outcomes
based
past
observational
data
common
application
data
mining
primary
goal
usually
predictive
accuracy
secondary
goals
speed
ease
use
interpretability
resulting
predictive
model
new
automated
procedures
predictive
data
mining
based
``boosting''
cart
regression
trees
described
goal
class
fast
``off
shelf''
procedures
regression
classification
competitive
accuracy
customized
approaches
fairly
automatic
use
little
tuning
highly
robust
especially
applied
less
clean
data
tools
presented
interpreting
visualizing
multiple
additive
regression
tree
mart
models
towards
understanding
boosting
author
bin
yu
uc
berkeley
peter
buhlmann
statistics
eth
zurich
abstract
boosting
effective
computational
procedure
improve
upon
initial
estimator
classifier
weak
learner
comes
machine
learning
impressive
successes
large
real
data
sets
talk
takes
recent
gradient
descent
point
view
boosting
understand
l2boost
regression
classification
especially
overfitting
resistance
particular
derive
l2boost's
interesting
exponential
bias
variance
trade
regression
approximate
0
1
generalization
error
smoothed
version
argue
importance
bias
reduction
classification
overfitting
resistance
0
1
loss
function
tapering
failure
attempts
decompose
0
1
loss
sum
bias
variance
moreover
put
forward
thesis
proper
l2boost
classification
worse
maybe
even
better
logitboost
adaboost
conclude
exponentially
diminishing
variance
centered
higher
moments
plus
overfitting
resistance
0
1
loss
responsible
overfitting
resistance
l2boosting
possibly
forms
boosting
classification
model
averaging
work
author
yoav
freund
labs
abstract
last
years
seen
increased
interest
model
averaging
techniques
bagging
boosting
statistician
interesting
aspect
techniques
resistance
fitting
will
describe
two
theoretical
explanations
phenomenon
one
explanation
applies
boosting
margin
based
methods
svm
applies
bagging
pseudo
bayesian
techniques
explanations
differ
common
statistical
analysis
based
``non
generative''
models
world
will
explain
generative
non
generative
models
differ
difference
important
model
based
clustering
session
organizer
adrian
raftery
session
time
thursday
june
14th
10
45am
12
30pm
room
location
viejo
room
session
chair
adrian
raftery
university
washington
model
based
approach
text
categorization
clustering
author
alejandro
murua
university
washington
insightful
corporation
jeremy
tantrum
university
washington
werner
stuetzle
university
washington
solveig
sieberts
university
washington
abstract
work
develop
complete
methodology
document
classification
clustering
documents
can
represented
high
dimensional
vectors
term
word
frequencies
study
depth
effect
dimensionality
reduction
via
principal
components
analysis
term
weighting
frequency
transformation
classification
clustering
tasks
conclude
increasing
feature
space
dimension
beyond
certain
critical
value
depending
complexity
size
data
improve
performance
worsens
applying
logarithm
square
root
transformation
term
frequencies
reduces
error
rates
used
findings
construct
model
based
document
clustering
mbdc
algorithm
explicitly
models
data
drawn
gaussian
mixture
used
construct
clusters
based
likelihood
data
classify
documents
according
bayes
rule
one
main
advantage
approach
ability
automatically
select
number
clusters
present
document
collection
via
bayes
factors
experiments
topic
detection
tracking
corpus
demonstrates
ability
mbdc
choose
sensible
number
clusters
well
meaningful
partitions
data
moreover
motivated
document
clustering
problem
introduce
novel
algorithm
cluster
high
dimensional
large
data
sets
algorithm
uses
model
based
clustering
context
splitting
data
manageable
subsets
way
fractionation
extension
method
model
based
re
fractionation
also
proposed
model
based
clustering
gene
expression
data
author
ka
yee
yeung
university
washington
walter
ruzzo
university
washington
abstract
many
biologists
excited
emerging
dna
microarray
technologies
since
make
possible
first
time
study
simultaneous
variations
activities
thousands
genes
great
need
develop
analytical
methodologies
extract
information
contained
rapidly
growing
data
sets
large
number
genes
complexity
biological
networks
clustering
useful
exploratory
technique
analysis
data
among
many
clustering
algorithms
proposed
model
based
clustering
stands
one
rigorous
probabilistic
underpinnings
namely
assumes
data
generated
mixture
underlying
probability
distributions
multivariate
normal
distributions
gaussian
mixture
model
shown
powerful
tool
many
applications
will
present
experiences
applying
model
based
clustering
algorithms
gene
expression
data
compare
many
clustering
approaches
tried
discuss
challenges
remaining
field
model
based
clustering
multidimensional
scaling
author
man
suk
oh
ewha
women's
university
adrian
raftery
university
washington
abstract
multidimensional
scaling
widely
used
handle
data
consists
similarity
dissimilarity
measures
pairs
objects
major
problems
multidimensional
scaling
object
configuration
choice
dimension
clustering
objects
propose
bayesian
approach
deal
problems
using
mixture
multivariate
normal
distributions
prior
distribution
object
coordinates
markov
chain
monte
carlo
algorithm
used
estimate
object
configuration
group
membership
simultaneously
simple
criterion
determine
dimension
object
configuration
number
groups
proposed
time
series
sequential
analysis
contributed
session
session
time
thursday
june
14th
10
45am
12
30pm
room
location
capistrano
room
session
chair
laszlo
engelman
uc
irvine
bayesian
inference
new
class
multi
scale
time
series
models
author
marco
ferreira
duke
university
mike
west
duke
university
david
higdon
duke
university
herbie
lee
duke
university
abstract
introduce
class
multi
scale
models
time
series
novel
framework
couples
``simple''
standard
markov
models
time
series
stochastic
process
different
levels
aggregation
links
via
``error''
models
induce
new
rich
class
structured
linear
models
reconciling
modelling
information
different
levels
resolution
jeffrey's
rule
conditioning
used
revise
implied
distributions
ensure
probability
distributions
different
levels
strictly
compatible
construction
several
interesting
characteristics
variety
autocorrelation
functions
resulting
just
parameters
ability
combine
information
different
scales
capacity
emulate
long
memory
processes
least
three
uses
multi
scale
framework
integrate
information
data
observed
different
scales
induce
particular
process
data
observed
finest
scale
prior
underlying
multi
scale
process
bayesian
estimation
based
mcmc
analysis
developed
issues
forecasting
discussed
two
interesting
applications
presented
first
application
illustrate
basic
concepts
multiscale
class
models
analysis
flow
river
second
application
use
multiscale
framework
model
daily
monthly
log
volatilities
exchange
rates
variable
memory
markovian
modeling
approach
unsupervised
sequence
segmentation
author
gill
bejerano
hebrew
university
yevgeny
seldin
hebrew
university
naftali
tishby
hebrew
university
abstract
outline
novel
unsupervised
sequence
segmentation
algorithm
motivated
information
theoretic
principles
algorithm
segments
sequences
alternating
variable
memory
markov
sources
based
competitive
learning
markov
models
implemented
prediction
suffix
trees
using
minimum
description
length
principle
applying
model
clustering
procedure
based
rate
distortion
theory
combined
deterministic
annealing
obtain
hierarchical
segmentation
sequences
alternating
markov
sources
algorithm
seems
self
regulated
automatically
avoids
segmentation
method
applied
successfully
unsupervised
segmentation
texts
languages
able
infer
number
languages
language
switching
points
applied
protein
sequence
families
demonstrate
method's
ability
identify
biologically
meaningful
sub
sequences
within
proteins
correspond
important
functional
sub
units
known
protein
domains
causal
investigation
time
series
using
graphical
modelling
author
marco
reale
university
canterbury
granville
tunnicliffe
wilson
lancaster
university
abstract
abstract
available
evaluating
sequential
tests
class
stochastic
processes
author
xiaoping
xiong
st
jude
children's
research
hospital
ming
tan
st
jude
children's
research
hospital
abstract
propose
computation
methods
sequential
tests
class
stochastic
processes
probability
density
test
statistic
can
factorized
product
known
likelihood
function
independent
stopping
rule
conditional
probability
independent
parameters
proposed
methods
improve
accuracy
efficiency
computation
enable
us
valuate
properties
special
interests
sequential
tests
probability
discordance
sequential
test
nonsequential
test
last
stage
sequential
test
give
examples
evaluating
sequential
tests
information
time
normal
outcomes
comparison
reversible
jump
mcmc
algorithms
dna
sequence
segmentation
using
hidden
markov
models
author
boys
newcastle
university
henderson
newcastle
university
abstract
dna
sequences
display
evidence
compositional
heterogeneity
form
patches
domains
similar
structure
paper
address
problem
identifying
regions
homogeneous
composition
genome
sequences
using
hidden
markov
models
office
naval
research
overview
session
organizer
wendy
martinez
session
time
thursday
june
14th
10
45am
12
30pm
room
location
laguna
room
session
chair
wendy
martinez
onr
functional
analysis
computer
network
data
author
jeff
solka
nswcdd
david
marchette
nswcdd
abstract
talk
focus
recent
efforts
application
functional
data
analysis
methods
mail
web
server
access
data
application
cluster
analysis
data
will
illustrated
methods
visualization
data
will
also
presented
time
permits
results
illustrate
identification
outliers
within
data
set
will
also
given
inferring
internal
losses
delays
communication
networks
edge
measurements
author
robert
nowak
rice
university
abstract
optimizing
communication
network
performance
detecting
attacks
intrusions
requires
knowledge
loss
rates
queueing
delays
different
points
network
however
impractical
directly
monitor
packet
losses
delays
every
router
measurements
edge
network
sources
receivers
relatively
easy
inexpensive
comparison
consequently
natural
consider
following
inverse
problem
edge
based
measurements
can
infer
loss
rates
delays
experienced
internal
points
network
paper
presents
unified
formulation
problems
internal
loss
delay
estimation
describes
expectation
maximization
algorithm
computing
maximum
likelihood
estimates
also
propose
new
method
jointly
visualizing
network
connectivity
network
performance
parameters
texture
modeling
using
self
similar
wavelets
pomms
author
jennifer
davidson
iowa
state
university
richard
barton
university
houston
abstract
stochastic
models
well
suited
modeling
natural
texture
images
wavelet
transform
also
useful
analyzing
texture
data
previous
work
supported
factor
limiting
widespread
use
models
high
number
parameters
necessary
describe
model
recent
work
shown
number
parameters
can
reduced
making
assumptions
data
self
similarity
wavelet
coefficients
assumptions
make
assumptions
wavelet
transform
wt
coefficients
work
include
stationarity
wt
coefficients
well
dependence
within
across
scales
wt
coefficients
done
fitting
empirical
version
stationary
partially
ordered
markov
model
pomm
transform
domain
within
across
scales
pomm
assume
dependence
wt
coefficient
values
local
neighborhood
scale
dependence
parent
node
adjacent
scale
coarser
resolution
empirical
pdf
pomm
found
based
quantized
version
wt
coefficients
quantization
states
performed
modeling
wt
coefficients
mixture
two
gaussians
estimating
parameters
gaussians
variance
assume
zero
mean
use
expectation
maximization
algorithm
find
variances
states
considered
unobserved
data
maximum
likelihood
used
determine
states
states
found
empirical
pomm
pdf
calculated
model
used
represent
data
synthesis
straightforward
real
time
since
pomms
algorithm
produces
sample
distribution
single
visit
pixel
locations
image
examples
shown
synthesis
technique
varying
values
neighborhood
size
pomm
applications
also
pursued
including
classification
texture
data
encoding
storage
transmission
adaptive
data
cube
experiment
hyperspectral
pattern
recognition
author
carey
priebe
johns
hopkins
university
abstract
present
experiment
hyperspectral
pattern
recognition
designed
illustrate
potential
``adaptive
data
cube''
integrated
sensing
processing
isp
visualization
data
mining
session
organizer
diane
cook
session
time
thursday
june
14th
2
00pm
3
45pm
room
location
santa
ana
room
session
chair
diane
cook
iowa
state
ggobi
meets
extensible
environment
interactive
dynamic
data
visualization
author
deborah
swayne
labs
research
duncan
temple
lang
lucent
bell
laboratories
andreas
buja
labs
research
diane
cook
iowa
state
university
abstract
ggobi
direct
descendant
xgobi
designed
can
embedded
software
controlled
using
api
application
programming
interface
design
developed
tested
partnership
ggobi
used
result
full
marriage
ggobi's
direct
manipulation
graphical
environment
r's
familiar
extensible
environment
statistical
data
analysis
ggobi
several
advances
xgobi
including
multiple
plotting
windows
flexible
color
management
xml
file
handling
portability
windows
graphical
post
analysis
association
rules
author
heike
hofmann
university
ausgburg
germany
abstract
association
rules
widely
used
tool
data
mining
major
problems
originate
mass
output
well
restriction
support
confidence
measures
quality
will
introduce
graphical
techniques
examining
association
rules
allow
us
assess
quality
single
rule
visually
also
provide
overview
structure
among
rules
laying
basis
interpretation
extraction
``real''
results
uncovering
complexity
data
sound
author
mark
hansen
bell
laboratories
ben
rubin
ear
studio
abstract
today
almost
every
aspect
lives
can
``rendered''
digitally
advances
data
collection
technologies
made
commonplace
continuous
high
resolution
measurements
physical
environment
weather
patterns
seismic
events
ecological
indicators
equally
open
observation
routine
movements
interactions
physical
surroundings
automobile
air
traffic
large
scale
land
use
computer
mediated
settings
activities
either
depend
crucially
consist
entirely
complex
digital
data
financial
transactions
accesses
global
information
systems
web
site
internet
usage
reflection
diversity
variety
systems
study
data
based
descriptions
daily
lives
tend
massive
size
dynamic
character
replete
rich
structures
advent
enormous
repositories
digital
information
presents
us
interesting
challenge
can
represent
interpret
complex
abstract
socially
important
data
new
collaboration
begun
give
voice
variety
internet
related
data
streams
initial
work
studied
traffic
across
web
site
www
lucent
com
recently
focus
capturing
large
scale
``chatter''
web
built
monitoring
agents
allow
us
collect
streams
thousands
public
forums
bulletin
boards
chat
rooms
incorporation
textual
components
audio
displays
presents
new
interesting
challenges
talk
will
try
put
work
context
presenting
brief
biased
review
use
data
music
compositions
well
previous
attempts
incorporate
sound
directly
process
data
analysis
work
part
lucent
program
sponsoring
collaborations
researchers
bell
laboratories
brooklyn
academy
music
beyond
correlation
session
organizer
thomas
belin
session
time
thursday
june
14th
2
00pm
3
45pm
room
location
costa
mesa
room
session
chair
thomas
belin
ucla
causal
inference
statistics
gentle
introduction
author
judea
pearl
ucla
abstract
talk
will
provide
conceptual
introduction
causal
inference
aimed
assisting
researchers
gain
access
recent
advances
area
talk
will
stress
paradigmatic
shifts
must
undertaken
moving
traditional
statistical
analysis
causal
analysis
multivariate
data
special
emphasis
placed
assumptions
underly
causal
inferences
languages
used
formulating
assumptions
conditional
nature
causal
claims
inferred
nonexperimental
studies
emphases
will
illustrated
brief
survey
recent
results
including
control
confounding
corrections
noncompliance
symbiosis
counterfactual
graphical
methods
analysis
background
information
can
viewed
path
http
www
cs
ucla
edu
judea
defining
role
``principal
effects
comparing
treatments
using
general
post
treatment
variables
author
constantine
frangakis
johns
hopkins
university
donald
rubin
harvard
university
abstract
study
general
demanding
problem
make
treatment
comparisons
adjust
post
treatment
variable
standard
methods
can
create
post
treatment
selection
bias
precisely
situations
scientific
reason
adjustment
propose
general
framework
comparing
treatments
adjusting
post
treatment
variables
yields
``principal
effects''
based
``principal
stratification''
principal
stratification
respect
post
treatment
variable
cross
classification
subjects
defined
joint
potential
values
variable
treatments
compared
principal
effects
defined
causal
effects
within
principal
stratum
key
property
principal
strata
affected
treatment
assignment
therefore
can
used
pre
treatment
covariate
defining
covariate
based
estimand
result
central
property
principal
effects
always
causal
effects
suffer
post
treatment
selection
bias
discuss
briefly
principal
causal
effects
link
two
recently
worked
applications
post
treatment
variables
treatment
noncompliance
ii
missingness
outcomes
dropout
following
treatment
noncompliance
discuss
open
problem
surrogate
endpoints
show
using
principal
effects
current
definitions
surrogacy
even
perfectly
true
generally
interpretation
causal
effects
attributable
surrogate
also
formulate
new
approach
based
principal
stratification
principal
effects
show
better
properties
standard
methods
statistical
models
text
session
organizer
john
lafferty
session
time
thursday
june
14th
2
00pm
3
45pm
room
location
viejo
room
session
chair
andrew
mccallum
active
learning
support
vector
machines
applications
text
classification
author
simon
tong
stanford
university
daphne
koller
stanford
university
abstract
support
vector
machines
met
significant
success
numerous
real
world
learning
tasks
however
like
machine
learning
algorithms
generally
applied
using
randomly
selected
training
set
classified
advance
many
settings
also
option
using
pool
based
active
learning
instead
using
randomly
selected
training
set
learner
access
pool
unlabeled
instances
can
guide
sampling
process
querying
labels
certain
pool
instances
based
upon
data
seen
far
introduce
new
algorithm
performing
active
learning
support
vector
machines
algorithm
choosing
instances
request
next
provide
theoretical
motivation
algorithm
using
notion
version
space
present
experimental
results
showing
employing
active
learning
method
can
significantly
reduce
need
labeled
training
instances
standard
inductive
transductive
settings
conditional
random
fields
text
processing
author
john
lafferty
carnegie
mellon
university
andrew
mccallum
whizbang
labs
research
fernando
pereira
whizbang
labs
research
abstract
present
framework
building
probabilistic
models
segment
label
sequence
data
using
random
fields
globally
conditioned
input
sequence
conditional
random
fields
offer
several
advantages
hidden
markov
models
stochastic
grammars
tasks
including
ability
relax
strong
independence
assumptions
made
models
incorporate
hierarchical
overlapping
features
model
conditional
random
fields
also
avoid
fundamental
limitation
maximum
entropy
markov
models
memms
discriminative
markov
models
based
directed
graphical
models
can
biased
towards
states
successor
states
present
iterative
parameter
estimation
algorithms
conditional
random
fields
compare
performance
hmms
memms
synthetic
natural
language
data
relevant
encoding
linguistic
data
via
information
bottleneck
method
author
naftali
tishby
hebrew
university
jerusalem
abstract
introduce
general
information
theoretic
method
extracting
relevant
information
one
set
variables
another
relevant
set
mutual
information
two
random
variables
answers
question
`what
minimal
number
yes
questions
bits
needed
asked
variable
order
learn
can
variable
'
value
tell
anything
however
content
questions
need
know
variable
provides
information
variable
call
answers
questions
relevant
components
information
propose
general
method
generating
questions
relevant
encoding
respect
introduce
top
bottom
algorithms
task
discuss
applications
data
clustering
text
categorization
classification
word
sense
disambiguation
bioinformatics
based
joint
work
noam
slonim
bill
bialek
fernando
pereira
bayesian
methods
contributed
session
session
time
thursday
june
14th
2
00pm
3
45pm
room
location
capistrano
room
session
chair
james
press
uc
riverside
split
merge
markov
chain
monte
carlo
procedure
dirichlet
process
mixture
model
author
sonia
jain
university
toronto
radford
neal
university
toronto
abstract
propose
split
merge
markov
chain
algorithm
address
problem
inefficient
sampling
conjugate
dirichlet
process
mixture
models
traditional
markov
chain
monte
carlo
methods
bayesian
mixture
models
gibbs
sampling
can
become
trapped
isolated
modes
corresponding
inappropriate
clustering
data
points
article
describes
metropolis
hastings
procedure
can
escape
local
modes
splitting
merging
mixture
components
metropolis
hastings
algorithm
employs
new
technique
appropriate
proposal
splitting
merging
components
obtained
using
restricted
gibbs
sampling
scan
demonstrate
empirically
method
outperforms
gibbs
sampler
situations
two
components
similar
structure
priors
bayesian
neural
networks
author
mark
robinson
university
british
columbia
abstract
recent
years
neural
networks
nn
become
popular
data
analytic
tool
statistics
computer
science
many
fields
nns
can
used
universal
approximators
tool
regressing
dependent
variable
possibly
complicated
function
explanatory
variables
nn
parameters
unfortunately
notoriously
hard
interpret
bayesian
view
propose
discuss
prior
distributions
network
parameters
encourage
parsimony
reduce
overfit
eliminating
redundancy
promoting
orthogonality
linearity
additivity
thus
consider
senses
parsimony
discussed
existing
literature
investigate
predictive
performance
networks
fit
various
priors
adaptive
metropolis
hastings
samplers
bayesian
analysis
large
linear
gaussian
systems
author
stephen
kh
yeung
university
newcastle
upon
tyne
darren
wilkinson
university
newcastle
upon
tyne
abstract
paper
concerns
implementation
efficient
bayesian
computation
large
linear
gaussian
models
containing
many
latent
variables
models
often
arise
context
dynamic
linear
modeling
underlying
stochastic
process
evolves
time
conventional
applications
hierarchical
linear
modeling
genetic
analysis
melanoma
onset
using
estimating
equations
bayesian
hierarchical
models
author
kim
anh
university
texas
anderson
cancer
center
abstract
complex
relative
contributions
genetic
shared
environmental
factors
increased
risk
melanoma
data
queensland
familial
melanoma
project
comprising
15
907
persons
1
912
families
2
118
melanoma
cases
analyzed
estimate
additive
genetic
common
unique
environmental
contributions
variation
age
onset
melanoma
two
complementary
approaches
analyzing
correlated
time
onset
family
data
considered
generalized
estimating
equations
gee
method
one
can
estimate
relationship
specific
dependence
simultaneously
regression
coefficients
describe
average
population
response
changing
covariates
subject
specific
bayesian
mixed
model
heterogeneity
regression
parameters
explicitly
modeled
different
components
variation
may
estimated
directly
proportional
hazards
weibull
models
utilized
produce
natural
frameworks
estimating
relative
risks
adjusting
simultaneous
effects
covariates
simple
markov
chain
monte
carlo
method
covariate
imputation
missing
data
used
actual
implementation
bayesian
model
based
gibbs
sampling
using
free
ware
package
bugs
addition
also
used
bayesian
model
investigate
relative
contribution
genetic
environmental
effects
expression
naevi
freckles
known
risk
factors
melanoma
gdagsim
sparse
matrix
algorithms
bayesian
computation
author
darren
wilkinson
university
newcastle
abstract
gdagsim
software
library
analysis
conditionally
specified
linear
models
particular
can
used
carry
conditional
sampling
gaussian
directed
acyclic
graph
gdag
models
hence
can
used
implementation
efficient
block
mcmc
samplers
models
army
research
office
overview
session
organizer
robert
launer
session
time
thursday
june
14th
2
00pm
3
45pm
room
location
laguna
room
session
chair
robert
launer
aro
approximations
dirichlet
processes
applications
author
jayaram
sethuraman
florida
state
university
abstract
dirichlet
process
can
used
priors
unknown
distribution
appearing
modeling
data
introduced
ferguson
direct
constructive
definition
dirichlet
process
given
us
simplifies
proofs
many
properties
talk
show
several
approximations
dirichlet
processes
will
useful
computational
bayesian
analysis
involving
dirichlet
processes
present
proofs
show
approximations
work
appropriate
senses
adequate
applications
finally
present
examples
applications
approximations
computational
bayesian
problems
banks
interacting
bayesian
filters
author
boris
rozovskii
university
southern
california
blazek
university
southern
california
petrov
university
southern
california
abstract
emerging
applications
statistics
network
intrusion
detection
estimation
network
congestion
target
tracking
volatility
estimation
financial
markets
etc
bring
forefront
new
complicated
problems
state
estimation
applications
require
models
abrupt
sporadic
changes
unusual
structures
observation
paper
proposes
use
randomly
modulated
jump
diffusion
systems
model
behavior
systems
present
bank
interacting
bayesian
filters
provide
optimal
mean
square
sense
estimate
state
process
recursive
algorithm
computing
estimates
will
discussed
data
reduction
quantization
author
edward
wegman
george
mason
university
center
computational
statistics
nkem
amin
martin
khumbah
george
mason
university
abstract
massive
data
sets
challenge
limits
computability
visualization
therefore
desirable
compress
data
sets
approximately
10
6
10
7
bytes
seems
desirable
can
done
sampling
thinning
quantization
binning
binning
essentially
maps
original
sample
space
new
discrete
sample
space
commonly
thought
data
sparse
lumpy
high
dimensions
binning
therefore
consists
identifying
clusters
determining
statistical
properties
within
clusters
proposal
identify
statistical
properties
within
bins
replace
original
data
statistically
equivalent
data
much
smaller
scale
paper
explores
ideas
principle
practice
minimum
description
length
author
bin
yu
university
california
berkeley
abstract
minimum
description
length
mdl
principle
statistical
modeling
states
occam's
razor
precise
language
coding
information
theory
one
hand
generalizes
maximum
likelihood
principle
motivates
useful
effective
model
selection
criteria
moreover
serves
objective
platform
comparing
model
selection
procedures
frequentist
bayesian
statistics
alike
talk
will
give
overview
mdl
describe
fast
low
delay
perceptually
lossless
coder
music
speech
based
cascaded
lms
mdl
weighting
gene
expression
contributed
session
session
time
thursday
june
14th
4
15pm
6
00pm
room
location
santa
ana
room
session
chair
dennis
kibler
uc
irvine
bayesian
approach
analysis
cdna
microarray
data
author
black
purdue
university
craig
purdue
university
tanurdzic
purdue
genetics
program
purdue
university
doerge
purdue
university
abstract
recent
explosion
interest
microarray
technology
resulted
becoming
preferred
methodology
conducting
gene
expression
experiments
although
ability
array
experiment
examine
expression
thousands
genes
simultaneously
gives
previously
unheard
level
insight
researchers
also
raises
plethora
statistical
questions
regarding
sheer
volume
data
produced
well
level
variability
inherent
relatively
new
technology
paper
present
statistical
methods
based
bayesian
linear
models
investigating
various
sources
variability
present
array
experiments
examples
involving
data
cdna
microarray
experiments
conducted
purdue
university
computational
genomics
facility
will
used
illustrate
methodology
statistical
analysis
radiolabeled
gene
expression
data
author
rafael
irizarry
johns
hopkins
university
giovanni
parmigiani
johns
hopkins
university
mingzhou
guo
johns
hopkins
university
tatiana
dracheva
national
cancer
institute
jin
jen
johns
hopkins
university
abstract
paper
considers
statistical
issues
analysis
designed
experiment
investigate
differential
gene
expression
colon
cancer
normal
colon
tissue
experiment
gene
expression
measured
using
radiolabeling
based
array
filters
specific
statistical
issues
arise
connection
radiolabeling
technology
absence
direct
control
replaced
empty
spots
filter
designed
experiments
opportunity
systematically
quantify
important
sources
random
variation
consider
three
aspects
detail
normalization
expression
intensities
shrinkage
estimates
intensity
ratios
cancer
normal
tissue
ranking
genes
strength
evidence
differentially
expressed
propose
robust
simple
implement
procedures
normalization
shrinkage
addresses
technology
specific
way
problem
estimating
ratios
presence
small
noisy
denominators
also
discuss
graphical
display
rank
genes
using
metric
based
quantiles
null
distribution
obtained
replicating
array
experiment
normal
tissue
replication
appropriate
statistical
analysis
required
accurate
interpretation
dna
microarray
experiments
author
pin
hung
university
california
irvine
wesley
hatfield
university
california
irvine
abstract
simple
sense
dna
microarray
defined
orderly
arrangement
hundreds
hundreds
thousands
unique
dna
molecules
probes
known
sequence
two
basic
sources
dna
probes
array
either
unique
probe
individually
synthesized
situ
rigid
surface
pre
synthesized
probes
oligonucleotides
pcr
products
attached
array
platform
usually
glass
nylon
membranes
prior
studies
reported
clear
whether
comparable
data
obtained
different
dna
microarray
formats
report
use
rigorous
statistical
methods
compare
data
obtained
situ
synthesized
pre
synthesized
dna
microarrays
affymetrix
genechip
scriptstyle
rm
tm
sigma
genosys
scriptstyle
rm
tm
nylon
filters
respectively
results
dramatically
demonstrate
necessity
replication
appropriate
statistical
analysis
interpretation
results
dna
microarray
experiments
identifying
statistically
significant
similarities
gene
expression
patterns
via
bayesian
infinite
mixture
models
author
mario
medvedovic
university
cincinnati
medical
center
siva
sivaganesan
university
cincinnati
abstract
recent
development
dna
microarray
dna
``chip''
technologies
parallel
monitoring
expression
levels
large
number
genes
holds
promise
taking
understanding
molecular
processes
underlying
normal
functions
living
organisms
well
underlying
mechanisms
human
diseases
new
level
ability
dna
microarray
technology
produce
expression
data
large
number
genes
parallel
fashion
resulted
new
approaches
identifying
individual
genes
well
whole
pathways
involved
performing
different
biologic
functions
one
commonly
used
approach
making
conclusions
microarray
data
identify
groups
genes
similar
expression
patterns
across
different
experimental
conditions
cluster
analysis
biologic
significance
results
analyses
demonstrated
numerous
studies
various
clustering
procedures
ranging
simple
agglomerative
hierarchical
methods
optimization
based
global
procedures
self
organizing
maps
used
clustering
gene
expression
profiles
identifying
patterns
expression
procedures
depend
either
visual
identification
patterns
hierarchical
clustering
color
coded
display
correct
specification
number
patterns
present
data
prior
analysis
means
self
organizing
maps
developed
statistical
procedure
based
bayesian
infinite
mixture
model
conclusions
probability
set
gene
expression
profiles
generated
pattern
based
posterior
probability
distribution
clusterings
given
data
contrast
finite
mixture
approach
model
require
specifying
number
mixture
components
resulting
clustering
result
averaging
possible
number
mixture
components
implemented
gibbs
sampling
based
algorithm
generating
sample
posterior
distribution
clusterings
used
identify
groups
genes
similar
expression
profiles
publicly
available
dataset
interdisciplinary
program
employing
computational
biochemical
genomic
methods
examine
effects
chromosome
structure
regulation
gene
expression
author
lorenzo
tolleri
university
california
irvine
craig
benham
mount
sinai
school
medicine
new
york
pierre
baldi
university
california
irvine
wesley
hatfield
university
california
irvine
abstract
computed
position
sidd
sites
present
coli
chromosome
mid
physiological
superhelical
density
sigma
0
05
calculations
based
statistical
mechanical
approach
governing
partition
function
quantities
interest
evaluated
high
degree
precision
computations
determine
positions
sidd
sites
chromosome
superhelical
densities
encountered
topoisomerase
mutant
strains
currently
underway
using
hidden
markov
models
predict
location
high
affinity
ihf
binding
sites
coli
chromosome
determine
vivio
occupancy
ihf
chromosomal
binding
sites
developing
procedure
using
vivio
ihf
dna
crosslinking
immunoprecipitation
dna
microarrays
call
clip
chip
experiments
cells
log
phase
treated
formaldehyde
covalently
link
chromosomal
dna
dna
binding
proteins
graphical
models
session
organizer
michael
jordan
session
time
thursday
june
14th
4
15pm
6
00pm
room
location
costa
mesa
room
session
chair
michael
jordan
introductory
comments
michael
jordan
uc
berkeley
variational
methods
bayesian
estimation
author
tommi
jaakkola
massachusetts
institute
technology
abstract
sampling
methods
typically
used
counter
often
prohibitive
cost
exact
bayesian
calculations
will
discuss
alternative
deterministic
approach
problem
based
variational
methods
variational
methods
either
generate
adjustable
simplifying
transforms
likelihood
function
operate
space
distributions
find
best
posterior
approximation
within
simpler
family
tractable
distributions
resulting
variational
transforms
fast
lead
closed
form
posterior
approximations
parameters
thereby
provide
approximate
posterior
predictive
model
methods
can
readily
extended
graphical
models
complete
incomplete
observations
help
additional
variational
transforms
will
present
fundamentals
approach
discuss
limitations
relation
approaches
advanced
mean
field
methods
probabilistic
models
author
manfred
opper
ncrg
aston
university
birmingham
ole
winther
technical
university
denmark
abstract
mean
field
mf
methods
provide
tractable
approximations
computation
high
dimensional
sums
integrals
probabilistic
models
neglecting
certain
dependencies
random
variables
closed
set
equations
expected
values
variables
derived
often
can
solved
time
grows
polynomially
number
variables
talk
deals
principled
general
approaches
correcting
deficiencies
simple
mf
methods
origin
statistical
physics
also
discuss
relation
methods
belief
propagation
techniques
probability
assessment
maximum
entropy
graphical
models
author
wim
wiegerinck
snn
university
nijmegen
tom
heskes
snn
university
nijmegen
abstract
maximum
entropy
maxent
method
standard
method
searches
distribution
maximizes
entropy
given
set
constraints
roughly
spoken
selects
distribution
satisfies
given
constraints
without
introducing
additional
information
talk
will
discuss
maxent
applied
graphical
models
optimal
model
satisfy
given
set
constraints
also
given
set
independency
graphical
models
optimal
model
satisfy
given
set
constraints
also
given
set
independency
statements
application
show
maxent
graphical
models
can
provide
practical
tool
assessment
model
parameters
graphical
models
build
collaboration
domain
experts
environmental
modeling
session
organizer
paul
black
tom
stockton
session
time
thursday
june
14th
4
15pm
6
00pm
room
location
viejo
room
session
chair
paul
black
functional
data
analysis
complex
computer
simulation
output
case
study
nuclear
waste
disposal
risk
assessment
author
david
draper
university
california
santa
cruz
bruno
mendes
university
california
santa
cruz
university
bath
uk
abstract
key
issue
consolidation
process
nuclear
fuel
cycle
safe
disposal
radioactive
waste
deep
geological
disposal
based
multibarrier
concept
present
actively
investigated
option
visualize
deep
underground
facility
within
radioactive
materials
spent
fuel
rods
reprocessed
waste
previously
encapsulated
placed
surrounded
man
made
barriers
safety
concept
ultimately
relies
safety
mechanical
chemical
physical
barriers
offered
geological
formation
physico
chemical
behavior
disposal
system
geological
time
scales
hundreds
thousands
years
far
known
certainty
1996
1999
partners
italy
spain
sweden
involved
project
european
commission
gesamac
aimed
part
capture
relevant
sources
uncertainty
predicting
happen
disposal
barriers
compromised
future
processes
geological
faulting
human
intrusion
climatic
change
one
major
goal
project
development
methodology
predict
radiologic
dose
people
biosphere
function
time
far
disposal
facility
components
multibarrier
system
underground
factors
likely
related
dose
purpose
developed
complex
computer
simulation
environment
called
gtmchem
``deterministically''
models
one
dimensional
migration
radionuclides
geosphere
biosphere
talk
will
describe
application
methods
functional
data
analysis
fda
explore
dependence
predicted
radiologic
dose
curves
function
time
inputs
computer
simulations
fda
includes
extensions
traditional
statistical
methods
principal
components
analysis
analysis
variance
anova
case
outcome
instead
single
real
number
curve
case
logarithm
radiologic
dose
function
logarithm
time
previous
work
field
limited
methods
anova
applied
maximum
curves
fda
thus
permits
much
complete
investigation
relationship
dose
time
relationship
depends
computer
simulation
inputs
integrated
assessment
drinking
water
regulations
author
mitchell
small
carnegie
mellon
university
patrick
gurian
carnegie
mellon
university
mark
schervish
carnegie
mellon
university
lockwood
carnegie
mellon
university
abstract
evaluation
costs
benefits
drinking
water
regulations
united
states
involves
multiple
disciplines
diverse
datasets
widely
differing
expectations
beliefs
among
interested
parties
debate
epa's
recent
proposal
subsequent
withdrawal
new
maximum
contaminant
level
mcl
arsenic
highlights
high
degree
uncertainty
importance
assessment
present
results
integrated
bayesian
statistical
assessment
framework
evaluating
costs
benefits
alternative
mcl's
drinking
water
united
states
framework
includes
statistical
models
national
distribution
contaminant
concentrations
raw
source
waters
approx
56
000
community
water
suppliers
us
models
current
treatment
systems
place
pollutant
removal
efficiencies
resulting
finished
water
concentrations
population
exposures
health
risks
model
also
predicts
new
treatments
management
options
will
adopted
response
new
mcl's
costs
exposure
risk
reduction
benefits
full
uncertainty
analysis
conducted
informed
available
data
sets
raw
water
concentrations
treatments
place
finished
water
concentrations
health
risk
data
model
identified
estimated
using
bayesian
markov
chain
monte
carlo
methods
framework
illustrated
single
contaminant
arsenic
methods
discussed
evaluating
multiple
mcl's
suite
contaminants
bayesian
sensitivity
analysis
uncertainty
analysis
author
jeremy
oakley
university
sheffield
anthony
o'hagan
university
sheffield
abstract
problem
assessing
uncertainties
complex
computer
simulation
codes
increasing
importance
many
fields
particularly
true
environmental
applications
large
highly
complex
codes
used
relevant
science
always
clear
implications
errors
can
enormous
although
codes
generally
deterministic
statistical
methods
provide
powerful
tools
quantifying
uncertainties
talk
concerns
range
bayesian
techniques
offer
integrated
framework
addressing
problems
use
models
one
major
source
uncertainty
user
model
often
specifying
values
relevant
inputs
running
model
may
typically
mean
assigning
values
physical
parameters
either
unobservable
least
measured
scale
assumed
model
uncertainty
inputs
induces
uncertainty
model
outputs
objectives
sensitivity
uncertainty
analysis
first
quantify
uncertainty
explore
role
uncertain
input
overall
output
uncertainty
standard
techniques
analysis
involve
monte
carlo
methods
make
random
draws
distributions
inputs
run
code
sampled
input
configuration
thereby
obtain
sample
outputs
code
may
take
minutes
hours
even
days
single
run
thousands
runs
demanded
monte
carlo
methods
become
impracticable
bayesian
tools
described
talk
can
applied
effectively
far
fewer
code
runs
sensitivity
analysis
buried
radioactive
waste
risk
model
author
tom
stockton
neptune
co
abstract
complex
ecosystem
models
useful
investigating
dynamics
systems
multiple
variables
interacting
non
linear
manner
quantitatively
assessing
importance
input
variables
becomes
difficult
dimensionality
model
increases
sensitivity
analysis
deals
assigning
influence
measures
input
variables
given
model
local
sensitivity
analysis
deals
modification
input
parameters
one
time
although
local
sensitivity
analysis
useful
applications
region
possible
realizations
model
interest
left
largely
unexplored
global
sensitivity
analysis
attempts
explore
possible
realizations
model
completely
space
possible
realizations
model
can
explored
use
search
curves
evaluation
multi
dimensional
integrals
using
monte
carlo
methods
sensitivity
measures
computed
methods
estimate
portion
total
variation
response
can
attributed
input
variable
interest
anova
like
decomposition
main
interaction
effects
methods
sensitivity
analysis
multivariate
adaptive
regression
splines
mars
fourier
amplitude
sensitivity
test
fast
provide
tools
can
used
global
sensitivity
analysis
enhance
interpretability
model
output
providing
quantitative
measure
importance
input
parameters
application
global
sensitivity
analysis
tools
will
demonstrated
modeling
example
involving
performance
buried
radioactive
waste
computational
finance
session
organizer
halbert
white
session
time
thursday
june
14th
4
15pm
6
00pm
room
location
capistrano
room
session
chair
halbert
white
learning
trade
via
direct
reinforcement
author
john
moody
oregon
graduate
institute
abstract
present
new
methods
optimizing
portfolios
asset
allocations
trading
systems
based
direct
reinforcement
approach
investment
decision
making
viewed
stochastic
control
problem
strategies
discovered
directly
need
build
forecasting
models
eliminated
better
trading
performance
obtained
direct
reinforcement
approach
differs
dynamic
programming
reinforcement
algorithms
td
learning
learning
attempt
estimate
value
function
control
problem
present
adaptive
algorithm
called
recurrent
reinforcement
learning
rrl
enables
simpler
problem
representation
avoids
bellman's
curse
dimensionality
offers
compelling
advantages
efficiency
demonstrate
direct
reinforcement
can
used
directly
optimize
risk
adjusted
investment
returns
including
differential
sharpe
ratio
accounting
effects
transaction
costs
extensive
simulation
work
find
approach
based
rrl
produces
better
trading
strategies
systems
utilizing
learning
value
function
method
trading
based
forecasts
real
world
applications
include
monthly
asset
allocation
system
intra
daily
currency
trader
statistical
inference
bootstrap
neural
network
modeling
application
foreign
exchange
rates
author
jeff
racine
university
south
florida
halbert
white
university
california
san
diego
abstract
paper
propose
tests
individual
joint
irrelevance
network
inputs
tests
can
used
determine
whether
input
group
inputs
``belong''
particular
model
thus
permitting
valid
statistical
inference
based
estimated
feedforward
neural
network
models
approaches
employ
well
known
statistical
resampling
techniques
conduct
small
monte
carlo
experiment
showing
tests
reasonable
level
power
behavior
apply
methods
examine
whether
predictable
regularities
foreign
exchange
rates
find
exchange
rates
appear
contain
information
exploitable
enhanced
point
prediction
nature
predictive
relations
evolves
time
national
security
agency
overview
session
organizer
william
szewczyk
session
time
thursday
june
14th
4
15pm
6
00pm
room
location
laguna
room
session
chair
william
szewczyk
nsa
dynamic
visualization
changing
prior
posterior
bayesian
analysis
author
hani
doss
ohio
state
university
narasimhan
stanford
university
abstract
years
statistical
problems
given
nsasag
often
considered
bayesian
approach
invariably
raises
question
``how
choose
prior
''
ideally
one
want
know
posterior
distributions
wide
variety
priors
posterior
change
much
one
changes
prior
one
gets
feeling
reassurance
different
investigator
slightly
different
prior
may
even
bother
recompute
posterior
prior
hand
posterior
changes
significantly
one
changes
prior
important
record
fact
example
time
spent
prior
elicitation
therefore
almost
problem
one
carries
serious
data
analysis
one
wants
calculate
posterior
distribution
large
number
prior
distributions
especially
exploratory
stages
analysis
many
problems
posterior
estimated
markov
chain
monte
carlo
may
require
non
negligible
computer
time
unfortunately
precludes
consideration
large
number
priors
interactive
analysis
deal
problem
present
computing
environment
within
one
can
interactively
change
prior
immediately
see
corresponding
changes
posterior
environment
based
object
oriented
programming
language
lisp
stat
importance
sampling
procedure
enables
one
use
output
one
small
number
markov
chains
obtain
estimates
posterior
large
class
priors
environment
general
handles
wide
range
standard
models
including
example
glm's
hierarchical
models
nonparametric
clustering
author
david
scott
rice
university
abstract
use
density
estimation
find
clusters
data
supplementing
ad
hoc
hierarchical
methodology
examples
include
finding
high
density
regions
finding
modes
kernel
density
estimator
mode
tree
alternatively
mixture
model
may
fit
mixture
components
associated
individual
clusters
fitting
high
dimensional
mixture
model
many
components
difficult
estimate
practice
survey
mode
level
set
methods
finding
clusters
describe
new
algorithm
estimates
subset
mixture
model
particular
demonstrate
fit
one
component
time
fits
may
organized
reveal
complete
clustering
model
massive
data
sets
author
jon
kettenring
telcordia
technologies
abstract
tba
analyzing
web
data
session
organizer
byron
dom
session
time
friday
june
15th
10
45am
12
30pm
room
location
santa
ana
room
session
chair
byron
dom
statistical
learning
problems
associated
world
wide
web
author
byron
dom
ibm
almaden
research
center
abstract
problems
statistical
machine
learning
occur
many
domains
currently
much
activity
field
two
major
problems
addressed
supervised
unsupervised
learning
also
appeared
context
world
wide
web
talk
surveys
work
web
specific
instances
two
learning
problems
others
hope
making
statistics
community
aware
significant
work
performed
date
problems
discussed
include
supervised
classification
web
pages
clustering
web
pages
called
``resource
discovery''
finding
authoritative
information
specific
subjects
finite
state
approaches
information
extraction
author
andrew
mccallum
whizbang
labs
fernando
pereira
whizbang
labs
john
lafferty
carnegie
mellon
university
dayne
freitag
carnegie
mellon
university
abstract
finite
state
machines
dominant
model
information
extraction
research
industry
talk
will
give
overview
several
finite
state
approaches
information
extraction
culminating
presentation
conditional
random
fields
crfs
new
model
probabilistic
modeling
sequence
data
crfs
offer
several
advantages
hidden
markov
models
including
ability
relax
strong
independence
assumptions
made
models
conditional
random
fields
also
avoid
fundamental
limitation
maximum
entropy
markov
models
memms
discriminative
markov
models
based
directed
graphical
models
can
biased
towards
states
successor
states
will
present
parameter
estimation
algorithms
several
models
well
experiments
real
synthetic
data
graph
structure
web
author
andrew
tomkins
ibm
almaden
research
center
abstract
talk
discusses
recent
results
concerning
macroscopic
structure
web
show
web
``bow
tie''
decomposable
four
equal
sized
regions
based
connectivity
properties
also
examine
number
statistical
properties
web
graph
showing
results
concerning
distribution
links
distribution
sizes
strongly
weakly
connected
components
graph
diameter
various
definitions
resulting
picture
much
less
strongly
connected
previously
believed
bayesian
bioinformatics
session
organizer
mike
west
session
time
friday
june
15th
10
45am
12
30pm
room
location
costa
mesa
room
session
chair
mike
west
duke
genome
wide
binding
motif
discovery
via
microarray
prospect
sampler
author
jun
liu
harvard
university
xiaole
liu
stanford
unviersity
abstract
recent
biological
experiments
showed
combining
modified
chromatin
immunoprecipitation
procedure
microarray
analysis
one
can
strong
information
genome
wide
binding
locations
specific
tanscription
factor
protein
combined
computational
strategy
gibbs
motif
sampler
one
can
even
pinpoint
exact
binding
motif
cases
describe
improved
algorithm
specifically
designed
find
significant
protein
binding
motifs
chip
microarray
data
available
hierarchical
models
gene
expression
data
analysis
author
michael
newton
university
wisconsin
madison
christina
kendziorski
university
wisconsin
madison
abstract
hierarchical
statistical
models
provide
flexible
approach
analysis
gene
expression
data
enable
robust
efficient
inference
concerning
patterns
differential
expression
among
cell
types
one
rationale
models
thousands
gene
specific
questions
meaningful
treat
gene
specific
parameters
arising
array
specific
cell
type
specific
distributions
rather
treating
fixed
effects
two
parametric
formulations
proven
effective
gamma
gamma
multinomial
ggm
lognormal
normal
multinomial
lnnm
characterize
fluctuations
gene
specific
expected
expression
fluctuations
measured
expression
given
underlying
means
nonparametric
version
model
provides
insight
will
discuss
hierarchical
statistical
modeling
context
several
experiments
involving
oligonucleotide
chip
sets
will
try
demonstrate
utility
reporting
probability
various
forms
differential
expression
will
discuss
model
fitting
model
checking
role
interesting
arithmetic
geometric
mean
ratio
stochastic
models
sequences
non
local
dependency
structure
author
scott
schmidler
duke
university
abstract
describe
class
probability
models
capturing
non
local
dependencies
sequential
data
motivated
applications
biopolymer
protein
nucleic
acid
sequence
analysis
models
generalize
previous
work
segment
based
stochastic
models
sequence
analysis
provide
algorithms
bayesian
inference
models
via
dynamic
programming
markov
chain
monte
carlo
simulation
demonstrate
approach
application
protein
structure
prediction
john
tukey
interface
session
organizer
karen
kafadar
session
time
friday
june
15th
10
45am
12
30pm
room
location
viejo
room
session
chair
karen
kafadar
university
denver
anyone
know
correlation
coefficient
useful
study
times
extreme
river
flows
author
david
brillinger
university
california
berkeley
abstract
john
tukey
spoke
wrote
concerning
uses
limitations
correlation
regression
coefficients
particular
think
highly
former
except
limited
circumstances
recognized
substantial
difficulties
interpretation
going
along
latter
two
concepts
random
process
analogs
paper
considers
case
stationary
point
processes
use
setup
reasonably
well
understood
physically
situation
passage
extreme
water
flows
series
locks
along
mississippi
river
focus
investigation
validity
partial
coherency
analysis
analog
partial
correlation
analysis
maximum
likelihood
analysis
also
presented
interaction
statistics
computing
memory
john
tukey
author
luisa
fernholz
temple
university
abstract
talk
partially
based
unpublished
joint
work
john
tukey
will
discuss
tukey's
ideas
comments
data
analysis
statistics
mathematical
statistics
computing
etc
context
will
present
examples
interaction
statistical
theory
statistical
computing
can
used
generate
new
methodologies
offer
powerful
tools
analyzing
data
particular
will
present
data
dependent
method
outlier
detection
based
multihalver
leave
half
jackknife
approach
based
sequences
plackett
burman
designs
hadamard
matrices
used
generate
different
``halvings''
data
examples
will
given
show
effectiveness
multihalver
detect
outliers
multihalver
examples
based
joint
work
john
tukey
one
last
works
statistics
legacy
john
tukey
author
robert
launer
army
research
office
george
washington
university
abstract
talk
begins
another
look
john
tukey's
1949
paper
``one
degree
freedom
non
additivity''
spring
board
look
career
work
activity
consultant
united
states
government
highlighted
personal
reminiscence
ecological
earth
science
applications
contributed
session
session
time
friday
june
15th
10
45am
12
30pm
room
location
capistrano
room
session
chair
christine
mclaren
uc
irvine
spatio
temporal
prediction
incomplete
precipitation
records
author
craig
johns
university
colorado
denver
douglas
nychka
national
center
atmospheric
research
abstract
ecological
models
depend
preciptation
fields
inputs
however
monthly
precipitation
data
large
number
stations
united
states
large
number
years
contain
many
missing
observations
predict
infill
describe
model
completely
rely
stationary
models
completely
upon
observed
correlations
modifications
model
made
order
make
fitting
computationally
feasible
large
data
sets
bayesian
frequentist
inference
ecological
inference
case
author
ori
rosen
university
pittsburgh
wenxin
jiang
northwestern
university
gary
king
harvard
university
martin
tanner
northwestern
university
abstract
paper
propose
bayesian
frequentist
approaches
ecological
inference
based
times
contingency
tables
including
covariate
proposed
bayesian
model
extends
binomial
beta
hierarchical
model
developed
king
rosen
tanner
1999
2
times
2
case
times
case
2
times
2
case
inferential
procedure
employs
markov
chain
monte
carlo
mcmc
methods
resulting
mcmc
analysis
rich
computationally
intensive
frequentist
approach
based
first
moments
rather
entire
likelihood
provides
quick
inference
via
nonlinear
least
squares
retaining
good
frequentist
properties
two
approaches
illustrated
simulated
data
well
real
data
voting
patterns
weimar
germany
final
section
paper
provide
overview
range
alternative
inferential
approaches
trade
computational
intensity
statistical
efficiency
using
chemical
mass
balance
receptor
model
estimate
pollution
source
contributions
correlated
air
quality
observations
author
william
christensen
southern
methodist
university
abstract
environmental
sciences
receptor
models
used
evaluate
contribution
various
pollution
sources
air
composition
location
using
pollution
source
profiles
profile
uncertainties
measurement
error
variances
chemical
mass
balance
cmb
model
can
fit
order
partition
ambient
pollutants
measured
receptor
collection
source
contributions
discuss
use
cmb
model
analysis
multivariate
time
series
air
quality
measurements
consider
estimation
inference
procedures
account
multiple
sources
correlation
data
using
computer
simulation
approaches
compared
various
scenarios
standard
model
assumptions
violated
application
ensemble
combination
classifiers
land
cover
mapping
via
satellite
imagery
author
brian
steele
university
montana
david
patterson
university
montana
abstract
talk
concerns
application
statistical
classification
rules
constructing
land
cover
maps
landsat
thematic
mapper
tm
satellite
imagery
maps
widely
used
large
scale
management
usda
forest
service
agencies
tasks
prioritizing
fire
fighting
efforts
classification
problem
begins
landsat
tm
scene
approximately
170
times
170
km
partitioned
set
approximately
800
000
polygons
training
set
2000
5000
observations
collected
ground
visitation
classifier
constructed
training
set
assign
land
cover
type
15
20
types
unsampled
polygons
using
satellite
imagery
good
map
accuracy
difficult
achieve
mountainous
forested
landscapes
land
cover
type
transitions
sometimes
indistinct
satellite
measurement
error
talk
addresses
application
ensemble
methods
boosting
bagging
combination
methods
improving
classifier
accuracy
two
classifiers
particular
interest
discuss
nearest
neighbor
nn
classifier
estimates
group
membership
probabilities
using
exact
analytic
bootstrap
expectations
nn
probability
estimator
words
classifier
amounts
classifier
obtained
averaging
possible
bagging
versions
nn
classifier
second
classifier
simple
spatial
classifier
uses
distance
polygons
training
observations
classify
polygons
accuracy
classifier
quite
poor
combinations
spatial
nn
tree
classifiers
substantially
better
constituent
classifiers
method
combining
classifiers
particularly
important
fact
data
simple
method
works
nearly
well
mining
knowledge
ostracode
assemblages
tecolutla
river
delta
author
dale
magoun
university
louisiana
monroe
mervin
kontrovitz
university
louisian
monroe
daniel
stanley
smithsonian
institution
abstract
sediment
surface
samples
containing
ostracodes
twenty
one
locations
representing
different
aquatic
habitats
obtained
recent
study
pertaining
depositional
variability
tecolutla
river
delta
mexico
chen
stanley
wright
2000
aquatic
habitats
estuary
offshore
mangrove
tidal
ponds
marsh
river
surveyed
surface
samples
analyzed
organic
matter
grain
size
distributions
ostracodes
identified
lowest
taxonomic
level
paper
shows
results
several
approaches
analysis
interpretation
multivariate
study
methods
paper
provide
different
approaches
analysis
interpretation
interdependent
structures
exists
aquatic
habitats
found
riverine
environment
paper
discusses
findings
traditional
taxonomic
viewpoint
latest
techniques
using
correspondence
analysis
international
association
statistical
computing
overview
session
organizer
edward
wegman
session
time
friday
june
15th
10
45am
12
30pm
room
location
laguna
room
session
chair
ed
wegman
gmu
developing
data
mining
systems
author
arno
siebes
utrecht
university
abstract
data
mining
search
patterns
large
databases
last
couple
years
involved
development
two
data
mining
systems
start
development
third
system
geared
towards
bioinformatics
talk
will
tell
experiences
influence
design
new
system
will
especially
address
efficiency
issues
related
interrogation
large
databases
example
data
structures
database
speed
query
processing
type
query
operators
necessary
graphical
statistical
pruning
association
rules
author
adalbert
wilhelm
university
augsburg
abstract
association
rules
amongst
important
patterns
can
discovered
using
data
mining
discovery
supported
data
mining
tools
analysis
interpretation
discovered
rules
difficult
almost
impossible
given
huge
number
generated
rules
paper
propose
graphical
aids
statistical
tests
overcome
main
drawbacks
mining
association
rules
arbitrary
thresholds
support
confidence
huge
number
association
rules
meaningless
associations
due
presence
frequent
itemsets
empirical
evaluation
implications
strength
show
double
decker
plots
can
used
visualize
association
rules
plots
visualize
contingency
table
yields
association
rule
well
potential
rules
table
whether
meet
thresholds
gives
deeper
understanding
nature
correlation
left
hand
side
rule
right
hand
side
cancelled
analyzing
high
dimensional
online
monitoring
data
author
ursula
gather
university
dortmund
abstract
technical
process
control
also
modern
intensive
care
confronted
data
structures
massive
well
high
dimensional
dynamic
coming
along
complex
dependencies
components
time
report
problems
challenges
statistical
data
analysis
situation
especially
ability
methods
work
online
first
results
concerning
dimension
reduction
online
pattern
detection
will
also
presented
large
web
session
organizer
stephen
fienberg
lee
giles
session
time
friday
june
15th
2
00pm
3
45pm
room
location
santa
ana
room
session
chair
andrew
tomkins
ibm
almaden
searching
web
current
limitations
new
techniques
future
directions
author
lee
giles
pennsylvania
state
university
abstract
world
wide
web
continues
revolutionize
communication
information
systems
measurements
size
content
provide
us
insights
web's
future
suggestions
new
web
tools
sampling
web
found
web
though
large
large
industrial
databases
sampling
search
engines
found
search
engines
index
fraction
web
index
sites
equally
may
index
new
pages
months
talk
discusses
impact
results
future
web
search
makes
suggestions
future
web
tools
research
illustrate
new
techniques
information
access
web
describing
two
approaches
metasearch
illustrated
inquirus
content
based
metasearch
engine
researchindex
niche
search
engine
largest
free
full
text
index
computer
science
literature
joint
work
steve
lawrence
kurt
bollacker
eric
glover
big
world
wide
web
author
adrian
dobra
carnegie
mellon
university
stephen
fienberg
carnegie
mellon
university
abstract
considerable
efforts
dedicated
development
sound
procedures
assessing
size
world
wide
web
problem
compounded
fact
sampling
directly
web
possible
several
groups
researchers
found
sampling
schemes
consist
running
number
queries
several
major
search
engines
present
new
approach
analyze
datasets
collected
query
based
sampling
founded
hierarchical
bayes
formulation
rasch
model
multiple
capture
recapture
methodology
illustrate
approach
using
data
gathered
giles
lawrence
1997
support
vector
machines
session
organizer
tommi
jaakkola
session
time
friday
june
15th
2
00pm
3
45pm
room
location
costa
mesa
room
session
chair
tommi
jaakola
mit
tutorial
support
vector
machines
author
bernhard
sch
olkopf
biowulf
technologies
abstract
talk
will
present
general
tutorial
support
vector
machines
svms
talk
slides
postscript
kernel
methods
unsupervised
learning
author
bernhard
sch
olkopf
biowulf
technologies
abstract
last
years
ideas
svms
generalized
unsupervised
learning
problems
multi
dimensional
quantile
estimation
vector
quantization
algorithms
problems
will
presented
talk
chernoff
faces
interface
session
organizer
william
dumouchel
session
time
friday
june
15th
2
00pm
3
45pm
room
location
viejo
room
session
chair
william
dumouchel
graphical
representation
discipline
author
herman
chernoff
harvard
university
abstract
advent
computer
facilitated
development
use
graphical
representation
time
made
use
important
response
needs
many
novel
techniques
originated
time
replace
naive
dogmas
constitutes
good
representation
serious
study
fundamental
principles
clustering
genetics
complex
disease
author
richard
olshen
stanford
university
abstract
work
collaboration
alfred
lin
jing
huang
neil
risch
david
cox
koustubh
ranade
yii
der
ida
chen
dee
pei
chii
min
hwu
david
curb
beatriz
rodriguez
victor
dzau
many
others
part
talk
will
description
two
ongoing
projects
involved
many
individuals
search
genes
predispose
seems
polygenic
disease
first
project
sapphire
stanford
asian
pacific
program
hypertension
insulin
resistance
network
nhlbi's
family
blood
pressure
program
supported
donald
reynolds
foundation
concerned
cardiovascular
diseases
particular
genes
expressed
vessel
walls
talk
will
defining
phenotypes
general
called
``intermediate
phenotypes''
particular
sapphire
applied
means
clustering
variables
none
marginally
gaussian
quantify
levels
plasma
lipids
metabolism
insulin
glucose
technologies
choosing
``how
many
clusters''
firmly
choose
two
597
women
somewhat
less
certainly
two
535
men
however
others
insist
upon
three
clusters
differences
seem
interest
statistics
biology
case
one
cluster
clearly
``not
insulin
resistant''
cluster
cluster
membership
significantly
associated
hypertension
women
much
less
men
devised
various
permutation
tests
making
inferences
data
tests
respect
family
structures
mutlivariate
statistical
process
control
signature
analysis
using
eigenfactor
detection
methods
author
kuang
chen
massachusetts
institute
technology
duane
boning
massachusetts
institute
technology
roy
welsch
massachusetts
institute
technology
abstract
many
businesses
now
use
univariate
statistical
process
control
uspc
manufacturing
service
operations
automated
data
collection
low
cost
computation
product
design
facilitate
measurement
demands
higher
quality
lower
cost
increased
reliability
accelerated
use
uspc
however
many
situations
widespread
use
uspc
caused
backlash
processes
frequently
adjusted
shut
nothing
really
wrong
probability
false
positives
type
error
calculated
based
uspc
takes
little
account
multiple
tests
performed
correlation
structure
may
exist
data
attempts
deal
issues
focus
bonferroni
adjustments
hotelling's
squared
statistics
generalized
variance
problem
high
dimensionality
commonly
addressed
using
form
dimension
reduction
principal
component
analysis
pca
often
methods
indicate
sort
change
taken
place
provide
little
information
real
nature
change
paper
develop
multivariate
detection
method
called
eigenfactor
analysis
combines
information
contained
matrix
eigenvectors
eigenvalues
capable
detecting
new
events
subtle
changes
covariance
structure
process
information
regarding
covariance
structure
process
can
crucial
feedback
tuning
control
purposes
unlike
univariate
cases
distributions
null
alternative
hypotheses
aligned
axis
orientations
two
multivariate
distributions
can
much
complicated
multivariate
pca
squared
techniques
project
test
samples
distribution
model
based
training
data
hence
techniques
assume
alignment
distributions
consequently
detection
strategies
capable
differentiating
directional
drifts
become
desirable
paper
concludes
example
semiconductor
manufacturing
goal
detect
end
point
plasma
etch
using
optical
emission
spectra
results
using
new
procedures
compared
existing
univariate
multivariate
process
control
techniques
clusters
outliers
density
models
contributed
session
session
time
friday
june
15th
2
00pm
3
45pm
room
location
capistrano
room
session
chair
bob
newcomb
uc
irvine
data
sharpening
higher
order
density
estimation
author
michael
minnotte
utah
state
university
peter
hall
australian
national
university
abstract
data
sharpening
method
applying
carefully
chosen
transformations
data
obtain
superior
properties
simple
methods
analysis
case
kernel
density
estimation
show
transformations
based
pilot
estimates
density
derivatives
can
lead
arbitrarily
high
orders
bias
reduction
density
estimation
second
order
positive
kernels
although
transformation
bandwidth
dependent
requires
neither
subsidiary
smoothing
parameters
back
transformation
unlike
estimates
generated
using
traditional
higher
order
kernels
constrained
nonnegative
numerical
studies
demonstrate
also
improved
mean
square
error
properties
fewer
arbitrary
wiggles
robust
detection
multivariate
outliers
high
dimensions
high
levels
contamination
author
mark
werner
university
colorado
karen
kafadar
university
colorado
abstract
detection
multivariate
outliers
using
classical
statistical
measures
mean
standard
deviation
made
difficult
masking
effect
influence
multiple
outliers
measures
limits
identification
robust
methods
developed
use
classical
measures
tend
computationally
prohibitive
feasible
use
large
data
sets
building
ideas
pe
prieto
2001
rocke
woodruff
2001
investigate
success
variety
methods
detecting
outliers
particular
interest
mixture
algorithms
using
combination
classical
robust
methods
utilize
advantages
method
thereby
achieve
greater
success
utilizing
methods
separately
thus
investigate
performance
algorithms
various
types
outliers
outlier
clusters
find
different
algorithms
perform
better
different
types
outliers
therefore
recommend
combination
methods
achieve
highest
possible
outlier
identification
rate
confronted
data
set
containing
unknown
outlier
types
complexity
mcd
problem
author
paul
fischer
university
dortmund
lehrstuhl
informatik
2
thorsten
bernholt
university
dortmund
lehrstuhl
informatik
2
abstract
modern
statistics
robust
estimation
parameters
central
problem
minimum
covariance
determinant
mcd
see
rous84
probably
important
robust
estimator
multivariate
location
scatter
algorithmic
complexity
however
unknown
generally
thought
exponential
even
dimensionality
data
fixed
number
heuristics
solving
problem
developed
fast
mcd
rous99
feasible
solution
haol99
present
polynomial
time
algorithm
mcd
fixed
dimension
data
contrast
show
mcd
problem
np
hard
dimension
varies
hence
one
expect
find
efficient
algorithms
case
finding
committee
solutions
clustering
models
function
space
author
thomas
ragg
university
karlsruhe
abstract
forming
committee
approach
integrating
several
opinions
functions
instead
favoring
single
one
selecting
weighting
committee
members
done
several
ways
different
algorithms
possible
solutions
problem
still
topic
current
research
starting
point
decomposition
committee
error
bias
variance
like
term
two
requests
can
derived
equation
models
one
hand
regularized
properly
reduce
average
error
hand
independent
possible
mathematical
sense
decrease
committee
error
first
request
regularization
can
handled
bayesian
learning
framework
second
request
want
suggest
new
selection
method
committee
members
based
pairwise
stochastic
dependence
output
functions
maximizes
overall
independence
given
pairwise
similarity
values
models
can
separated
classes
hierarchical
clustering
algorithm
error
decomposition
committees
derive
criterion
allows
find
optimal
number
classes
optimal
stop
criteria
clustering
algorithm
benefits
approach
demonstrated
committees
neural
networks
noisy
benchmark
problem
well
problems
uci
repository
detection
novel
samples
mass
spectral
data
using
cluster
analysis
author
vladimir
svetnik
merck
co
inc
andy
liaw
merck
co
inc
abstract
present
application
cluster
analysis
detection
novel
unusual
outlying
samples
mass
spectral
data
samples
unlike
majority
others
may
represent
novel
chemical
structures
interest
scientists
details
specific
application
presented
1
typical
data
set
consists
nearly
thousand
mass
spectral
measurements
intensities
measured
equal
hundreds
mass
charge
ratios
since
sample
can
represented
point
dimensional
space
search
unusual
samples
can
considered
search
``abnormal''
outlier
points
space
well
known
outlier
identification
clustering
high
dimensions
extremely
difficult
will
argued
nature
mass
spectral
data
lend
dimension
reduction
significant
attention
data
mining
community
outlier
detection
problem
large
multidimensional
data
sets
see
example
2
4
similar
4
approach
utilizes
hierarchical
clustering
solve
problem
advantages
approach
unlike
many
methods
places
fewer
assumptions
data
can
used
various
distance
similarity
measures
based
clustering
algorithm
developed
complete
procedure
outlier
identification
start
working
definition
outliers
samples
members
clusters
``small''
cardinalities
far
away
clusters
``large''
cardinalities
parameter
depends
expected
number
novel
samples
data
used
define
``small''
clusters
note
one
monotonic
feature
hierarchical
clustering
algorithm
cardinalities
clusters
sample
belongs
non
decreasing
respect
number
clusters
feature
algorithm
allows
us
calculate
measure
outlyingness
sample
kmin
smallest
number
clusters
sample
belongs
outlying
cluster
cluster
cardinality
smaller
threshold
un
samples
ranked
kmin
values
top
submitted
investigation
scientists
use
lowly
populated
clusters
consistent
hypothesis
novel
samples
represent
small
fraction
data
discuss
use
different
similarity
measures
method
particular
focus
measures
traditionally
used
analysis
mass
spectral
data
also
present
bootstrap
procedure
used
assess
``confidence''
one
outlyingness
samples
identified
method
journal
computational
graphics
statistics
overview
session
organizer
andreas
buja
session
time
friday
june
15th
2
00pm
3
45pm
room
location
laguna
room
session
chair
andreas
buja
computational
approach
full
nonparametric
bayesian
inference
dirichlet
process
mixture
models
author
alan
gelfand
university
connecticut
athanasios
kottas
duke
university
abstract
widely
used
parametric
generalized
linear
models
unfortunately
somewhat
limited
class
specifications
nonparametric
aspects
often
introduced
enrich
class
resulting
semiparametric
models
focusing
single
sample
problems
many
classical
nonparametric
approaches
limited
hypothesis
testing
allow
estimation
limited
certain
functionals
underlying
distributions
moreover
associated
inference
often
relies
upon
asymptotics
nonparametric
specifications
often
appealing
smaller
sample
sizes
bayesian
nonparametric
approaches
avoid
asymptotics
date
limited
range
inference
working
dirichlet
process
priors
extend
effort
gelfand
mukhopadhyay
1995
paper
inference
confined
posterior
moments
linear
functionals
population
distribution
provide
computational
approach
obtain
entire
posterior
distribution
general
functionals
illustrate
three
applications
investigation
extreme
value
distributions
associated
single
population
comparison
medians
sample
problem
comparison
survival
times
different
populations
fairly
heavy
censoring
hierarchical
model
based
clustering
large
datasets
author
christian
posse
kangaroonet
inc
abstract
recent
years
hierarchical
model
based
clustering
provided
promising
results
variety
applications
however
use
large
datasets
hindered
time
memory
complexity
least
quadratic
number
observations
overcome
difficulty
propose
start
hierarchical
agglomeration
efficient
classification
data
many
classes
rather
usual
set
singleton
clusters
initial
partition
derived
subgraph
minimum
spanning
tree
associated
data
end
develop
graphical
tools
assess
presence
clusters
data
uncover
observations
difficult
classify
using
approach
analyze
two
large
real
datasets
multi
band
mri
image
human
brain
data
global
precipitation
climatology
last
case
discuss
ways
integrating
spatial
information
clustering
analysis
focus
two
stage
methods
second
stage
processing
using
established
methods
applied
output
algorithm
presented
paper
viewed
first
stage
software
support
bayesian
analysis
systems
session
organizer
wray
buntine
bernd
fischer
johann
schumann
session
time
friday
june
15th
4
15pm
6
00pm
room
location
santa
ana
room
session
chair
bernd
fischer
nasa
ames
computing
environments
bayesian
statistics
author
robert
gentleman
harvard
school
public
health
abstract
bayesian
computing
application
standard
bayesian
methods
particular
problems
certainly
important
able
apply
methodology
practical
problems
easily
perhaps
important
platform
encourages
development
new
bayesian
methodology
helps
us
extend
functionality
existing
tools
relatively
computing
environments
available
bayesian
computing
talk
will
discuss
functionality
needed
bayesian
computing
implementations
available
addition
changes
enhancements
existing
packages
will
encourage
use
bayesian
methods
will
proposed
prototypes
will
explored
stochastic
parameterized
grammars
bayesian
model
composition
author
eric
mjolsness
jpl
michael
turmon
jpl
wolfgang
fink
jpl
abstract
bayesian
analysis
systems
typically
input
language
describing
probabilistic
models
upon
exact
approximate
inference
performed
one
algorithmic
engines
proposed
many
useful
generative
probabilistic
models
can
appropriately
expressed
form
stochastic
grammars
recursively
generate
sets
words
numerical
parameters
attached
rules
stochastic
parameterized
grammars
spg
power
boltzmann
probability
distribution
suggesting
use
mean
field
theory
methods
model
inversion
model
composition
arises
level
multiple
rules
grammar
also
level
entire
grammars
called
subroutines
implement
rule
grammars
spg
viewpoint
raises
new
possibilities
interacting
subject
domain
experts
create
statistical
models
data
analysis
algorithms
raises
new
challenges
language
system
implementor
areas
mathematical
notation
algorithm
composition
using
clocked
objective
functions
software
synthesis
bayes
net
toolbox
matlab
author
kevin
murphy
uc
berkeley
abstract
bayes
net
toolbox
bnt
matlab
software
package
directed
graphical
models
supports
exact
approximate
inference
parameter
structure
learning
static
temporal
models
widely
used
academia
teaching
research
bnt
web
site
receives
average
300
hits
per
week
talk
will
describe
features
distinguish
bayes
net
software
packages
advantages
disadvantages
using
matlab
plus
plans
future
work
details
see
http
http
cs
berkeley
edu
murphyk
bayes
bnt
html
massive
data
sets
session
organizer
greg
ridgeway
session
time
friday
june
15th
4
15pm
6
00pm
room
location
costa
mesa
room
session
chair
matthew
schonlau
rand
data
squashing
constructing
summary
data
sets
author
william
dumouchel
shannon
labs
abstract
one
chief
obstacles
effective
data
mining
clumsiness
managing
analyzing
data
large
files
process
model
search
model
fitting
often
require
many
passes
large
dataset
random
access
elements
large
dataset
many
statistical
fitting
algorithms
assume
entire
dataset
analyzed
fits
computer
memory
restricting
number
feasible
analyses
define
``large
dataset''
one
analyzed
using
particular
desired
combination
hardware
software
computer
memory
constraints
two
basic
approaches
problem
either
switch
different
hardware
software
analysis
strategy
else
substitute
smaller
dataset
large
one
assume
former
strategy
unavailable
undesirable
consider
ways
constructing
smaller
substitute
dataset
latter
approach
named
data
squashing
dumouchel
volinsky
johnson
cortes
pregibon
1999
``squashing
flat
files
flatter''
kdd'99
proceedings
formally
data
squashing
form
lossy
compression
attempts
preserve
statistical
information
suppose
original
``mother''
dataset
matrix
rows
entities
columns
variables
squashed
dataset
matrix
rows
1
columns
ll
extra
column
column
weights
1
ldots
0
sum
assumed
small
enough
can
processed
desired
hardware
software
software
can
make
appropriate
use
weight
variable
dimensional
distribution
rows
weighted
intended
approximate
distribution
rows
well
enough
statistical
analysis
acceptable
substitute
desired
analysis
squashing
procedure
evaluated
much
closely
modeling
squashed
pseudo
data
approximates
results
full
data
results
random
sample
size
methods
data
squashing
will
presented
compared
exploratory
analysis
retail
sales
billions
items
author
william
eddy
carnegie
mellon
university
dunja
mladenic
stefan
institute
slovenia
carnegie
mellon
univ
usa
scott
ziolko
carnegie
mellon
university
abstract
report
preliminary
analyses
data
set
collected
past
year
grocery
chain
containing
hundreds
stores
record
data
set
represents
individual
item
processed
individual
laser
scanner
particular
store
particular
time
particular
day
record
contains
additional
information
store
department
price
etc
together
identifying
information
particular
checkout
scanner
transactions
customer
identification
total
data
set
contains
billions
items
can
aggregated
hundreds
millions
transactions
millions
repeat
customers
talk
will
describe
number
analyses
undertaken
simply
focused
ascertaining
``quality''
data
others
narrowly
focussed
simple
questions
like
``which
pairs
items
frequently
purchased
together''
``what
relationship
basket
size
number
baskets
''
sheer
size
data
set
forced
us
go
beyond
simple
``data
mining''
methods
become
involved
``meta
mining
''
post
processing
results
basic
analyses
mining
large
datasets
author
johannes
gehrke
cornell
university
abstract
talk
two
parts
will
first
survey
recent
work
scalable
decision
tree
construction
massive
training
databases
second
part
will
address
algorithms
mining
high
speed
data
streams
census
2000
lessons
census
2010
session
organizer
nancy
gordon
session
time
friday
june
15th
4
15pm
6
00pm
room
location
viejo
room
session
chair
nancy
gordon
census
bureau
technology
2010
census
author
carol
van
horn
census
bureau
abstract
success
2000
census
due
part
application
new
technologies
image
capture
internet
laptop
computers
data
collection
activities
coming
decade
census
bureau
planning
program
integrates
three
components
collecting
long
form
data
national
survey
american
community
survey
acs
enhancing
master
address
file
tiger
geographic
database
bring
compliance
gps
coordinates
re
engineered
2010
short
form
census
next
census
2010
advances
technology
will
provide
opportunities
successes
research
testing
will
involve
handheld
computers
equipped
gps
creation
initial
address
list
use
nonresponse
field
followup
activities
enabling
field
workers
enter
responses
directly
computer
file
short
form
data
collection
holds
promise
expanding
internet
electronic
reporting
options
modes
data
collection
paper
describes
opportunities
benefits
challenges
census
bureau
faces
using
technology
2010
census
expanded
use
technology
2010
will
greatly
reduce
census
bureau's
reliance
paper
questionnaires
census
bureau's
maf
tiger
system
internal
external
interfaces
author
robert
marx
census
bureau
linda
franz
census
bureau
abstract
census
bureau's
overall
mission
preeminent
collector
provider
timely
relevant
quality
data
people
economy
united
states
accomplish
mission
census
bureau
using
master
address
file
topologically
integrated
geographic
encoding
referencing
maf
tiger
system
fifteen
years
support
various
census
sample
survey
activities
addition
maf
tiger
database
used
foundation
burgeoning
geographic
information
system
gis
industry
united
states
support
analytical
programs
gis
activities
managed
federal
agencies
numerous
state
local
tribal
governments
private
sector
academic
organizations
maf
tiger
system
aging
national
resource
census
bureau
needs
prepare
significantly
automation
21st
century
including
improvement
street
map
feature
locations
addition
accurate
housing
unit
locations
enhanced
feature
change
detection
methodology
provide
timely
updates
modernizing
processing
environment
``home
grown''
systems
one
based
cots
gis
software
accomplishing
needed
improvements
will
increase
effectiveness
sponsoring
participating
organizations
depend
census
bureau's
statistical
data
geographic
infrastructure
title
abstract
unavailable
author
latanya
sweeney
carnegie
mellon
university
abstract
gene
expression
ii
contributed
session
session
time
friday
june
15th
4
15pm
6
00pm
room
location
capistrano
room
session
chair
maryann
hill
uc
irvine
assessing
patient
survival
using
microarray
gene
expression
data
via
partial
least
squares
proportional
hazard
regression
author
danh
nguyen
uc
davis
david
rocke
uc
davis
abstract
high
dimensional
data
sets
microarray
experiments
number
variables
genes
far
exceed
number
samples
render
traditional
statistical
tools
little
direct
use
however
statistical
tools
used
conjunction
appropriate
dimension
reduction
method
can
effective
paper
introduce
use
proportional
hazard
ph
regression
cox
1972
conjunction
dimension
reduction
partial
least
squares
pls
since
number
covariates
exceeds
number
samples
setting
typical
gene
expression
data
dna
microarrays
specifically
given
vector
response
values
times
event
death
censored
times
gene
expressions
covariates
address
issue
assess
estimate
survival
experience
curve
ll
approach
taken
cope
high
dimensionality
reduce
dimension
via
dimension
reduction
component
extraction
method
first
stage
estimate
survival
distribution
using
ph
regression
model
second
stage
primary
method
component
extraction
considered
pls
pls
achieves
dimension
reduction
constructing
components
maximize
covariance
response
survival
times
linear
combination
covariates
gene
expressions
sequentially
analogous
principal
components
analysis
pca
optimization
criterion
pca
variance
rather
covariance
pls
demonstrate
use
methodology
diffuse
large
cell
lymphoma
dlbcl
textit
complementary
dna
cdna
data
set
lessons
learned
analyzing
differential
gene
expression
data
normal
tumor
tissues
head
neck
cancer
patients
author
jack
lee
university
texas
anderson
cancer
center
hyung
woo
kim
university
texas
anderson
cancer
center
feng
zhan
university
texas
anderson
cancer
center
adel
el
naggar
university
texas
anderson
cancer
center
abstract
gene
expression
head
neck
cancer
patients
assessed
using
research
genetics
cdna
membranes
gf200
gf211
major
objective
identify
differentially
expressed
genes
normal
tumor
tissues
presentation
will
share
many
lessons
learned
acquiring
displaying
analyzing
microarray
data
data
analysis
experimental
condition
need
carefully
documented
example
information
patient
characteristics
tissue
extraction
method
choice
primer
lot
number
strip
number
membrane
hybridization
procedure
exposure
time
parameters
image
acquisition
needs
recorded
performed
duplicate
experiments
cases
aligned
images
multiple
times
estimate
alignment
variability
experiment
variability
patient
variability
effect
due
multiple
stripping
membrane
also
inspected
standardization
background
correction
nonparametric
regression
based
loess
method
examined
differential
expressed
genes
identified
computing
raw
folds
change
statistics
change
respect
interquartile
range
exploratory
graphical
methods
using
hexbin
plot
brushing
methods
applied
results
data
analysis
using
various
tools
compared
conclusion
data
analysis
biological
interpretation
will
reported
taming
genetic
microarray
data
paradigm
using
well
known
case
study
author
howard
thaler
memorial
sloan
kettering
cancer
center
abstract
microarray
technology
gene
chip
expression
analysis
probe
array
affymetrix
generates
expression
levels
thousands
genes
single
specimen
array
data
used
characterize
genetic
differences
individuals
types
tissue
however
statistical
properties
data
produced
present
challenges
interpretation
statistical
analyses
identification
true
outliers
associated
causal
genes
data
deviate
wildly
classical
statistical
assumptions
normality
homoscedasticity
additivity
well
known
publicly
available
leukemia
patients
data
set
published
gollub
et
al
means
standard
deviations
varied
several
orders
magnitude
cost
constrained
modest
sample
size
precludes
using
asymptotic
approximations
simple
data
transformation
tamed
data
come
reasonably
close
satisfying
assumptions
several
ad
hoc
criteria
number
genes
measured
exceeded
number
sample
specimens
100
fold
simple
dimensionality
reduction
strategy
ameliorated
multiplicity
problem
facilitated
evaluation
group
differences
covariate
effects
yielding
focused
results
transformed
data
raw
data
conclusion
proposed
paradigm
analyzing
microarray
gene
expression
data
yielded
precise
concise
reliable
results
statistical
modelling
micro
array
data
author
ziad
taib
biostatistics
astrazeneca
lndal
abstract
summarize
statistical
issues
encountered
attempting
analyse
gene
expression
data
try
argue
basing
analysis
statistical
model
can
far
rewarding
using
ad
hoc
methods
cut
criteria
national
science
foundation
overview
session
organizer
james
rosenberger
session
time
friday
june
15th
4
15pm
6
00pm
room
location
laguna
room
session
chair
james
rosenberger
penn
state
unraveling
defining
biocomplexity
author
william
michener
university
new
mexico
james
rosenberger
penn
state
university
abstract
presentation
discuss
biocomplexity
word
describes
new
research
focus
evolved
past
three
years
nsf
fosters
interdisciplinary
research
understand
model
complex
interrelationships
underlying
biological
systems
examples
biocomplexity
research
given
research
paradigms
described
essential
components
success
presented
importance
mathematical
statistical
sciences
can
seen
integrating
components
reductionist
research
quantitative
model
provides
predictive
outcome
appropriate
measures
uncertainly
biocomplexity
biocomplexity
environment
funding
programs
will
described
opportunities
statisticians
mathematicians
computer
scientists
discussed
theoretical
computational
challenges
entropy
evaluation
macromolecules
author
harshinder
singh
west
virginia
university
james
harner
west
virginia
university
eugene
demchuk
niosh
held
vladimir
hnizdo
niosh
held
abstract
evaluation
entropy
important
biological
processes
order
predict
stability
molecular
conformation
entropy
evaluation
requires
probabilistic
modeling
conformations
internal
coordinates
since
fluctuations
rotational
angle
torsional
coordinates
make
pivotal
contribution
overall
configurational
entropy
molecule
review
circular
probability
modeling
approaches
modeling
torsional
angles
since
macromolecules
proteins
large
number
interdependent
torsional
angles
distributions
many
multimodal
even
skewed
discuss
theoretically
computationally
challenging
problems
arise
simultaneous
modeling
angles
based
data
molecular
dynamics
simulations
computational
methods
tools
contributed
session
session
time
saturday
june
16th
9
00am
10
45am
room
location
santa
ana
room
session
chair
william
christenen
smu
ciphertext
size
requirement
ciphertext
attack
vigenere
cipher
author
qiong
yang
boston
university
song
guo
college
computer
science
northeastern
university
abstract
index
coincidence
ic
testing
method
used
cryptoanalysis
vigenere
cipher
however
testing
method
used
based
intuition
rather
probability
theory
paper
studied
statistical
properties
ic
test
proved
ic
unbiased
estimator
sum
2
furthermore
using
cauchy
inequality
one
sample
statistic
theory
proved
ic
gaussian
distributed
based
results
developed
probability
framework
ic
testing
method
applied
determine
sample
size
requirement
interval
computation
gamma
probabilities
inverses
author
trong
wu
southern
illinois
university
edwardsville
abstract
new
method
computing
gamma
cumulative
distribution
functions
inverses
presented
paper
method
uses
two
continued
fractions
computation
one
incomplete
gamma
function
complement
incomplete
gamma
function
improved
interval
method
computation
implemented
language
classes
used
self
validated
computation
developed
programming
techniques
speed
increment
iterative
loops
finding
inverse
gamma
cumulative
distribution
function
given
probability
fact
inverses
can
considered
random
gamma
variates
uniform
random
number
generator
used
generate
probabilities
interval
0
1
entire
computation
involves
two
simple
algebraic
functions
use
transcendental
functions
auxiliary
functions
power
series
newton's
method
computation
therefore
one
can
expect
easy
implement
smooth
quadratures
volterra
integral
equations
applications
estimation
hiv
infection
rates
projection
aids
incidence
author
john
hsieh
university
toronto
abstract
many
scientific
research
problems
arise
solutions
volterra
integral
equations
first
kind
given
function
mu
expressed
integral
0
product
known
kernel
unknown
function
lambda
respect
mu
lambda
positive
0
leq
leq
tau
positive
tau
positive
geq
0
square
integrable
0
leq
leq
tau
0
leq
leq
tau
biomedical
research
problems
data
mu
often
come
step
functions
time
method
quadratures
may
employed
solve
equation
estimate
lambda
step
function
time
however
method
tends
yield
negative
erratic
values
solutions
lambda
obtain
non
negative
smooth
estimates
lambda
shall
treat
lambda
mean
linear
non
homogeneous
poisson
process
mu
mean
translated
planar
non
homogeneous
poisson
process
probability
distribution
translation
linear
poisson
planar
poisson
construct
likelihood
function
lambda
terms
planar
poisson
point
process
probability
distribution
em
expectation
maximization
formula
derived
iterative
estimation
lambda
smooth
nonparametric
maximum
likelihood
estimates
lambda
obtained
applying
em
algorithm
coupled
smoothing
step
iteration
accuracy
estimates
lambda
can
checked
comparing
estimates
mu
calculated
volterra
equation
using
estimated
solution
lambda
known
distribution
observed
planar
poisson
process
via
statistical
goodness
fit
tests
furthermore
extrapolating
estimates
lambda
future
volterra
integral
equation
can
used
project
future
course
mu
function
paper
employed
various
parametric
nonparametric
distributions
applied
method
estimate
hiv
infection
rates
make
short
term
projections
aids
incidence
using
hiv
aids
diagnosis
data
parametric
distributions
weibull
gamma
distributions
several
new
functions
non
parametric
distributions
linear
cubic
spline
functions
chosen
take
account
fact
hiv
screening
test
available
since
1985
treatment
made
available
hiv
positive
patients
1987
method
produced
results
fit
observed
aids
incidence
better
produced
existing
methods
based
data
canada
australia
designing
experiments
causal
networks
author
william
heavlin
advanced
micro
devices
abstract
causal
networks
generalizations
ishikawa
diagrams
emphasizing
tolerance
design
applications
work
presents
optimal
design
algorithm
variables
organized
causal
network
causal
network
transformed
causal
map
represents
factors
responses
points
common
dimensional
metric
space
design
approach
algorithmic
optimizing
wynn
entropy
criterion
criterion
maximizes
dispersion
among
multiple
responses
using
distance
space
coefficients
disco
model
key
constraint
block
self
containment
blocks
analyzable
without
reference
one
another
complemented
block
analyses
criteria
response
dispersion
efficiency
column
rank
skewing
blocks
target
factors
explored
multi
layer
structured
correlation
designs
heterogeneous
unbalanced
clustered
data
author
edward
chao
insightful
corporation
abstract
data
high
dimensional
hierarchical
structures
often
occur
longitudinal
studies
geographical
studies
family
studies
usual
approach
multi
level
random
effects
models
situation
interest
case
number
clusters
small
number
hierarchical
levels
cluster
large
particular
interests
focus
heterogeneous
unbalanced
clustering
multi
level
models
might
difficulty
fitting
data
unbalanced
clusters
propose
designing
correlation
multi
layer
structures
layer
represents
unique
parameterization
correlation
type
chosen
generic
structures
ar
exchangeable
stationary
etc
nested
designs
factor
nested
another
factor
multiple
levels
cases
layer
correlation
corresponds
modeling
correlation
structure
within
factor
hierarchical
structures
layer
may
consist
multiple
blocks
blocks
layer
parameterization
blocks
represent
levels
factor
associated
layer
approach
can
easily
extended
unbalanced
hierarchical
structures
heterogeneous
clusters
algorithms
based
approach
embedded
gee
methods
case
study
prostate
cancer
simulation
studies
show
approach
efficient
existing
gee
methods
multivariate
methods
perfect
stability
characteristic
function
author
jinhyo
kim
cheju
national
university
south
korea
bongsu
ko
cheju
national
university
south
korea
abstract
algorithm
stability
used
support
notion
cf
superior
mgf
terms
numerically
stable
behavior
shown
exist
computationally
better
tool
cf
terms
numerical
stability
uniqueness
vandermonde
matrix
perfect
condition
number
characterized
numerical
behavior
cf
statistical
software
web
based
applications
contributed
session
session
time
saturday
june
16th
9
00am
10
45am
room
location
viejo
room
session
chair
deborah
swayne
environment
creating
interactive
statistical
documents
author
samuel
buttrey
naval
postgraduate
school
deborah
nolan
university
california
berkeley
duncan
temple
lang
bell
laboratories
abstract
spectacular
growth
acceptance
web
made
attractive
medium
interactive
documents
web
based
reporting
industry
``live''
documents
research
interactive
worksheets
education
material
many
ways
ideal
uses
web
types
documents
frequently
display
dynamic
statistical
output
form
text
plots
unfortunately
much
effort
creating
types
documents
focussed
re
inventing
existing
statistical
software
often
inferior
results
reason
systems
sas
integrated
reader's
browser
better
approach
allow
author
create
document
using
common
authoring
tools
latex
ms
word
html
editors
conveniently
insert
dynamic
interactive
components
languages
author
focuses
presentation
display
components
including
usual
multi
media
elements
text
images
sounds
uses
html
form
elements
java
components
provide
interactive
controls
reader
can
manipulate
contents
document
finally
performs
statistical
computations
renders
visual
displays
using
statistical
software
embedded
within
reader's
browser
presentation
describe
created
environment
interactive
statistical
documents
allows
author
use
html
javascript
create
content
interactivity
reader
accesses
interactive
dynamic
functionality
document
via
plug
netscape
embeds
within
different
languages
reasonably
standard
tools
used
purposes
designed
makes
reasonably
straightforward
environment
quickly
simply
create
interfaces
various
different
applications
audiences
course
web
based
statistics
author
juergen
symanzik
utah
state
university
natascha
vukasinovic
utah
state
university
abstract
many
statistics
courses
taught
make
use
web
based
statistical
tools
teachware
tools
electronic
textbooks
statistical
software
web
however
best
knowledge
course
statistical
issues
web
systematically
discussed
talk
provide
overview
web
based
statistics
course
including
detailed
discussions
lecture
topics
homework
assignments
student
projects
discuss
references
papers
urls
useful
class
summarize
student
surveys
conducted
course
finish
talk
recommendations
future
similar
courses
assist
package
spline
smoothing
plus
template
author
yuedong
wang
univ
california
chunlei
ke
st
jude
medical
abstract
present
suite
user
friendly
plus
functions
fitting
among
others
smoothing
spline
models
independent
correlated
gaussian
data
independent
binomial
poisson
gamma
data
semi
parametric
regression
models
non
parametric
mixed
effects
models
semi
parametric
nonlinear
mixed
effects
models
general
form
smoothing
splines
based
reproducing
kernel
hilbert
space
used
model
non
parametric
functions
thus
plus
functions
deal
many
different
situations
unified
fashion
well
known
special
cases
polynomial
splines
including
popular
cubic
splines
periodic
splines
spherical
splines
thin
plate
splines
splines
generalized
additive
models
smoothing
spline
anova
models
self
modeling
nonlinear
regression
models
fixed
mixed
non
parametric
semi
parametric
models
widely
used
practice
analyze
data
arise
many
areas
investigation
medicine
epidemiology
pharmacokinetics
social
science
one
goal
software
development
collect
existing
programs
make
user
friendly
researchers
can
use
ease
also
written
new
programs
fill
gaps
java
implementation
multiple
linear
regression
models
patient
specific
longitudinal
data
monitor
chemotherapy
induced
anemia
author
christine
mclaren
university
california
irvine
wagner
truppel
university
california
irvine
randall
holcombe
chao
family
comprehensive
cancer
center
edward
kambour
prostrategic
solutions
abstract
physicians
typically
compare
laboratory
result
individual
patient
previous
values
population
based
reference
ranges
determine
significance
change
provide
statistical
basis
process
previously
developed
approach
sequentially
analyze
laboratory
test
results
identify
departures
past
values
statistical
methods
include
hierarchical
multiple
regression
modeling
weighted
minimum
risk
criteria
model
selection
choose
models
indicating
changes
mean
values
time
optimal
model
chosen
one
smallest
statistically
significant
weighted
estimated
risk
compared
null
change
model
routine
use
clinical
settings
now
describe
improved
design
implementation
numerical
algorithms
sequential
change
detection
mean
algorithms
enhanced
analytical
versions
matrices
implemented
using
java
programming
language
input
computer
program
included
vector
sequential
readings
desired
statistical
significance
level
positive
weight
factors
used
model
evaluation
read
stored
table
efficient
techniques
developed
computation
gasser
sroka
jenner
steinmetz
gsjs
variance
estimate
associated
unbiased
risk
expected
loss
function
subset
regression
indicator
variables
input
vector
regression
residuals
estimated
auto
correlation
model
residuals
gui
constructed
support
numerical
graphical
input
output
structure
simulations
used
assess
speed
portability
java
implementation
termed
change
detector
analyzed
data
patients
treated
cisplatin
based
chemotherapy
regimens
60
determine
significant
changes
hematocrit
values
compared
original
plus
implementation
found
new
java
program
accurate
easier
use
faster
conclude
change
detector
provides
improved
statistical
program
automated
review
laboratory
data
clinical
setting
development
community
nutrition
map
cnmap
author
alvin
nowverl
usda
ars
bhnrc
cnrg
abstract
community
nutrition
map
cnmap
web
application
display
nutritional
demographic
information
geographic
areas
within
united
states
using
compilation
data
variety
sources
data
sources
include
united
states
department
agriculture's
1994
96
1998
continuing
surveys
food
intakes
individuals
department's
food
nutrition
service
web
site
data
also
obtained
bureau
census'
1995
1998
current
population
surveys
adjusted
total
population
data
files
initial
phase
cnmap
will
provide
reports
state
level
number
nutritional
indicators
1
percentage
individuals
meeting
recommended
daily
allowances
select
group
nutrients
2
percentage
individuals
meeting
minimum
requirements
pyramid
servings
food
groups
3
percentage
american
households
receiving
food
stamps
4
percentage
individuals
using
supplements
development
cnmap
several
statistical
data
processing
issues
addressed
include
maintaining
confidentiality
reporting
survey
data
geographically
obtaining
proper
estimates
combining
different
sources
information
proper
use
sampling
weights
using
static
dynamic
web
page
design
decision
support
forecasting
contributed
session
session
time
saturday
june
16th
9
00am
10
45am
room
location
capistrano
room
session
chair
hyunjoong
kim
cost
growth
models
nasa's
programs
author
tze
san
lee
western
illinois
university
dale
thomas
national
aeronautics
space
administration
abstract
two
cost
growth
indices
annual
absolute
relative
cost
growth
probability
based
models
constructed
basis
functions
nasa's
technology
readiness
levels
use
johnson's
four
parameter
system
bounded
unbounded
lognormal
distributions
addition
statistical
prediction
models
built
programs
result
research
shows
program's
initial
cost
estimate
significant
predictor
program's
annual
absolute
cost
growth
weighted
average
technology
readiness
level
program's
components
significant
predictor
program's
annual
relative
cost
growth
series
approximations
risk
analysis
author
reza
modarres
george
washington
university
costas
christophi
george
washington
university
abstract
several
asymptotic
approximation
methods
computing
distribution
multiplicative
risk
model
discussed
consider
asymptotic
expansion
prod
1
positive
random
variables
independent
identically
distributed
generalized
central
limit
theorem
used
provide
approximation
distribution
study
conditions
approximation
valid
edgeworth
expansion
distribution
discussed
independent
identically
distributed
case
will
also
discuss
saddlepoint
approximation
distribution
quantile
functions
model
accuracy
approximations
illustrate
several
examples
results
compared
exact
available
monte
carlo
results
adequate
statistic
exponentially
distributed
censoring
data
author
nair
creighton
university
cheng
creighton
university
abstract
problem
estimating
parameter
underlying
probability
distribution
sufficient
statistic
one
summarizes
exhausts
relevant
information
parameter
contained
sample
similar
basic
problem
statistics
predicting
future
yet
observed
random
variable
basis
existing
observable
random
variables
parameter
underlying
probability
distribution
concern
us
directly
likewise
looking
statistic
one
exhaustive
relevant
information
future
random
variable
available
current
observable
random
variables
notion
adequate
statistics
initiated
fisher
skibinsky
deal
concern
subsequently
extensively
investigated
literatures
article
make
comprehensive
study
finding
adequate
statistic
total
time
test
data
assumed
exponentially
distributed
censored
th
failure
comparing
two
measurement
devices
review
extensions
estimate
new
device
variability
author
brian
eastwood
eli
lilly
company
abstract
much
literature
available
methods
comparing
two
measurement
systems
supposed
equivalent
methods
briefly
reviewed
context
comparing
two
vitro
assays
measuring
clotting
times
new
method
intended
replace
old
method
basic
study
comparison
results
run
assays
study
possible
determine
relative
performance
assays
respect
bias
variability
standard
deviation
using
techniques
described
bland
altman
1981
lin
1989
determine
``in
agreement''
possible
study
describe
bias
variability
either
assay
usually
much
historical
information
available
``old''
method
incorporating
information
straightforward
obtain
point
estimates
bias
variability
new
assay
using
distributional
assumptions
bootstrap
estimates
one
can
obtain
confidence
interval
estimates
conduct
hypothesis
tests
absolute
relative
performance
two
assays
performance
various
bootstrap
exact
distribution
estimates
compared
first
context
``demonstrating
agreement''
context
examining
new
assay
actually
improved
performance
old
assay
computationally
intensive
techniques
fully
bayesian
decision
theoretic
approach
financial
forecasting
portfolio
selection
author
andrew
simpson
university
newcastle
darren
wilkinson
university
newcastle
abstract
paper
considers
problem
bayesian
modelling
forecasting
multivariate
financial
time
series
example
prices
related
stocks
exhibit
dependencies
series
well
usual
dependencies
time
multivariate
dynamic
linear
state
space
models
west
harrison
1997
often
appropriate
explaining
log
price
behaviour
problem
bayesian
inference
underlying
states
covariance
matrices
examined
variety
algorithms
literature
order
algorithms
work
efficiently
variety
kalman
filtering
smoothing
simulation
smoothing
techniques
required
na
ve
implementations
suffer
problems
associated
slow
mixing
convergence
classification
methods
contributed
session
session
time
saturday
june
16th
9
00am
10
45am
room
location
laguna
room
session
chair
johannes
gehrke
cornell
statistical
view
support
vector
machine
author
yi
lin
university
wisconsin
madison
abstract
establish
relationship
support
vector
machine
bayes
rule
bayes
rule
optimal
classification
rule
underlying
distribution
data
known
therefore
bayes
rule
directly
available
practice
can
used
ideal
benchmark
classification
procedure
show
support
vector
machine
approaches
bayes
rule
asymptotic
sense
results
established
mild
condition
allowing
arbitrary
number
discontinuities
underlying
conditional
probability
function
contrast
asymptotic
results
statistical
literature
underlying
conditional
probability
functions
assumed
smooth
given
order
results
clarify
mechanism
beneath
support
vector
machine
highlight
advantage
limitation
support
vector
machine
methodology
lazy
class
probability
estimators
author
dragos
margineantu
oregon
state
university
thomas
dietterich
oregon
state
university
abstract
many
practical
applications
one
rather
learning
algorithms
compute
accurate
values
probabilities
possible
class
instead
single
class
label
unfortunately
existing
classification
algorithms
give
poor
class
probability
estimates
specifically
designed
maximize
classification
accuracy
result
learned
models
will
output
probability
values
extreme
close
0
1
paper
introduces
new
lazy
learning
algorithm
lazy
option
trees
based
derive
method
computing
good
class
probability
estimates
algorithm
builds
basic
ideas
lazy
decision
tree
classification
algorithm
introduced
friedman
et
al
1996
order
compute
good
probability
estimates
multiple
tests
performed
node
query
instance
algorithm
also
allows
tests
continuous
attributes
performs
local
smoothing
leaf
nodes
class
probability
estimates
improved
using
breiman's
bagging
one
important
uses
accurate
class
probability
estimates
machine
learning
data
mining
prediction
presence
arbitrarily
large
costs
associated
different
kinds
errors
tested
method
different
cost
models
application
domains
uci
ml
repository
majority
tasks
probability
estimates
improve
probability
estimates
decision
trees
bagged
probability
estimation
trees
unpruned
uncollapsed
smoothed
decision
trees
pets
one
best
existing
class
probability
estimators
evaluating
quality
predictions
cost
sensitive
context
employed
paired
bdeltacost
procedure
introduced
margineantu
dietterich
2000
perfect
random
tree
classifiers
author
adele
cutler
utah
state
university
guohua
zhao
abstract
ensemble
classifiers
accurate
general
purpose
classifiers
available
introduce
new
ensemble
classifier
pert
ensemble
perfectly
fit
random
trees
compared
ensemble
methods
pert
fast
fit
considering
random
nature
trees
pert
surprisingly
accurate
calculations
suggest
one
reason
pert
performs
well
although
trees
extremely
weak
also
almost
uncorrelated
multicategory
support
vector
machines
author
yoonkyung
lee
university
wisconsin
madison
yi
lin
university
wisconsin
madison
grace
wahba
university
wisconsin
madison
abstract
support
vector
machine
svm
shown
great
performance
practice
classification
methodology
recently
even
though
svm
implements
optimal
classification
rule
asymptotically
binary
case
one
versus
rest
approach
solve
multicategory
case
using
svm
optimal
proposed
multicategory
svms
extend
binary
svm
multicategory
case
encompass
binary
svm
special
case
multicategory
svm
implements
optimal
classification
rule
sample
size
gets
large
overcoming
suboptimality
conventional
one
versus
rest
approaches
proposed
method
deals
equal
misclassification
cost
unequal
cost
case
unified
way
using
pseudo
predictors
improve
performance
classification
rule
author
majid
mojirsheibani
carleton
university
abstract
consider
iterative
procedure
improve
misclassification
error
rate
initial
classification
rule
proposed
procedure
involves
two
steps
iterative
method
generating
sequence
classifiers
initial
one
ii
combining
procedure
``pools
together''
sequence
constructed
classifiers
order
produce
new
classifier
far
effective
asymptotic
sense
initial
one
sequence
classifiers
step
generated
based
repeated
augmentation
feature
vector
carefully
constructed
pseudo
predictors
mechanics
asymptotic
validity
proposed
procedure
discussed
will
also
discuss
methods
selecting
number
iterations
regression
function
approximation
contributed
session
session
time
saturday
june
16th
11
05am
12
50pm
room
location
santa
ana
room
session
chair
gareth
james
usc
self
modeling
regression
random
effects
author
naomi
altman
cornell
university
julio
villarreal
edvision
corporation
abstract
many
longitudinal
studies
response
can
modeled
discretely
sampled
curve
time
subject
often
curves
common
shape
function
individual
subjects
differ
common
shape
transformation
time
response
scales
lindstrom
1995
represented
common
shape
free
knot
regression
spline
used
parametric
random
effects
model
represent
differences
curves
extend
lindstrom's
work
representing
common
shape
penalized
regression
spline
use
parametric
random
effects
model
represent
differences
curves
use
penalized
regression
splines
allows
generalization
modeling
estimation
testing
parameters
easily
implemented
iterative
two
step
algorithm
proposed
fitting
model
conditional
fitted
common
shape
model
possible
fit
test
nonlinear
mixed
effects
using
standard
methods
sieve
parametric
form
model
suggests
conditional
likelihood
ratio
test
available
testing
whether
shape
varies
time
invariant
covariate
null
distribution
likelihood
ratio
test
may
chi
squared
support
vector
machine
regression
chemometrics
author
ayhan
demiriz
verizon
inc
kristin
bennett
rensselaer
polytechnic
institute
curt
breneman
rensselaer
polytechnic
institute
mark
embrechts
rensselaer
polytechnic
institute
abstract
predicting
biological
activity
compound
chemical
structure
fundamental
problem
drug
design
ability
exists
generate
vast
amounts
potential
pharmecutical
compounds
statistical
machine
learning
methods
can
provide
efficient
means
estimating
bioreponses
compounds
order
expedite
drug
design
paper
develop
support
vector
machine
regression
svmr
methodology
estimating
bioresponse
molecules
based
large
sets
descriptors
since
concerned
data
characterized
large
numbers
descriptors
data
points
adapt
svmr
model
selection
bagging
strategies
order
avoid
overfitting
proposed
approach
compares
favorably
partial
least
squares
pls
well
known
commonly
used
method
chemometrics
performance
quantitative
structure
activity
relationships
qsar
analysis
based
real
chemistry
data
data
driven
optimal
denoising
signal
recovery
derivative
using
multiwavelets
author
nathaniel
tymes
jr
university
new
mexico
sam
efromovich
university
new
mexico
cristina
pereyra
university
new
mexico
joseph
lakey
new
mexico
state
university
abstract
multiwavelets
relative
newcomers
world
wavelets
thus
surprise
used
methods
denoising
modified
universal
thresholding
procedures
developed
uniwavelets
hand
specific
multiwavelet
discrete
transform
typical
errors
identically
distributed
independent
normal
errors
thus
suggest
alternative
denoising
procedure
based
efromovich
pinsker
algorithm
rip
gams
application
human
brain
research
author
michael
schimek
karl
franzens
university
graz
abstract
backfitting
still
popular
numerical
technique
generalized
additive
models
gam
implementation
plus
stable
sufficient
large
number
fitting
problems
take
interest
gam
fitting
rather
complicated
data
showing
patterns
correlation
result
account
rank
deficiency
system
matrix
due
spatial
temporal
correlation
variables
illustrate
example
human
brain
research
cope
situations
introduce
idea
relaxed
iterative
projection
generalized
additive
models
rip
gam
backfitting
gam
rip
gam
common
use
plus
functions
provided
generalized
additive
modelling
spline
smoother
features
main
results
example
rip
seem
run
numerical
troubles
backfitting
slow
convergence
instances
standard
situations
however
procedures
produce
estimation
results
adaptive
learned
temporal
radial
basis
function
network
recursive
function
estimation
author
yiu
ming
cheung
chinese
university
hong
kong
lei
xu
chinese
university
hong
kong
abstract
present
temporal
radial
basis
function
rbf
network
recursive
function
estimation
network
dynamic
hybrid
system
consists
two
sub
rbf
networks
one
sub
network
models
relationship
current
network
output
past
ones
sub
network
describes
relationship
current
network
output
inputs
sub
network
kernel
parameters
hidden
layer
output
layer
adaptively
determined
globally
using
expectation
maximization
em
algorithm
xu
1998
performance
proposed
network
also
demonstrated
comparison
classic
one
visualization
image
data
contributed
session
session
time
saturday
june
16th
11
05am
12
50pm
room
location
viejo
room
session
chair
michael
minnotte
utah
state
statistical
approach
segmentation
mr
imagery
volume
estimation
stroke
lesions
author
benjamin
stein
university
massachusetts
joseph
horowitz
university
massachusetts
abstract
propose
3d
method
segment
magnetic
resonance
imagery
mri
ischemic
stroke
patients
lesion
background
hence
estimate
lesion
volumes
hierarchical
regularized
method
based
classical
statistics
produces
rigorous
confidence
interval
lesion
volume
approach
requires
limited
amount
user
interaction
initialize
procedure
tested
real
mr
data
volume
estimates
within
6
derived
doctors'
hand
segmentations
according
physicians
working
results
clinically
useful
evaluate
stroke
therapies
visualizing
spatial
autocorrelation
dynamically
linked
windows
author
luc
anselin
university
illinois
urbana
champaign
ibnu
syabri
university
illinois
urbana
champaign
oleg
smirnov
university
texas
dallas
yanqui
ren
university
illinois
urbana
champaign
abstract
several
recent
efforts
focused
adding
exploratory
data
analysis
functionality
geographic
information
systems
means
various
coupling
mechanisms
established
statistical
software
packages
gis
paper
outline
alternative
approach
functionality
built
scratch
using
combination
small
libraries
dedicated
functions
rather
relying
full
scope
existing
software
suites
suggested
approach
modular
completely
freestanding
allowing
use
data
formats
different
vendors
combines
within
overall
framework
fully
dynamically
linked
windows
cartographic
representation
data
map
traditional
statistical
graphics
histograms
box
plots
scatterplots
addition
includes
several
devices
visualize
spatial
autocorrelation
lattice
regional
data
moran
scatterplot
lisa
maps
apart
freestanding
new
program
dynesda2
implements
number
advances
capability
brush
polygon
coverages
simultaneous
linking
multiple
maps
multiple
statistical
graphics
interactive
lisa
maps
compression
analysis
large
imagery
data
sets
using
spatial
statistics
author
james
shine
george
mason
university
us
army
topgraphic
engineering
center
abstract
remote
sensing
instruments
evolve
size
imagery
data
sets
derived
remote
sensing
continues
increase
several
satellites
currently
offer
resolution
1
meter
per
pixel
better
resolution
even
small
geographic
area
leads
large
data
set
1
square
mile
example
represented
approximately
2
6
times
10
6
pixels
many
sensors
now
multispectral
even
hyperspectral
increasing
size
data
set
10
2
processing
images
classification
mapping
purposes
thus
poses
increasing
computational
challenge
paper
describes
use
spatial
statistics
compress
size
large
1
meter
imagery
data
sets
images
taken
locations
united
states
using
camis
computerized
airborne
multispectral
imaging
system
instrument
flown
airplane
registered
trained
image
analysts
models
spatial
variation
first
computed
entire
image
subsampled
sets
image
parameters
models
used
compress
original
image
image
analysis
operations
performed
original
compressed
images
performance
compared
cases
possible
compress
data
several
orders
magnitude
without
substantially
degrading
results
subsequent
analysis
hierarchical
visualization
environmental
data
web
using
nvizn
author
lacey
jones
utah
state
university
abstract
statistical
analyses
large
scale
data
can
often
hard
interpret
many
times
several
pages
numbers
must
used
describe
data
makes
finding
numerical
output
interest
tedious
difficult
converting
numbers
information
understandable
useful
someone
without
extensive
statistical
background
also
task
easily
accomplished
visual
representations
maps
graphs
charts
can
aid
process
help
create
better
understanding
visual
representation
information
processes
data
explaining
recent
improvement
visual
statistics
use
internet
new
illumitek
software
tool
nvizn
follow
graphics
production
library
interactive
tool
allows
user
expand
narrow
focus
visual
representation
order
overlay
rearrange
format
data
fast
user
connection
can
process
without
needing
statistical
computing
background
software
also
utilizes
internet
give
opportunities
easy
access
anyone
internet
connection
graphical
user
interface
without
purchase
personal
statistical
computing
package
impractical
price
will
using
new
software
creating
hierarchical
visual
images
maps
charts
data
modeling
concentration
estimates
148
hazardous
air
pollutants
60
803
census
tracts
continental
united
states
obtained
epa
cumulative
exposure
project
hierarchical
interactive
visualization
system
author
peter
tino
aston
university
uk
ian
nabney
aston
university
uk
yi
sun
aston
university
uk
abstract
paper
propose
interactive
hierarchical
visualization
system
level
hierarchy
provides
user
data
projections
also
corresponding
magnification
factor
directional
curvature
plots
magnification
factors
quantify
extend
areas
magnified
projection
data
space
directional
curvatures
capture
local
folding
patterns
projection
manifold
visualization
system
constructed
statistically
principled
framework
data
mining
contributed
session
session
time
saturday
june
16th
11
05am
12
50pm
room
location
capistrano
room
session
chair
eddie
ip
usc
tree
based
scan
statistic
database
disease
surveillance
author
martin
kulldorff
university
connecticut
school
medicine
zixing
fang
university
connecticut
school
medicine
stephen
walsh
university
connecticut
school
medicine
abstract
many
databases
exist
possible
study
relationship
health
events
various
potential
risk
factors
among
databases
variables
naturally
form
hierarchical
tree
structure
pharmaceutical
drugs
occupations
example
ecotrin
brand
aspirin
belongs
class
nonsteoridal
anti
inflammatory
drug
turn
belongs
larger
class
analgesic
drugs
another
example
occupational
classification
system
census
`statisticians'
subset
`mathematical
computer
scientists'
subset
`professional
specialty
occupations'
turn
subset
`managerial
professional
specialty
occupations'
paper
propose
tree
based
scan
statistic
database
surveillance
use
used
independent
variable
can
defined
form
hierarchical
tree
proposed
method
illustrated
looking
whether
death
silicosis
particularly
common
among
specific
occupations
classified
census
bureau
without
preconceived
idea
specific
occupation
group
occupations
may
related
increased
risk
method
can
used
many
different
types
databases
proposed
method
will
described
terms
`occupation'
`mortality'
creating
ensembles
decision
trees
sampling
author
chandrika
kamath
lawrence
livermore
national
laboratory
erick
cantu
paz
lawrence
livermore
national
laboratory
abstract
recent
work
classification
indicates
significant
improvements
accuracy
can
obtained
growing
ensemble
classifiers
vote
popular
class
paper
focuses
ensembles
decision
trees
created
randomized
procedure
based
sampling
randomization
can
introduced
using
random
samples
training
data
bagging
arcing
running
conventional
tree
building
algorithm
randomizing
induction
algorithm
objective
paper
describe
first
experiences
novel
randomized
tree
induction
method
uses
subset
samples
node
determine
split
empirical
results
show
ensembles
generated
using
approach
yield
results
competitive
accuracy
superior
computational
cost
data
mining
diabetic
databases
rough
sets
useful
addition
author
joseph
breault
tulane
university
alton
ochsner
medical
foundation
abstract
publicly
available
pima
indian
diabetic
database
pidd
uc
irvine
machine
learning
lab
become
standard
testing
data
mining
algorithms
see
accuracy
predicting
diabetic
status
8
variables
given
looking
392
complete
cases
guessing
non
diabetic
gives
accuracy
65
1
since
1988
many
dozens
publications
using
various
algorithms
resulted
accuracy
rates
66
81
rough
sets
data
mining
predictive
tool
used
medical
areas
since
late
1980s
applied
pidd
knowledge
apply
rough
sets
pidd
using
rosetta
software
predictive
accuracy
82
6
better
data
mining
methods
aware
rough
sets
useful
addition
analysis
diabetic
databases
model
complexity
based
design
radial
basis
function
networks
data
mining
applications
author
miyoung
shin
syracuse
university
etri
korea
amrit
goel
syracuse
university
abstract
radial
basis
function
rbf
models
particular
class
neural
networks
recently
become
popular
pattern
recognition
tasks
fast
learning
capability
good
mathematical
properties
best
universal
approximation
however
current
algorithms
learning
model
parameters
tend
produce
inconsistent
designs
due
ad
hoc
trial
error
nature
paper
develop
new
mathematical
framework
rbf
design
specifically
use
singular
value
decomposition
study
complexity
interpolation
design
matrices
form
foundation
sg
algorithm
proposed
algorithm
provides
consistent
approach
determining
rbf
parameters
viz
number
basis
functions
widths
centers
weights
shown
can
obtained
effective
rank
purpose
new
model
complexity
measure
introduced
relationship
singular
values
derived
centers
determined
qr
factorization
column
pivoting
right
singular
vectors
shown
selected
's
reflect
best
compromise
structural
stability
residual
minimization
finally
weights
computed
usual
pseudo
inverse
method
combining
decision
trees
using
systematic
patterns
author
hyunjoong
kim
worcester
polytechinc
institute
abstract
tree
ensemble
voting
methods
using
re
sampling
technique
highlighted
recently
statistical
classification
data
mining
paper
propose
new
ensemble
method
decision
trees
utilizes
systematic
patterns
classifications
new
method
improved
prediction
accuracy
single
decision
tree
algorithm
also
expected
method
performs
reasonably
well
fewer
number
re
samples
compared
popular
bagging
boosting
methods
experiment
real
dataset
carried
see
performance
new
method
sampling
resampling
methods
contributed
session
session
time
saturday
june
16th
11
05am
12
50pm
room
location
laguna
room
session
chair
ori
rosen
university
pittsburgh
resampling
time
series
seasonal
components
author
dimitris
politis
university
california
san
diego
abstract
case
time
series
seasonal
component
well
known
block
bootstrap
procedure
directly
applicable
propose
modification
block
bootstrap
successfully
addresses
issue
seasonalities
show
properties
subgraph
sampling
relational
data
author
david
jensen
university
massachusetts
amherst
jennifer
neville
university
massachusetts
amherst
abstract
sampling
central
evaluating
inductive
learning
algorithms
sampling
relational
data
far
challenging
error
prone
sampling
non
relational
contexts
examine
class
algorithms
sampling
relational
data
analyze
characteristics
samples
produce
show
important
pitfalls
unwary
researchers
inference
sample
maxima
presence
serrial
correlation
heavy
tailed
distributions
author
tucker
mcelroy
university
california
san
diego
dimitris
politis
ucsd
abstract
consider
data
infinite
order
moving
average
time
series
model
inputs
stable
domain
attraction
sample
maximum
data
interest
settings
insurance
finance
produce
normalization
statistic
conjunction
subsampling
methods
will
allow
asymptotically
correct
estimation
cumulative
distribution
function
bootqc
bootstrap
statistical
quality
control
applications
aviation
safety
analysis
author
regina
liu
rutgers
university
hueychung
teng
rutgers
university
abstract
control
charts
widely
used
effective
online
monitoring
tools
statistical
quality
control
existing
methods
constructing
control
charts
parametric
nature
applicability
much
restricted
requirement
predetermined
models
normal
distributions
nonparametric
alternative
based
bootstrap
methods
proposed
liu
tang
1996
using
standard
bootstrap
moving
block
bootstrap
new
bootstrap
control
charts
valid
monitoring
independent
data
well
dependent
data
assigning
proper
false
alarm
rates
bootstrap
control
charts
paper
develops
meaningful
threshold
system
regulating
monitoring
aviation
safety
data
threshold
system
can
serve
set
standards
evaluating
performance
aviation
entities
provide
guidelines
identifying
unexpected
performances
assigning
appropriate
corrective
measures
consequently
threshold
system
can
help
achieve
effective
regulation
air
traffic
safety
bootstrap
control
charts
threshold
systems
demonstrated
analysis
aviation
surveillance
data
collected
faa
several
air
carriers
demonstration
uses
software
bootqc
liu
teng
2000
bootqc
microsoft
excel
add
file
written
visual
basic
applications
vba
graphical
user
interfaces
can
provide
easy
access
online
data
analysis
applying
bootstrap
methods
generating
statistical
quality
control
charts
threshold
charts
selection
shrinkage
factor
two
stage
testimator
normal
mean
using
bootstrap
likelihood
author
makarand
ratnaparkhi
wright
state
university
dayton
vasant
waikar
maimi
university
oxford
ohio
frederick
schuurmann
miami
university
oxford
ohio
abstract
paper
new
methodology
based
likelihood
bootstrap
samples
introduced
improving
efficiency
two
stage
shrinkage
testimator
waikar
et
al
2001
estimation
normal
mean
particular
method
useful
selecting
shrinkage
factor
two
stage
testimator
view
increasing
efficiency
testimator
method
useful
number
estimation
problems
however
presentation
estimation
normal
mean
considered
related
simulation
studies
discussion
results
