sli classes cs178 notes boosting 
boosting 
classes
group
research
publications
code
login
classes
cs178
notes
boosting
boosting
another
popular
style
ensemble
classifier
boosted
ensembles
learners
trained
sequential
way
new
learner
focusing
data
yet
well
classified
basic
outline
boosted
learner
follows
take
type
base
classifier
make
sure
can
train
weighted
data
words
rather
attempting
minimize
classification
error
base
learner
will
need
minimize
weighted
classification
error
note
exactly
standard
empirical
classification
error
now
beginning
uniform
weights
train
base
classifier
predict
training
example
training
examples
get
right
reduce
weight
training
examples
get
wrong
increase
train
next
learner
repeat
focuses
next
learner
data
consistently
gotten
wrong
often
hard
data
points
finally
combine
learners
evaluating
rather
simple
average
take
weighted
average
multiplying
learner
j's
decision
linear
coefficient
precise
details
process
typically
distinguishes
different
boosting
algorithms
adaboost
popular
algorithm
adaboost
also
boosting
algorithm
first
popularized
framework
easiest
describe
adaboost
using
classes
adaboost
compute
coefficient
learner
based
weighted
error
rate
use
quantity
update
weights
well
re
normalize
weights
sum
one
note
since
quantity
either
1
1
equals
1
prediction
agrees
true
class
1
weights
incorrect
predictions
weights
correct
ones
final
classifier
given
summing
weighted
individual
learners
applying
threshold
boosting
surrogate
cost
functions
boosting
algorithms
can
shown
correspond
particular
surrogate
loss
function
replacement
classification
error
rate
particular
adaboost
corresponds
exponential
loss
function
note
share
sign
prediction
correct
value
will
small
smaller
confident
decision
boundary
cost
1
continues
increase
confident
incorrect
prediction
exponential
loss
function
nice
part
convex
upper
bounds
0
1
loss
classification
error
first
property
makes
easy
optimize
second
means
optimize
least
trying
push
downward
classification
loss
us
last
modified
february
25
2012
01
44
pm
bren
school
information
computer
science
university
california
irvine
